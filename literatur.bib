@article{adadi_blackbox_2018,
  author  = {Adadi, Amina and Berrada, Mohammed},
  journal = {IEEE Access},
  title   = {Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)},
  year    = {2018},
  volume  = {6},
  number  = {},
  pages   = {52138-52160},
  doi     = {10.1109/ACCESS.2018.2870052}
}

@book{alley_1996,
  title     = {{The Craft of Scientific Writing}},
  author    = {Michael Alley},
  year      = {1996},
  volume    = {3},
  publisher = {Springer}
}

@article{arrieta_explainable_2019,
  title        = {Explainable Artificial Intelligence ({XAI}): Concepts, Taxonomies, Opportunities and Challenges toward Responsible {AI}},
  url          = {http://arxiv.org/abs/1910.10045},
  shorttitle   = {Explainable Artificial Intelligence ({XAI})},
  abstract     = {In the last years, Artificial Intelligence ({AI}) has achieved a notable momentum that may deliver the best of expectations over many application sectors across the field. For this to occur, the entire community stands in front of the barrier of explainability, an inherent problem of {AI} techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of {AI}. Paradigms underlying this problem fall within the so-called {eXplainable} {AI} ({XAI}) field, which is acknowledged as a crucial feature for the practical deployment of {AI} models. This overview examines the existing literature in the field of {XAI}, including a prospect toward what is yet to be reached. We summarize previous efforts to define explainability in Machine Learning, establishing a novel definition that covers prior conceptual propositions with a major focus on the audience for which explainability is sought. We then propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at Deep Learning methods for which a second taxonomy is built. This literature analysis serves as the background for a series of challenges faced by {XAI}, such as the crossroads between data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of {AI} methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to {XAI} with a reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of {AI} in their activity sectors, without any prior bias for its lack of interpretability.},
  journaltitle = {{arXiv}:1910.10045 [cs]},
  author       = {Arrieta, Alejandro Barredo and Díaz-Rodríguez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and García, Salvador and Gil-López, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
  urldate      = {2021-05-19},
  date         = {2019-12-26},
  eprinttype   = {arxiv},
  eprint       = {1910.10045},
  keywords     = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing}
}

@online{clearbox_website,
  author = {{Clearbox AI}},
  title  = {{Manage AI models with confidence}},
  date   = {2021-09},
  url    = {https://clearbox.ai/}
}

@techreport{clearbox_wp,
  author  = {{Clearbox AI}},
  title   = {{Clearbox AI Model Assessment}},
  type    = {Whitepaper},
  year    = {2021},
  url     = {https://clearbox.ai/pdf/ClearboxAI_Technical_Whitepaper.pdf},
  urldate = {2021-04}
}

@online{cocoai_website,
  author = {CoCoAI},
  title  = {{Cooperative and communicating AI methods for medical image-guided diagnostics - A research project at the University of Lübeck.}},
  date   = {2021-09},
  url    = {https://cocoai.uni-luebeck.de}
}

@report{DIN,
  shorttitle  = {{DIN EN ISO 9241-210}},
  title       = {{DIN EN ISO 9241-210: Ergonomie der Mensch-System-Interaktion - Teil 210: Prozess zur Gestaltung gebrauchstauglicher interaktiver Systeme.}},
  year        = {2011},
  publisher   = {Beuth Verlag},
  institution = {{DIN Deutsches Institut für Normung e. V.}}
}

@techreport{eu_com_ai,
  author  = {{European Commision}},
  title   = {{On Artificial Intelligence - A European approach to excellence and trust}},
  type    = {Whitepaper},
  year    = {2020},
  url     = {https://ec.europa.eu/info/sites/default/files/commission-white-paper-artificial-intelligence-feb2020_en.pdf},
  urldate = {2021-09}
}

@online{eu_trustworthy_ai,
  author = {{European Commision}},
  title  = {{Ethics guidelines for trustworthy AI}},
  date   = {2021-03},
  url    = {https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai}
}

@online{ford,
  author = {Ford, Rebecca},
  title  = {{Earthquake: Twitter Users Learned of Tremors Seconds Before Feeling Them}},
  date   = {2011-08},
  url    = {http://www.hollywoodreporter.com/news/earthquake-twitter-users-learned-tremors-226481}
}

@book{greenberg_2012,
  title     = {{Sketching User Experiences. The Workbook}},
  author    = {S Greenberg and S Carpendale and N Marquardt and B Buxton},
  year      = {2012},
  publisher = {Morgen Kaufmann}
}

@software{hadoop,
  author  = {{Apache Software Foundation}},
  title   = {Hadoop},
  url     = {https://hadoop.apache.org},
  version = {0.20.2},
  date    = {2010-02-19}
}

@article{jordan_machine_2015,
  title        = {Machine learning: Trends, perspectives, and prospects},
  volume       = {349},
  issn         = {0036-8075, 1095-9203},
  url          = {https://www.sciencemag.org/lookup/doi/10.1126/science.aaa8415},
  doi          = {10.1126/science.aaa8415},
  shorttitle   = {Machine learning},
  abstract     = {Machine learning addresses the question of how to build computers that improve automatically through experience. It is one of today’s most rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of artificial intelligence and data science. Recent progress in machine learning has been driven both by the development of new learning algorithms and theory and by the ongoing explosion in the availability of online data and low-cost computation. The adoption of data-intensive machine-learning methods can be found throughout science, technology and commerce, leading to more evidence-based decision-making across many walks of life, including health care, manufacturing, education, financial modeling, policing, and marketing.},
  pages        = {255--260},
  number       = {6245},
  journaltitle = {Science},
  shortjournal = {Science},
  author       = {Jordan, M. I. and Mitchell, T. M.},
  urldate      = {2021-08-29},
  date         = {2015-07-17},
  langid       = {english}
}

@article{keane_how_2019,
  title        = {How Case Based Reasoning Explained Neural Networks: An {XAI} Survey of Post-Hoc Explanation-by-Example in {ANN}-{CBR} Twins},
  volume       = {11680},
  url          = {http://arxiv.org/abs/1905.07186},
  doi          = {10.1007/978-3-030-29249-2_11},
  shorttitle   = {How Case Based Reasoning Explained Neural Networks},
  abstract     = {This paper surveys an approach to the {XAI} problem, using post-hoc explanation by example, that hinges on twinning Artificial Neural Networks ({ANNs}) with Case-Based Reasoning ({CBR}) systems, so-called {ANN}-{CBR} twins. A systematic survey of 1100+ papers was carried out to identify the fragmented literature on this topic and to trace it influence through to more recent work involving Deep Neural Networks ({DNNs}). The paper argues that this twin-system approach, especially using {ANN}-{CBR} twins, presents one possible coherent, generic solution to the {XAI} problem (and, indeed, {XCBR} problem). The paper concludes by road-mapping some future directions for this {XAI} solution involving (i) further tests of feature-weighting techniques, (iii) explorations of how explanatory cases might best be deployed (e.g., in counterfactuals, near-miss cases, a fortori cases), and (iii) the raising of the unwelcome and, much ignored, issue of human user evaluation.},
  pages        = {155--171},
  journaltitle = {{arXiv}:1905.07186 [cs]},
  author       = {Keane, Mark T. and Kenny, Eoin M.},
  urldate      = {2021-08-29},
  date         = {2019},
  eprinttype   = {arxiv},
  eprint       = {1905.07186},
  keywords     = {Computer Science - Artificial Intelligence}
}

@book{knapp_2016,
  title     = {{Sprint. How to solve big problems and test new ideas in just five days}},
  author    = {J Knapp and J Zeratsky and B Kowitz},
  year      = {2016},
  publisher = {Simon \& Schuster Paperbacks}
}

@book{kumar_2013,
  title     = {{101 Design Methods - A Structured Approach for Driving Innovation in Your Organization}},
  author    = {V Kumar},
  year      = {2013},
  publisher = {John Wiley \& Sons, Inc.}
}

@article{ras_explanation_2018,
  title        = {Explanation Methods in Deep Learning: Users, Values, Concerns and Challenges},
  url          = {http://arxiv.org/abs/1803.07517},
  shorttitle   = {Explanation Methods in Deep Learning},
  abstract     = {Issues regarding explainable {AI} involve four components: users, laws \& regulations, explanations and algorithms. Together these components provide a context in which explanation methods can be evaluated regarding their adequacy. The goal of this chapter is to bridge the gap between expert users and lay users. Different kinds of users are identified and their concerns revealed, relevant statements from the General Data Protection Regulation are analyzed in the context of Deep Neural Networks ({DNNs}), a taxonomy for the classification of existing explanation methods is introduced, and finally, the various classes of explanation methods are analyzed to verify if user concerns are justified. Overall, it is clear that (visual) explanations can be given about various aspects of the influence of the input on the output. However, it is noted that explanation methods or interfaces for lay users are missing and we speculate which criteria these methods / interfaces should satisfy. Finally it is noted that two important concerns are difficult to address with explanation methods: the concern about bias in datasets that leads to biased {DNNs}, as well as the suspicion about unfair outcomes.},
  journaltitle = {{arXiv}:1803.07517 [cs, stat]},
  author       = {Ras, Gabrielle and van Gerven, Marcel and Haselager, Pim},
  urldate      = {2021-05-19},
  date         = {2018-03-29},
  eprinttype   = {arxiv},
  eprint       = {1803.07517},
  keywords     = {68-02, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{ribeiro_why_2016,
  title        = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
  url          = {http://arxiv.org/abs/1602.04938},
  shorttitle   = {"Why Should I Trust You?},
  abstract     = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose {LIME}, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  journaltitle = {{arXiv}:1602.04938 [cs, stat]},
  author       = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  urldate      = {2021-05-19},
  date         = {2016-08-09},
  eprinttype   = {arxiv},
  eprint       = {1602.04938},
  keywords     = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{samek_explaining_2021,
  title        = {Explaining Deep Neural Networks and Beyond: A Review of Methods and Applications},
  volume       = {109},
  issn         = {1558-2256},
  doi          = {10.1109/JPROC.2021.3060483},
  shorttitle   = {Explaining Deep Neural Networks and Beyond},
  abstract     = {With the broader and highly successful usage of machine learning ({ML}) in industry and the sciences, there has been a growing demand for explainable artificial intelligence ({XAI}). Interpretability and explanation methods for gaining a better understanding of the problem-solving abilities and strategies of nonlinear {ML}, in particular, deep neural networks, are, therefore, receiving increased attention. In this work, we aim to: 1) provide a timely overview of this active emerging field, with a focus on “post hoc” explanations, and explain its theoretical foundations; 2) put interpretability algorithms to a test both from a theory and comparative evaluation perspective using extensive simulations; 3) outline best practice aspects, i.e., how to best include interpretation methods into the standard usage of {ML}; and 4) demonstrate successful usage of {XAI} in a representative selection of application scenarios. Finally, we discuss challenges and possible future directions of this exciting foundational field of {ML}.},
  pages        = {247--278},
  number       = {3},
  journaltitle = {Proceedings of the {IEEE}},
  author       = {Samek, Wojciech and Montavon, Grégoire and Lapuschkin, Sebastian and Anders, Christopher J. and Müller, Klaus-Robert},
  date         = {2021-03},
  keywords     = {Artificial intelligence, Best practices, Black-box models, Deep learning, Interpretability, Machine learning, Neural networks, Problem-solving, Systematics, Unsupervised learning, deep learning, explainable artificial intelligence ({XAI}), model transparency, neural networks}
}

@online{thinksono_website,
  author = {ThinkSono},
  title  = {{The world’s first software to detect DVT!}},
  date   = {2021-09},
  url    = {https://thinksono.com/}
}