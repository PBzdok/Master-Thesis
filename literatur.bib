@article{adadi_blackbox_2018,
  author  = {Adadi, Amina and Berrada, Mohammed},
  journal = {IEEE Access},
  title   = {Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)},
  year    = {2018},
  volume  = {6},
  number  = {},
  pages   = {52138-52160},
  doi     = {10.1109/ACCESS.2018.2870052}
}

@book{alley_1996,
  title     = {{The Craft of Scientific Writing}},
  author    = {Michael Alley},
  year      = {1996},
  volume    = {3},
  publisher = {Springer}
}

@article{arrieta_explainable_2019,
  title        = {Explainable Artificial Intelligence ({XAI}): Concepts, Taxonomies, Opportunities and Challenges toward Responsible {AI}},
  url          = {http://arxiv.org/abs/1910.10045},
  shorttitle   = {Explainable Artificial Intelligence ({XAI})},
  abstract     = {In the last years, Artificial Intelligence ({AI}) has achieved a notable momentum that may deliver the best of expectations over many application sectors across the field. For this to occur, the entire community stands in front of the barrier of explainability, an inherent problem of {AI} techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of {AI}. Paradigms underlying this problem fall within the so-called {eXplainable} {AI} ({XAI}) field, which is acknowledged as a crucial feature for the practical deployment of {AI} models. This overview examines the existing literature in the field of {XAI}, including a prospect toward what is yet to be reached. We summarize previous efforts to define explainability in Machine Learning, establishing a novel definition that covers prior conceptual propositions with a major focus on the audience for which explainability is sought. We then propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at Deep Learning methods for which a second taxonomy is built. This literature analysis serves as the background for a series of challenges faced by {XAI}, such as the crossroads between data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of {AI} methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to {XAI} with a reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of {AI} in their activity sectors, without any prior bias for its lack of interpretability.},
  journaltitle = {{arXiv}:1910.10045 [cs]},
  author       = {Arrieta, Alejandro Barredo and Díaz-Rodríguez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and García, Salvador and Gil-López, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
  urldate      = {2021-05-19},
  date         = {2019-12-26},
  eprinttype   = {arxiv},
  eprint       = {1910.10045},
  keywords     = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing}
}

@article{braun_thematical_2006,
  title        = {Using thematic analysis in psychology},
  volume       = {3},
  issn         = {1478-0887, 1478-0895},
  url          = {http://www.tandfonline.com/doi/abs/10.1191/1478088706qp063oa},
  doi          = {10.1191/1478088706qp063oa},
  pages        = {77--101},
  number       = {2},
  journaltitle = {Qualitative Research in Psychology},
  shortjournal = {Qualitative Research in Psychology},
  author       = {Braun, Virginia and Clarke, Victoria},
  urldate      = {2021-09-04},
  date         = {2006-01},
  langid       = {english}
}

@online{captum_website,
  author = {{Facebook}},
  title  = {{Captum: Model Interpretability for PyTorch}},
  date   = {2021-07},
  url    = {https://captum.ai/}
}

@article{chiou_trusting_2021,
  title        = {Trusting Automation: Designing for Responsivity and Resilience},
  issn         = {0018-7208},
  url          = {https://doi.org/10.1177/00187208211009995},
  doi          = {10.1177/00187208211009995},
  shorttitle   = {Trusting Automation},
  abstract     = {{ObjectiveThis} paper reviews recent articles related to human trust in automation to guide research and design for increasingly capable automation in complex work environments.{BackgroundTwo} recent trends?the development of increasingly capable automation and the flattening of organizational hierarchies?suggest a reframing of trust in automation is needed.{MethodMany} publications related to human trust and human?automation interaction were integrated in this narrative literature review.{ResultsMuch} research has focused on calibrating human trust to promote appropriate reliance on automation. This approach neglects relational aspects of increasingly capable automation and system-level outcomes, such as cooperation and resilience. To address these limitations, we adopt a relational framing of trust based on the decision situation, semiotics, interaction sequence, and strategy. This relational framework stresses that the goal is not to maximize trust, or to even calibrate trust, but to support a process of trusting through automation responsivity.{ConclusionThis} framing clarifies why future work on trust in automation should consider not just individual characteristics and how automation influences people, but also how people can influence automation and how interdependent interactions affect trusting automation. In these new technological and organizational contexts that shift human operators to co-operators of automation, automation responsivity and the ability to resolve conflicting goals may be more relevant than reliability and reliance for advancing system design.{ApplicationA} conceptual model comprising four concepts?situation, semiotics, strategy, and sequence?can guide future trust research and design for automation responsivity and more resilient human?automation systems.},
  pages        = {00187208211009995},
  journaltitle = {Human Factors},
  shortjournal = {Hum Factors},
  author       = {Chiou, Erin K. and Lee, John D.},
  urldate      = {2021-06-22},
  date         = {2021-04-27},
  langid       = {english},
  keywords     = {adaptive automation, autonomy, human–automation interaction, intelligent agents, intersubjectivity, trust}
}

@online{clearbox_website,
  author = {{Clearbox AI}},
  title  = {{Manage AI models with confidence}},
  date   = {2021-10},
  url    = {https://clearbox.ai/}
}

@techreport{clearbox_wp,
  author  = {{Clearbox AI}},
  title   = {{Clearbox AI Model Assessment}},
  type    = {Whitepaper},
  year    = {2021},
  url     = {https://clearbox.ai/pdf/ClearboxAI_Technical_Whitepaper.pdf},
  urldate = {2021-04}
}

@online{cocoai_website,
  author = {CoCoAI},
  title  = {{Cooperative and communicating AI methods for medical image-guided diagnostics - A research project at the University of Lübeck.}},
  date   = {2021-09},
  url    = {https://cocoai.uni-luebeck.de}
}

@report{DIN,
  shorttitle  = {{DIN EN ISO 9241-210}},
  title       = {{DIN EN ISO 9241-210: Ergonomie der Mensch-System-Interaktion - Teil 210: Prozess zur Gestaltung gebrauchstauglicher interaktiver Systeme.}},
  year        = {2011},
  publisher   = {Beuth Verlag},
  institution = {{DIN Deutsches Institut für Normung e. V.}}
}

@techreport{eu_com_ai,
  author  = {{European Commision}},
  title   = {{On Artificial Intelligence - A European approach to excellence and trust}},
  type    = {Whitepaper},
  year    = {2020},
  url     = {https://ec.europa.eu/info/sites/default/files/commission-white-paper-artificial-intelligence-feb2020_en.pdf},
  urldate = {2021-09}
}

@online{eu_trustworthy_ai,
  author = {{European Commision}},
  title  = {{Ethics guidelines for trustworthy AI}},
  date   = {2021-03},
  url    = {https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai}
}

@online{ford,
  author = {Ford, Rebecca},
  title  = {{Earthquake: Twitter Users Learned of Tremors Seconds Before Feeling Them}},
  date   = {2011-08},
  url    = {http://www.hollywoodreporter.com/news/earthquake-twitter-users-learned-tremors-226481}
}

@article{franke_personal_2019,
  title        = {A Personal Resource for Technology Interaction: Development and Validation of the Affinity for Technology Interaction ({ATI}) Scale},
  volume       = {35},
  issn         = {1044-7318, 1532-7590},
  url          = {https://www.tandfonline.com/doi/full/10.1080/10447318.2018.1456150},
  doi          = {10.1080/10447318.2018.1456150},
  shorttitle   = {A Personal Resource for Technology Interaction},
  pages        = {456--467},
  number       = {6},
  journaltitle = {International Journal of Human–Computer Interaction},
  shortjournal = {International Journal of Human–Computer Interaction},
  author       = {Franke, Thomas and Attig, Christiane and Wessel, Daniel},
  urldate      = {2021-05-21},
  date         = {2019-04-03},
  langid       = {english}
}

@inproceedings{gordon_disagreement_2021,
  location   = {Yokohama Japan},
  title      = {The Disagreement Deconvolution: Bringing Machine Learning Performance Metrics In Line With Reality},
  isbn       = {9781450380966},
  url        = {https://dl.acm.org/doi/10.1145/3411764.3445423},
  doi        = {10.1145/3411764.3445423},
  shorttitle = {The Disagreement Deconvolution},
  eventtitle = {{CHI} '21: {CHI} Conference on Human Factors in Computing Systems},
  pages      = {1--14},
  booktitle  = {Proceedings of the 2021 {CHI} Conference on Human Factors in Computing Systems},
  publisher  = {{ACM}},
  author     = {Gordon, Mitchell L. and Zhou, Kaitlyn and Patel, Kayur and Hashimoto, Tatsunori and Bernstein, Michael S.},
  urldate    = {2021-09-21},
  date       = {2021-05-06},
  langid     = {english}
}

@book{greenberg_2012,
  title     = {{Sketching User Experiences. The Workbook}},
  author    = {S Greenberg and S Carpendale and N Marquardt and B Buxton},
  year      = {2012},
  publisher = {Morgen Kaufmann}
}

@software{hadoop,
  author  = {{Apache Software Foundation}},
  title   = {Hadoop},
  url     = {https://hadoop.apache.org},
  version = {0.20.2},
  date    = {2010-02-19}
}

@article{hoffman_metrics_2019,
  title        = {Metrics for Explainable {AI}: Challenges and Prospects},
  url          = {http://arxiv.org/abs/1812.04608},
  shorttitle   = {Metrics for Explainable {AI}},
  abstract     = {The question addressed in this paper is: If we present to a user an {AI} system that explains how it works, how do we know whether the explanation works and the user has achieved a pragmatic understanding of the {AI}? In other words, how do we know that an explanainable {AI} system ({XAI}) is any good? Our focus is on the key concepts of measurement. We discuss specific methods for evaluating: (1) the goodness of explanations, (2) whether users are satisfied by explanations, (3) how well users understand the {AI} systems, (4) how curiosity motivates the search for explanations, (5) whether the user's trust and reliance on the {AI} are appropriate, and finally, (6) how the human-{XAI} work system performs. The recommendations we present derive from our integration of extensive research literatures and our own psychometric evaluations.},
  journaltitle = {{arXiv}:1812.04608 [cs]},
  author       = {Hoffman, Robert R. and Mueller, Shane T. and Klein, Gary and Litman, Jordan},
  urldate      = {2021-05-19},
  date         = {2019-02-01},
  eprinttype   = {arxiv},
  eprint       = {1812.04608},
  keywords     = {Computer Science - Artificial Intelligence}
}

@article{JMLR:v17:15-618,
  author  = {Sebastian Lapuschkin and Alexander Binder and Gr{{\'e}}goire Montavon and Klaus-Robert M{{{\"u}}}ller and Wojciech Samek},
  title   = {The LRP Toolbox for Artificial Neural Networks},
  journal = {Journal of Machine Learning Research},
  year    = {2016},
  volume  = {17},
  number  = {114},
  pages   = {1-5},
  url     = {http://jmlr.org/papers/v17/15-618.html}
}

@article{jordan_machine_2015,
  title        = {Machine learning: Trends, perspectives, and prospects},
  volume       = {349},
  issn         = {0036-8075, 1095-9203},
  url          = {https://www.sciencemag.org/lookup/doi/10.1126/science.aaa8415},
  doi          = {10.1126/science.aaa8415},
  shorttitle   = {Machine learning},
  abstract     = {Machine learning addresses the question of how to build computers that improve automatically through experience. It is one of today’s most rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of artificial intelligence and data science. Recent progress in machine learning has been driven both by the development of new learning algorithms and theory and by the ongoing explosion in the availability of online data and low-cost computation. The adoption of data-intensive machine-learning methods can be found throughout science, technology and commerce, leading to more evidence-based decision-making across many walks of life, including health care, manufacturing, education, financial modeling, policing, and marketing.},
  pages        = {255--260},
  number       = {6245},
  journaltitle = {Science},
  shortjournal = {Science},
  author       = {Jordan, M. I. and Mitchell, T. M.},
  urldate      = {2021-08-29},
  date         = {2015-07-17},
  langid       = {english}
}

@article{keane_how_2019,
  title        = {How Case Based Reasoning Explained Neural Networks: An {XAI} Survey of Post-Hoc Explanation-by-Example in {ANN}-{CBR} Twins},
  volume       = {11680},
  url          = {http://arxiv.org/abs/1905.07186},
  doi          = {10.1007/978-3-030-29249-2_11},
  shorttitle   = {How Case Based Reasoning Explained Neural Networks},
  abstract     = {This paper surveys an approach to the {XAI} problem, using post-hoc explanation by example, that hinges on twinning Artificial Neural Networks ({ANNs}) with Case-Based Reasoning ({CBR}) systems, so-called {ANN}-{CBR} twins. A systematic survey of 1100+ papers was carried out to identify the fragmented literature on this topic and to trace it influence through to more recent work involving Deep Neural Networks ({DNNs}). The paper argues that this twin-system approach, especially using {ANN}-{CBR} twins, presents one possible coherent, generic solution to the {XAI} problem (and, indeed, {XCBR} problem). The paper concludes by road-mapping some future directions for this {XAI} solution involving (i) further tests of feature-weighting techniques, (iii) explorations of how explanatory cases might best be deployed (e.g., in counterfactuals, near-miss cases, a fortori cases), and (iii) the raising of the unwelcome and, much ignored, issue of human user evaluation.},
  pages        = {155--171},
  journaltitle = {{arXiv}:1905.07186 [cs]},
  author       = {Keane, Mark T. and Kenny, Eoin M.},
  urldate      = {2021-08-29},
  date         = {2019},
  eprinttype   = {arxiv},
  eprint       = {1905.07186},
  keywords     = {Computer Science - Artificial Intelligence}
}

@article{knapic_explainable_2021,
  title        = {Explainable Artificial Intelligence for Human Decision-Support System in Medical Domain},
  url          = {http://arxiv.org/abs/2105.02357},
  abstract     = {In the present paper we present the potential of Explainable Artificial Intelligence methods for decision-support in medical image analysis scenarios. With three types of explainable methods applied to the same medical image data set our aim was to improve the comprehensibility of the decisions provided by the Convolutional Neural Network ({CNN}). The visual explanations were provided on in-vivo gastral images obtained from a Video capsule endoscopy ({VCE}), with the goal of increasing the health professionals' trust in the black box predictions. We implemented two post-hoc interpretable machine learning methods {LIME} and {SHAP} and the alternative explanation approach {CIU}, centered on the Contextual Value and Utility ({CIU}). The produced explanations were evaluated using human evaluation. We conducted three user studies based on the explanations provided by {LIME}, {SHAP} and {CIU}. Users from different non-medical backgrounds carried out a series of tests in the web-based survey setting and stated their experience and understanding of the given explanations. Three user groups (n=20, 20, 20) with three distinct forms of explanations were quantitatively analyzed. We have found that, as hypothesized, the {CIU} explainable method performed better than both {LIME} and {SHAP} methods in terms of increasing support for human decision-making as well as being more transparent and thus understandable to users. Additionally, {CIU} outperformed {LIME} and {SHAP} by generating explanations more rapidly. Our findings suggest that there are notable differences in human decision-making between various explanation support settings. In line with that, we present three potential explainable methods that can with future improvements in implementation be generalized on different medical data sets and can provide great decision-support for medical experts.},
  journaltitle = {{arXiv}:2105.02357 [cs]},
  author       = {Knapič, Samanta and Malhi, Avleen and Saluja, Rohit and Främling, Kary},
  urldate      = {2021-06-11},
  date         = {2021-05-05},
  eprinttype   = {arxiv},
  eprint       = {2105.02357},
  keywords     = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning}
}

@book{knapp_2016,
  title     = {{Sprint. How to solve big problems and test new ideas in just five days}},
  author    = {J Knapp and J Zeratsky and B Kowitz},
  year      = {2016},
  publisher = {Simon \& Schuster Paperbacks}
}

@book{kumar_2013,
  title     = {{101 Design Methods - A Structured Approach for Driving Innovation in Your Organization}},
  author    = {V Kumar},
  year      = {2013},
  publisher = {John Wiley \& Sons, Inc.}
}

@article{lapuschkin_unmasking_2019,
  title        = {Unmasking Clever Hans predictors and assessing what machines really learn},
  volume       = {10},
  rights       = {2019 The Author(s)},
  issn         = {2041-1723},
  url          = {https://www.nature.com/articles/s41467-019-08987-4},
  doi          = {10.1038/s41467-019-08987-4},
  abstract     = {Current learning machines have successfully solved hard application problems, reaching high accuracy and displaying seemingly intelligent behavior. Here we apply recent techniques for explaining decisions of state-of-the-art learning machines and analyze various tasks from computer vision and arcade games. This showcases a spectrum of problem-solving behaviors ranging from naive and short-sighted, to well-informed and strategic. We observe that standard performance evaluation metrics can be oblivious to distinguishing these diverse problem solving behaviors. Furthermore, we propose our semi-automated Spectral Relevance Analysis that provides a practically effective way of characterizing and validating the behavior of nonlinear learning machines. This helps to assess whether a learned model indeed delivers reliably for the problem that it was conceived for. Furthermore, our work intends to add a voice of caution to the ongoing excitement about machine intelligence and pledges to evaluate and judge some of these recent successes in a more nuanced manner.},
  pages        = {1096},
  number       = {1},
  journaltitle = {Nature Communications},
  author       = {Lapuschkin, Sebastian and Wäldchen, Stephan and Binder, Alexander and Montavon, Grégoire and Samek, Wojciech and Müller, Klaus-Robert},
  urldate      = {2021-05-24},
  date         = {2019-03-11},
  langid       = {english}
}

@article{mueller_explanation_2019,
  title        = {Explanation in Human-{AI} Systems: A Literature Meta-Review, Synopsis of Key Ideas and Publications, and Bibliography for Explainable {AI}},
  url          = {http://arxiv.org/abs/1902.01876},
  shorttitle   = {Explanation in Human-{AI} Systems},
  abstract     = {This is an integrative review that address the question, "What makes for a good explanation?" with reference to {AI} systems. Pertinent literatures are vast. Thus, this review is necessarily selective. That said, most of the key concepts and issues are expressed in this Report. The Report encapsulates the history of computer science efforts to create systems that explain and instruct (intelligent tutoring systems and expert systems). The Report expresses the explainability issues and challenges in modern {AI}, and presents capsule views of the leading psychological theories of explanation. Certain articles stand out by virtue of their particular relevance to {XAI}, and their methods, results, and key points are highlighted. It is recommended that {AI}/{XAI} researchers be encouraged to include in their research reports fuller details on their empirical or experimental methods, in the fashion of experimental psychology research reports: details on Participants, Instructions, Procedures, Tasks, Dependent Variables (operational definitions of the measures and metrics), Independent Variables (conditions), and Control Conditions.},
  journaltitle = {{arXiv}:1902.01876 [cs]},
  author       = {Mueller, Shane T. and Hoffman, Robert R. and Clancey, William and Emrey, Abigail and Klein, Gary},
  urldate      = {2021-05-19},
  date         = {2019-02-05},
  eprinttype   = {arxiv},
  eprint       = {1902.01876},
  keywords     = {Computer Science - Artificial Intelligence}
}

@online{people_ai_google_website,
  author = {{Google}},
  title  = {{People + AI Guidebook}},
  date   = {2021-06},
  url    = {https://pair.withgoogle.com/guidebook/}
}

@article{ras_explainable_2021,
  title        = {Explainable Deep Learning: A Field Guide for the Uninitiated},
  url          = {http://arxiv.org/abs/2004.14545},
  shorttitle   = {Explainable Deep Learning},
  abstract     = {Deep neural networks ({DNNs}) have become a proven and indispensable machine learning tool. As a black-box model, it remains difficult to diagnose what aspects of the model's input drive the decisions of a {DNN}. In countless real-world domains, from legislation and law enforcement to healthcare, such diagnosis is essential to ensure that {DNN} decisions are driven by aspects appropriate in the context of its use. The development of methods and studies enabling the explanation of a {DNN}'s decisions has thus blossomed into an active, broad area of research. A practitioner wanting to study explainable deep learning may be intimidated by the plethora of orthogonal directions the field has taken. This complexity is further exacerbated by competing definitions of what it means ``to explain'' the actions of a {DNN} and to evaluate an approach's ``ability to explain''. This article offers a field guide to explore the space of explainable deep learning aimed at those uninitiated in the field. The field guide: i) Introduces three simple dimensions defining the space of foundational methods that contribute to explainable deep learning, ii) discusses the evaluations for model explanations, iii) places explainability in the context of other related deep learning research areas, and iv) finally elaborates on user-oriented explanation designing and potential future directions on explainable deep learning. We hope the guide is used as an easy-to-digest starting point for those just embarking on research in this field.},
  journaltitle = {{arXiv}:2004.14545 [cs, stat]},
  author       = {Ras, Gabrielle and Xie, Ning and van Gerven, Marcel and Doran, Derek},
  urldate      = {2021-10-06},
  date         = {2021-09-13},
  eprinttype   = {arxiv},
  eprint       = {2004.14545},
  keywords     = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{ras_explanation_2018,
  title        = {Explanation Methods in Deep Learning: Users, Values, Concerns and Challenges},
  url          = {http://arxiv.org/abs/1803.07517},
  shorttitle   = {Explanation Methods in Deep Learning},
  abstract     = {Issues regarding explainable {AI} involve four components: users, laws \& regulations, explanations and algorithms. Together these components provide a context in which explanation methods can be evaluated regarding their adequacy. The goal of this chapter is to bridge the gap between expert users and lay users. Different kinds of users are identified and their concerns revealed, relevant statements from the General Data Protection Regulation are analyzed in the context of Deep Neural Networks ({DNNs}), a taxonomy for the classification of existing explanation methods is introduced, and finally, the various classes of explanation methods are analyzed to verify if user concerns are justified. Overall, it is clear that (visual) explanations can be given about various aspects of the influence of the input on the output. However, it is noted that explanation methods or interfaces for lay users are missing and we speculate which criteria these methods / interfaces should satisfy. Finally it is noted that two important concerns are difficult to address with explanation methods: the concern about bias in datasets that leads to biased {DNNs}, as well as the suspicion about unfair outcomes.},
  journaltitle = {{arXiv}:1803.07517 [cs, stat]},
  author       = {Ras, Gabrielle and van Gerven, Marcel and Haselager, Pim},
  urldate      = {2021-05-19},
  date         = {2018-03-29},
  eprinttype   = {arxiv},
  eprint       = {1803.07517},
  keywords     = {68-02, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{ribeiro_anchors_2018,
  title        = {Anchors: High-Precision Model-Agnostic Explanations},
  volume       = {32},
  rights       = {Copyright (c)},
  issn         = {2374-3468},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/11491},
  shorttitle   = {Anchors},
  number       = {1},
  journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
  shortjournal = {{AAAI}},
  author       = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  urldate      = {2021-06-24},
  date         = {2018-04-25},
  langid       = {english},
  keywords     = {interpretability}
}

@article{ribeiro_why_2016,
  title        = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
  url          = {http://arxiv.org/abs/1602.04938},
  shorttitle   = {"Why Should I Trust You?},
  abstract     = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose {LIME}, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  journaltitle = {{arXiv}:1602.04938 [cs, stat]},
  author       = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  urldate      = {2021-05-19},
  date         = {2016-08-09},
  eprinttype   = {arxiv},
  eprint       = {1602.04938},
  keywords     = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{samek_explaining_2021,
  title        = {Explaining Deep Neural Networks and Beyond: A Review of Methods and Applications},
  volume       = {109},
  issn         = {1558-2256},
  doi          = {10.1109/JPROC.2021.3060483},
  shorttitle   = {Explaining Deep Neural Networks and Beyond},
  abstract     = {With the broader and highly successful usage of machine learning ({ML}) in industry and the sciences, there has been a growing demand for explainable artificial intelligence ({XAI}). Interpretability and explanation methods for gaining a better understanding of the problem-solving abilities and strategies of nonlinear {ML}, in particular, deep neural networks, are, therefore, receiving increased attention. In this work, we aim to: 1) provide a timely overview of this active emerging field, with a focus on “post hoc” explanations, and explain its theoretical foundations; 2) put interpretability algorithms to a test both from a theory and comparative evaluation perspective using extensive simulations; 3) outline best practice aspects, i.e., how to best include interpretation methods into the standard usage of {ML}; and 4) demonstrate successful usage of {XAI} in a representative selection of application scenarios. Finally, we discuss challenges and possible future directions of this exciting foundational field of {ML}.},
  pages        = {247--278},
  number       = {3},
  journaltitle = {Proceedings of the {IEEE}},
  author       = {Samek, Wojciech and Montavon, Grégoire and Lapuschkin, Sebastian and Anders, Christopher J. and Müller, Klaus-Robert},
  date         = {2021-03},
  keywords     = {Artificial intelligence, Best practices, Black-box models, Deep learning, Interpretability, Machine learning, Neural networks, Problem-solving, Systematics, Unsupervised learning, deep learning, explainable artificial intelligence ({XAI}), model transparency, neural networks}
}

@online{streamlit_website,
  author = {{Streamlit Inc.}},
  title  = {{Streamlit: The fastest way to build and share data apps}},
  date   = {2021-07},
  url    = {https://streamlit.io/}
}

@online{thinksono_website,
  author = {ThinkSono},
  title  = {{The world’s first software to detect DVT!}},
  date   = {2021-09},
  url    = {https://thinksono.com/}
}

@online{zotero_website,
  author = {{Corporation for Digital Scholarship}},
  title  = {{Your personal research assistant}},
  date   = {2021-07},
  url    = {https://www.zotero.org/}
}