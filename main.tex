\documentclass[11pt,a4paper,english]{scrreprt}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage[long,nodayofweek,level,24hr]{datetime} 
\usepackage[skip=5pt,font=footnotesize]{caption}

\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{pdfpages}
\usepackage{xcolor}
\usepackage[bookmarksnumbered]{hyperref}
\hypersetup{pdfborder={0 0 0}, breaklinks=true}
\usepackage{bookmark}
% \addtokomafont{disposition}{\rmfamily}
\usepackage[onehalfspacing]{setspace}

\usepackage[newfloat]{minted}
\input{minted-style}

% DISABLE TODO NOTES HERE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{todonotes}
%\usepackage[disable]{todonotes}

% HEADER & FOOTER DEFINITIONS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{fancyhdr}
\input{page-style}

% LITERATURE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[backend=biber,style=apa]{biblatex}
\addbibresource{literatur.bib}

% DEF FOR COVER %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\titelMAEnglish}{Explainable Artificial Intelligence:\\ Designing human-centric assessment system interfaces to increase explainability and trustworthiness of artificial intelligence in medical applications}

\newcommand{\authorMA}{Philipp Dominik Bzdok}

\newcommand{\examinerMA}{Univ.-Prof. Dr. rer. nat. Thomas Franke, Dipl.-Psych.}

\newcommand{\supporterMA}{Tim Schrills, M.Sc.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{common-macros}

% DEF FOR COMMENTS, CAN BE DELETED %%%%%%%%%%%%%%%%%%%%%%%%
\newenvironment{comment}
  {\par\medskip
   \begingroup\color{olive}%
   }
 {\endgroup
  \medskip}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\pagenumbering{gobble}
\input{deckblatt}
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

\input{preamble}
\tableofcontents

\newpage
\pagenumbering{arabic}
\chapter{Introduction}\label{chapter:introduction}
The use of modern Artificial Intelligence (\textbf{AI}) techniques is pervasive and can be found in many fields of application, such as digital image processing, search engines and speech recognition \parencite{eu_com_ai}. Other application fields, such as medical diagnosis systems, cannot benefit as easily from AI based technology compared to recreational domains. This impediment stems oftentimes from the AI being a "black box". In consequence humans struggle to understand such AI-systems and their output, leading to trust and compliance issues \parencite{adadi_blackbox_2018}. These issues are further enhanced in the medical context where decisions, possibly based on AI, can have severe consequences for users, especially patients.

An example for medical human-AI-interaction is the image-based recognition of Deep Vein Thrombosis (\textbf{DVT}) with real time AI support for medical professionals by \textit{Think\-Sono}. The system leverages AI to guide the user through the current gold-standard diagnosis, a compression ultrasound examination, so that it enables any healthcare professional to detect DVT \parencite{thinksono_website}. Closely related in this context is the interdisciplinary research project \textit{CoCoAI}, which aims to explore psychological, ethical and technological implications of human-centered, AI based applications in the DVT diagnosis and beyond \parencite{cocoai_website}. 

When AI based systems are used in high risk application contexts, such as medical diagnosis, the aspects of explainability, interpretability and trustworthiness become a primary concern for adoption and use of said system. \textcite{ribeiro_why_2016} already explored the importance of explainability and trust in AI based systems and postulated that AI systems will not be used if the users have no trust in the model or the results. Even though many machine learning algorithms score high on standard performance metrics, such as precision, recall or Area Under the Receiver Operating Characteristics (\textbf{AUROC}), user-facing performance may be way worse \parencite{gordon_disagreement_2021}. Understanding the AI's underlying machine learning (\textbf{ML}) model and its predictions is an important step for assessing trust and facilitating effective interaction \parencite{ribeiro_why_2016}. Recent technological advances are realized by \textit{Clearbox AI}, with the focus on trustworthy AI by implementing an AI model assessment \parencite{clearbox_website, eu_trustworthy_ai}. The model assessment can help model owners to identify robustness issues, potential undesired behaviour, and explain errors and uncertainties regarding the model predictions \parencite{clearbox_wp}.

Trust in AI systems is primarily induced by the users' understanding and the general interpretability of the machine learning model and their predictions \parencite{ribeiro_why_2016, ras_explanation_2018}. The wide array of different possible user groups and the complex constructs of understandability and trust demands for a human-centric approach in designing AI assessment systems. Because of the inherent complexity of non-linear machine learning models, especially Deep Neural Networks (\textbf{DNNs}) for image processing, suitable visualization and communication techniques are non-trivial. Additionally to the complex models for image classification, the input data is also more complex as it is unstructured. Non-linear neural networks and unstructured data provide additional challenges for Explainable Artificial Intelligence (\textbf{XAI}), as described in \textcite{keane_how_2019}. XAI is a research field that studies how AI decisions and data driving those decisions can be explained to people in order to provide transparency, enable assessment of accountability, demonstrate fairness, or facilitate understanding \parencite{arrieta_explainable_2019}. XAI plays an important role in the acceptance and finally in the usage of AI based technology. This is further underlined in the medical context where public authorities set strict regulations on the usage of technological systems and ethic concerns have to be thoughtfully addressed.

In the context of a image-based medical diagnosis system, it is important that the responsible stakeholders, such as medical practitioners, specialized doctors and clinic managements, are enabled to make informed decisions on the usage of AI based technology, even though their expertise in machine learning and data science is expected to be low. The stakeholders' trust in this system is a primary factor for the widespread use of said technology for real life applications. Therefore, increasing the understanding of the AI model and finding an optimal trust level in the predictions by designing human-centric explanation techniques within the AI model assessment system is a main goal of this work. Additionally it is conceivable that authorities will instantiate auditors for AI based systems in medical contexts. Having a comprehensible and scientifically proven assessment system could be a big step in the approval and adoption of said system. 

\section{Goals}\label{section:goals}
The users understanding of the AI model and trust in the model are highly essential as pointed out by \textcite{knapic_explainable_2021}. This holds especially true for medical applications where re-traceable results have to be provided and people acting on these results bear great responsibility. To facilitate understanding and trust the machine learning model has to be interpretable and explainable. In the context of Convolutional Neural Networks (\textbf{CNNs}) interpretability of models can pose a significant concern because of their inherent complexity. Explanations of AI models can provide insights on the machine's decision process and therefore generate user understanding. This can lead to the model being more interpretable by humans.

Assessing the suitability and performance of a CNN for a specific task by applying standard performance evaluation metrics is problematic, since these can be oblivious to distinguishing the diverse problem solving behaviors of a neural network \parencite{lapuschkin_unmasking_2019}. \textcite{samek_explaining_2021,JMLR:v17:15-618,ribeiro_anchors_2018} give an overview on the technical foundations of XAI and a presentation of practical methods, which will be used in conjunction with human-centric design to explore and evaluate suitable and efficient methods to explain a model's classification.

The goal of this thesis is to design, develop and evaluate interactive AI assessment system artifacts for medical professionals and machine learning specialists in a human-centric fashion to facilitate understandability and trustworthiness of AI models. Developing such a system, with human concerns in focus, leads to following research questions:
\begin{itemize}
    \item[Q1:] How is the stakeholders' (medical professionals, clinic managements or data scientists) subjective information processing awareness linked to trust for a specific model and its predictions in the medical domain?
    \item[Q2:] How can different explanation techniques, ranging from perturbation based approaches to explorative alternatives, increase trust in image classifier models and predictions?
    \item[Q3:] What are interaction methods to explain and possibly optimize trust levels in image classification models?
    \item[Q4:] To what extend can structured metadata increase the stakeholder's understanding of a model's operational range and performance?
\end{itemize}

\section{State of the Art}\label{section:state_of_the_art}
An AI assessment system is currently offered by Clearbox AI. The \textit{AI Control Room} cloud platform enables users to assess, improve and validate ML models and data in accordance with the principles of Trustworthy AI \parencite{clearbox_website,eu_trustworthy_ai}. \textcite{clearbox_website} describes its AI assessment system as a "Deep Pre‑production Analysis" tool:
\begin{displayquote}
    "AI Control Room automatically generates a model assessment to help model owners to identify robustness issues, potential undesired behaviour, and explain errors and uncertainties regarding the model predictions."
\end{displayquote}

Concretely the product enables users to perform following tasks for AI models working on tabular data:
\begin{description}
    \item [Model behaviour validation:] Validation metrics and plausible causes of error are clearly presented, potential limitations and irreducible uncertainty are identified and local explanations of the model behaviour are generated selecting representative points in the dataset.
    \item [Synthetic data generation:] A generative model can be used to create synthetic data points that preserve the statistical properties of the original dataset. These points can augment the original training set to improve generalization, to increase model robustness, and to oversample specific labels when in the presence of unbalanced data.
    \item [Data-centric analysis:] Generative models perform a probabilistic analysis of the underlying data allowing for robust outliers detection and uncertainty analysis. This information can help you to evaluate data quality.
    \item [Centralised tracking system:] AI Control Room acts as a centralised tracking system to store lineage, versioning, and metadata of your datasets and models. Assessments generated are securely persisted along with models and datasets.
\end{description}

Besides general information and standard metrics (see \autoref{fig:model_assessment_overview}) the assessment system offers varied insights into different aspects of the machine learning model: \autoref{fig:model_assessment_graphs} shows graphs of training and validation precision, recall and calibration, while \autoref{fig:model_assessment_analysis} shows the models strong points and limitations by analyzing the feature distribution in the data. Furthermore, the second half of the model assessment focuses more on the interpretability aspect of machine learning: \autoref{fig:model_assessment_interpret} shows a confusion matrix of possible classification results, which is then extended by example data points, chosen by the assessment system (see \autoref{fig:model_assessment_examples}). These examples can then be further explored to generate understanding of the models inner workings by applying an attribution based explanation technique combined with a decision rule explanation.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/screenshots/model_assessment_overview.png}
    \caption{AI Control Room - Model Assessment Overview with Standard Metrics}
    \label{fig:model_assessment_overview}
\end{figure}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/screenshots/model_assessment_graphs.png}
    \caption{AI Control Room - Precision-Recall and Calibration Graphs}
    \label{fig:model_assessment_graphs}
\end{figure}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/screenshots/model_assessment_analysis.png}
    \caption{AI Control Room - Model String Points and Limitations}
    \label{fig:model_assessment_analysis}
\end{figure}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/screenshots/model_assessment_interpret.png}
    \caption{AI Control Room - Interpretability Assessment}
    \label{fig:model_assessment_interpret}
\end{figure}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/screenshots/model_assessment_examples.png}
    \caption{AI Control Room - Example Data}
    \label{fig:model_assessment_examples}
\end{figure}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/screenshots/model_assessment_explanation.png}
    \caption{AI Control Room - Prediction Explanation for Examples Data}
    \label{fig:model_assessment_explanation}
\end{figure}
\clearpage

\section{Approach}
As already described in the introduction of \autoref{chapter:introduction}, many machine learning algorithms score high on standard performance metrics, but user-facing performance may be way worse \parencite{gordon_disagreement_2021}. This issue is caused by real world applications being very dependent on the actual human-AI-interaction. Following this reasoning, it lends itself to utilize a human-centered design process for creating AI assessment systems. \autoref{fig:DIN_EN_ISO_9241} shows a standardized process of human-centered design, which was applied in this thesis to conceptualize, implement and evaluate assessment system artifacts. The key take-away is the inclusion of human aspects in all stages of the process. Research on evaluation of AI explanations revealed that there is a big gap between the perceived and actual usefulness of explanations, as described by \textcite{ras_explainable_2021}. This further underlines the need for a human-centered approach in designing AI assessment systems.

The thesis' structure will reflect the human-centered approach, which is visualized in \autoref{fig:DIN_EN_ISO_9241}: As already alluded in \autoref{chapter:introduction}, \autoref{chapter:analysis} is about understanding and setting the context of use by conducting literature research and user interviews. Based on the established requirements \autoref{chapter:conception} will describe the conception of functionalities and interaction design. The development of solutions will be described in \autoref{chapter:implementation}, while \autoref{chapter:evaluation} is about the evaluation of the solutions. This general process is embedded in a iterative loop, where intermediate results are evaluated against the requirements and subject to change.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/figures/DIN_EN_ISO_9241-210.png}
    \caption{Human-centered Design Process \parencite{DIN}}
    \label{fig:DIN_EN_ISO_9241}
\end{figure}

\newpage
\chapter{Analysis}\label{chapter:analysis}
Following the human-centered design process, it is important to incorporate the potential users from the beginning. This is also reflected in the analysis, where the context and setting of use has to be understood and set. Besides the human factors, there are also more general and theoretical aspects to be analysed, such as the context of AI in the medical application domain for specific tasks, such as classifying disease patterns with medical imaging.

In the following the context, task, problem and users will be described and analysed as a foundation for the human-centered design process and the following conception of AI assessment system artifacts.

\section{Data Sources}
Three main data sources where used for the analysis, ranging from general scientific literature about XAI to specially elaborated user interviews and cooperation with developers of an existing application.

\subsection{Scientific Literature}
Literature is the foundation of the analysis. As described by \textcite{mueller_explanation_2019}, the amount of scientific publications on the topic of explanation in intelligent systems has surged in the last 5 years, revealing many important and relevant information on this subject area through openly accessible papers. In the beginning of the thesis (July 2021) a general search on \textit{Google Scholar} was conducted to gain a overview on available publications. A non-exhaustive list of search terms at the time was:
\begin{itemize}
    \item XAI
    \item XAI in Medical Applications
    \item Explainable Artificial Intelligence
    \item Explainable Artificial Intelligence in Medical Applications
    \item Explainable Machine Learning
    \item Interpretable Machine Learning
    \item Explaining Black-Box Machine Learning Models
    \item Explaining DNNs
\end{itemize}
This general research yielded already good results, as there were many relatively new and popular publications on the topic of XAI, such as \textcite{mueller_explanation_2019, ras_explanation_2018, adadi_blackbox_2018, hoffman_metrics_2019}.

The results of the internet research were then further reinforced by academic partners from the University of Lübeck, with whom related research was conducted in the context of the \textit{CoCoAI} project. Leveraging the available resources and support from research partners boosted the yield on relevant scientific literature tenfold. Over the course of multiple months the list of literature grew and is still being maintained in a shared \textit{Zotero} library \parencite{zotero_website}. The most important scientific papers for this analysis were: \textcite{ras_explanation_2018, arrieta_explainable_2019, ribeiro_why_2016, adadi_blackbox_2018,knapic_explainable_2021, samek_explaining_2021, chiou_trusting_2021, hoffman_metrics_2019}. 

\subsection{Interviews}\label{subsection:interviews}
Complementing the general research on XAI, specially elaborated user interviews where conducted. These interviews specifically target medical professionals and data scientists. Interviews with the actual user group of a potential solution is key to understanding and setting the context and requirements of use. The participants for the interviews were chosen with following requirements in mind:
\begin{description}
    \item[Medical Professionals:] Has interest and/or knowledge in AI-systems; has worked with or researched AI-systems in the medical domain; can judge the benefits and risks of the use of AI in medical applications.
    \item[Data Scientists / AI Researchers:] Is familiar with the XAI topic; has interest in explainability and trustworthiness of machine learning models; has worked with AI in the medical context.
\end{description}
Participants for the interviews were gathered via academic partners, internet research and word of mouth. In total 16 suitable people from 6 different institutions (among them UKSH, TU München and Mevis Frauenhofer Bremen) were contacted about potential interviews. Ten leads were medical professionals while six leads where data scientists or AI researchers. From the total of 16 potential interview partners only four interviews have been conducted. This low yield is due to the time constraint of the individuals, who are mainly full time medical practitioners or researchers. The participants are described in further detail in \autoref{table:interview_participants}.

\begin{table}[htbp]
    \centering
    \begin{tabularx}{\textwidth}{ l l l X X l }
        \toprule
        ID & Age & Gender & Occupation & Education Level & AAII Score \\
        \midrule
        1 & 28 & male & Assistant Physician in Neuroradiology & State Examination & 4.89 \\ 
        2 & 24 & female & Research Associate (ML) & Masters Degree & 5.12 \\ 
        3 & 48 & male & Surgeon & Dr. med. & 5.45 \\ 
        4 & 27 & female & Assistant Physician in Neuroradiology & State Examination & 4.67 \\ 
        \bottomrule
    \end{tabularx}
    \caption{Interview Participants}
    \label{table:interview_participants}
\end{table}

The Interviews were conducted in german and executed as 1 to 1 online interviews. For reference the interviews were recorded if consent was present. Additionally the interviews were supported by a research colleague, who kept protocol. After the interviews the recordings were transcribed for further analysis and the participants were asked to answer the \textit{Affinity for AI Interaction} (\textbf{AAII}) questionnaire, which is a modified version of the \textit{Affinity for Technology Interaction} (\textbf{ATI}) questionnaire \parencite{franke_personal_2019}. ATI aims to determine the tendency to actively engage in intensive technology interaction, as a key personal resource for coping with technology. Analogously the AAII questionnaire aims to determine the tendency to actively engage in AI interaction.

In terms of content, the interviews for medical professionals and data scientists were slightly different as seen in \autoref{table:interview_topics}. The reason for this is the heterogenous expertise on the subject area of machine learning models and potential user requirements. The whole interview guideline can be found in \autoref{appendix:interview_guideline}.
\begin{table}[htbp]
    \centering
    \begin{tabularx}{\textwidth}{ l l }
        \toprule
        Medical Professionals & Data Scientists / AI researchers \\
        \midrule
        Actual Usage of AI & Actual Usage of AI \\ 
        Perspective on AI Usage & Comparison of AI Models \\  
        Trust in AI & Perspective on AI Usage \\
        Potential Problems with AI Usage & Trust in AI \\
        Own Explanation Techniques & Potential Problems with AI Usage \\
        Familiarity with XAI & Own Explanation Techniques \\
        Assessment on Local Explanations & Familiarity with XAI \\
        Information Processing & Need for Local Explanations \\
        Reliability vs. Trust vs. Understanding & Need for Global Explanations \\
         & Information Processing \\
         & Trust-Behavior Connection \\
         & Reliability vs. Trust vs. Understanding \\
        \bottomrule
    \end{tabularx}
    \caption{Interview Topics}
    \label{table:interview_topics}
\end{table}

After gathering the interview data via protocols and transcripts a thematic analysis according to \textcite{braun_thematical_2006} was applied to identify common topics and codes. The thematic analysis is a widely used qualitative analysis method mainly found in the field of psychology and can be used as a primary tool to access data from interviews. Applying this method resulted in a thematic map showcasing common overlapping topics found in the interviews, which can be seen in \autoref{fig:thematic_mind_map}. The thematic map serves as the baseline for further context and requirement analysis.

\begin{figure}[htbp]
    \centering
    \includegraphics[height=0.8\textwidth, angle=90]{img/figures/Thematic_Mind_Map.pdf}
    \caption{Thematic Mind Map}
    \label{fig:thematic_mind_map}
\end{figure}

\subsection{Existing Applications}\label{subsection:existing_apps}
A scientific cooperation with Clearbox AI allowed us to access a additional source of information, valuable for the analysis and conception. As already described in \autoref{section:state_of_the_art} Clearbox developed an AI model assessment solution among other things. During the period of cooperation regular meetings were held with the CTO and other employees. These meetings were used for knowledge exchange on the subject of XAI literature, previous experiences, feedback and conceptional workshops.

The enormous previous experience of Clearbox is a great resource of information for the thesis. Many aspects of analysis and conception were supported by the regular, bi-weekly meetings. In particular, resources such as \textcite{people_ai_google_website, captum_website,streamlit_website, lapuschkin_unmasking_2019} were supplied by Clearbox. Furthermore the (beta) access to the Clearbox AI \textit{Control Room} cloud platform and the communication of user feedback was invaluable to gather information on the analysis and conception of an assessment system for image-based AI models.

\section{Context Analysis}
Black-box DNNs have become pervasive in todays society and represent a proven and indispensible machine learning tool. While these machine learning models can easily be used in recreational non-risky contexts, this does not hold true for the medical domain where decisions based on the results of a machine learning model can bear great risks for users and patients. This issue stems from the lack of interpretability and trustworthiness of DNNs \parencite{adadi_blackbox_2018}. DNNs architectures are inherently hard to understand and therefore interpretability of and trust in the results of such neural networks are a challenge.

Creating a solution to explain AI models a priori to the use can help setting the right expectations towards the AI model. Consequently the users of such a explanatory system gain the ability to build a fair mental model before using the AI, which in turn can support the formation of appropriate levels of trust \parencite{hoffman_metrics_2019}. This is beneficial to the user and facilitates efficient usage of the model in production \parencite{hoffman_metrics_2019, people_ai_google_website}.

Explaining the model a priori also enables the solution to build computationally complex explanations, which can depend on datasets of thousands of images. Supplying the user with explanations potentially based on the whole dataset can also be beneficial: Statistical analysis and clustering of the data and metadata can support the understanding of model limitations and edge cases, while exemplary local explanations can be statistically distributed to gain a better overview of global behavior of the model. Combining different types of a priori explanations improve the coverage of the key attributes of explanations - understandability, feeling of satisfaction, sufficiency of detail, completeness, usefulness, accuracy, and trustworthiness - which were described by \textcite{hoffman_metrics_2019}.

The aspects mentioned above are also reflected in the interview data. The following list showcases translated quotes from interview partners regarding the need for (a priori) explanations for AI models aimed at the medical domain:
\begin{displayquote}
    "I think you have to be critical and look at the results carefully - is the result at all plausible?"
\end{displayquote}
\begin{displayquote}
    "What is the information that is interesting for the system or that is decisive for the decisions? What information is rather irrelevant?"
\end{displayquote}
\begin{displayquote}
    "If the explanations for the model are based on any data that are meaningless from my clinical experience, such as a stroke being pinned down by bone shapes. That would shake my confidence in the machine, even though it might give reliable results."
\end{displayquote}
\begin{displayquote}
    "If, for example, there is an outlier data point in a specific case and you are not quite sure why it is like this or how you should interpret it, then something like this [local explanation] is great, so that you can understand why the result is like this or like this."
\end{displayquote}
\begin{displayquote}
    "I've always been interested in exactly how this works, how much training data it's based on, what's behind it, why the system decides the way it does."
\end{displayquote}
\begin{displayquote}
    "[...] and you might also learn what the machine pays attention to, which I personally could also pay attention to when I look at the picture. That would certainly strengthen my confidence in the application."
\end{displayquote}

\section{Problem and Task Analysis}
The following problem scenario summarizes the starting point of the problem and task analysis: The general medical practitioner Dr. med. Mustermann wants to offer thrombosis diagnosis in his office. He is not specialized in vein examination and thus has just basic knowledge and also no necessary equipment. In the past this has lead to him referring patients with venous disorders to a specialized clinic. Through a colleague he was made aware of "\textit{AutoDVT}", a AI based software developed by ThinkSono, which can help him offer DVT diagnosis in his office. The AutoDVT system works with image-based machine learning and can support medical professionals in real time with identifying DVT. Since Dr. med. Mustermann has very little knowledge of AI and machine learning he is very skeptical towards this innovative but foreign software system. Although he sees the immediate benefits of using a system which supports him with the examination of DVT, his trust in the system’s predictions is very low and he fears relying on the AI’s assessment. AI based technology is a black box for him, which he does not fully understand. The predictions of the systems are opaque to him and thus lead to rejection of the system.

DNNs for image classification are able to detect various disease patterns with medical imaging and can be used by medical professionals to support diagnosis and possibly increase efficiency and effectivity if trusted and used correctly \parencite{adadi_blackbox_2018,knapic_explainable_2021}. However, the reality looks different: Medical professionals bear the responsibility for their decisions regarding the patient and thus are rather relucant about using AI based systems - even though the AI could outperform them in image classification tasks. Most of the time decisions are based on personal experience, which was developed over a long period of time. This sentiment is reflected in the interview statements of medical professionals:
\begin{displayquote}
    "One risk, I believe, is also that you hand over responsibility to the machine."
\end{displayquote}
\begin{displayquote}
    "The classic risk of simply relying on what the algorithm says."
\end{displayquote}
\begin{displayquote}
    "The doctor with his expert knowledge will always compare this with his knowledge and experience, is this correct, what is the probability, is this in the range?"
\end{displayquote}

Modern DNN based image classificators, such as the \texttt{XRV\--DenseNet121\--densenet121\--res224\--all} model from \textcite{cohen_limits_2020}, can provide very good results in the prediction of pathologies. For example, the prediction accuracy for pneumonia is benchmarked as 86\% \parencite{torchxrayvision_github}. The use of such model or comparitive ones could benefit medical professionals in many ways as described by the interview partners:
\begin{displayquote}
    "It [AI system] facilitates standardized findings in particular"
\end{displayquote}
\begin{displayquote}
    "It [AI system] might give you a little peace of mind that you haven't missed anything."
\end{displayquote}
\begin{displayquote}
    "I always think to myself that this is based on CT gray levels, i.e. on these density values, and I always think to myself that it makes sense that a computer can distinguish these density levels better than my eyes."
\end{displayquote}

The benefits of using a DNN based image classificator to support medical professionals in detection and diagnosis of diseases are clear. For users to leverage these benefits trust in such a system must be given, which cannot be generated by mere accuracy metrics \parencite{samek_explaining_2021}. To overcome the hesitation of using AI in medical applications, a AI assessment system can be used to facilitate understanding of and trust in the algorithms. Stakeholders, such as practitioners or clinic managements, can use such a system to gain customized insights into the model and data prior to using it in everyday clinical practice.

\section{User Analysis}\label{section:user_analysis}
Trustworthiness and explainability of AI models only makes sense considering the potential user groups. For an assessment system that aims to explain AI models in the medical domain those user groups are: (1) Medical professionals, such as practitioners and clinic managements and (2) data scientists and AI researchers developing AI models. Naturally those two groups differ greatly from each other. While medical professionals have great expertise in various fields of examination, diagnosis and communication of pathologies, they also are expected to have little knowledge in computer sciences and machine learning. Data scientists on the other hand do have great knowledge of computer sciences, machine learning and neural network architectures, but lack the concrete medical expertise. Following the human-centered process the needs, requirements and whishes in regard to an AI assessment system of those user groups were analysed, mainly referencing the interviews from \autoref{subsection:interviews} and the resulting thematic map (see \autoref{fig:thematic_mind_map}).

\subsection{Medical professionals}
The user group of medical professionals is a very heterogenous one, which is to be expected from a professional field with many different specializations. This was also found out in the interviews. Surveying practitioners of different ages showed, that especially the younger ones, working in neuroradiology, are open towards using AI in their daily routine or that they are already using it. Great initiative was shown by two assistant physicians, who also took courses on machine learning in the medical domain during their studies. Then again the older, but way more experienced surgeon has stated that he has much less contact with AI in clinical practice. All interviewed medical professionals showed interest in medical AI in research projects and were positive on the benefits of AI, especially computer vision tasks. The most cited benefits were the great ability of machine learning to identify pathologies in medical images, the take over of redundant tasks, the backup for diagnosis and the handling of computationally expensive tasks. The medical professionals were also wary and timid about using AI. This stance was stated to be mainly routed in the missing experience in using and trusting these systems. The black box character of DNNs was stated to be a central issue: Not being able to re-trace the decisions of the AI and having to give away responsibility lead to trust and compliance issues, which was already stated by \textcite{ras_explainable_2021}. Depending on standard metrics was also stated to be insufficient, as the interviewed experts showed interest in the training data set and active comparison of the AI's results with their own experiences. While standard metrics, such as accuracy, sensitivity and specificity were important to the interviewees, the critical evaluation of the results and the validation of the behavior were whished for by every one of them.

As \autoref{table:interview_participants} shows, all interviewees had a high affinity for AI interaction. This is a important aspect to consider, since it shows their tendency to actively engage in interaction with AI while also being interested in it. This fact explains that the physicians were so interested in the explainability and comprehensibility of AI models. The interviewees stated that the explanation of AI decision and therefore the understanding of the model is important to them. Also the training data and its quality was a very common topic amongst all participants. Interestingly it was also stated, that understanding and trusting the model is important to being able to propagate the knowledge and trust to fellow medical practitioners and also patients.

Even though the interest in the functionality of machine learning models was big, the medical practitioners admitted that they have little knowledge on this subject and are limited regarding understanding the technical complexities. However they also stated that there is ongoing collaboration with AI researchers and software engineers for research purposes.

\subsection{Data scientists and AI researchers}
This potential group of AI assessment system users stand in great contrast to the previously mentioned one. Data scientists and AI researchers have a good understanding of the complexities and inner workings of machine learning algorithms. Therefore the requirements and needs of this user group are expected to be very different from the medical professionals. As \autoref{table:interview_participants} shows, unfortunately only one AI researcher could be interviewed during the analysis.

The interviewee stated experience with many kinds of neural networks, while also being familiar with clustering, featuring and interpretation tools. The perspective of AI researchers on interpretability and trustworthiness seems to be also quite different. Important aspects mentioned were: Performance metrics do not guarantee usability in real applications; separation of system results and system architecture; experimental validation; relations to developers; reviewing code. Understanding the complexities of such AI systems was a key aspect as stated by the interviewee. For this literature and study experience were mentioned to be crucial. Even some experimentation with heatmap-based explanation tools were used to understand AI models better.

While data scientists and AI researcher are different to medical professionals in their expertise, some overlap was found in the interviews: The interviewee stated to generate trust by doing exemplary input-output experiments, screening the training data set and reviewing own expectations. Another common topic was the insufficiency of standard metrics for assessing algorithm performance.

An explicit topic was the use of local explanations. The interviewee stated interest in local explanations as they are needed for improving their own understanding and for publications as proof that an algorithm works. Global explanations were not distinguished from local explanations by the interview partner, since the goal seemed to be the same: Determining if a model has weaknesses and to what it can be safely applied. Generating a benchmark for comparability of models to enable better adoption was wished for. Furthermore it was stated that theory should be researched further for understanding in addition to empiricism, which is also stated by \textcite{people_ai_google_website}:
\begin{displayquote}
    "When is an explanation really meaningful? Explaining everything is difficult, but finding out when explanation should be given."
\end{displayquote}

\section{Conclusion on the Analysis}
While the use of DNN based image classifiers for medical applications has many benefits, actual adoption is impeded by the black box character of such systems. The potential users, such as medical practitioners have trust and compliance issues. Even though the users see the immediate benefits of such AI based systems, especially in computer vision tasks, the issue with handing over responsibility to a system they do not understand prevails.

Insufficient standard metrics shall be supplemented with more interactive explanations with a focus on the comprehensibility of complex AI models. Promoting the formation of an appropriate mental model and therefore trust in the abilities of such systems is a key aspect which was identified by the analysis. The use of a priori explanations via an assessment system where the users can experiment with different AI models in a protected environment covers many requirements of the users: Screening the training data, exploring model strengths and limitations, analyzing visual explanations for images and doing input-output experiments. These are the core requirements for an assessment system to increase explainability and trustworthiness of AI in medical, image-based applications. Other relevant findings suggest that such an assessment system needs to actively consider the intention of the user, since it can vary greatly depending on the person's background: Medical professionals with low expertise in machine learning may need to have a more guided user experience, while experts on the subject of machine learning may prefer a more open interaction style. Furthermore it is conceivable that statistical clustering, based on (training) metadata distributions can improve the explanation satisfaction by offering a balanced access to huge datasets in a way that is not susceptible to biases.

\newpage
\chapter{Conception}\label{chapter:conception}
The conception of the AI assessment system follows from the requirements specified in the analysis. Based on the thesis objectives and user needs, functionalities have been derived. Core aspects to be adopted from the analysis are interactive exploration of data, visual explanation of attribution, comparative explanations, input-output experiments and interaction guidance. Before diving into the details of functional conception (\autoref{section:functionalities}), interaction design (\autoref{section:interaction_design}), and system architecture (\autoref{section:system_architecture}), an overview of the conceptual approach (\autoref{section:conceptual_approach}) and foundational use cases (\autoref{section:use_cases}) will be given.

\section{Conceptual Approach}\label{section:conceptual_approach}
Based on \autoref{chapter:analysis} and the primary objectives, the conception follows the already known design process. To begin with, building upon the previously gathered information, a functional specification was created. This specification relies heavily on the thematic analysis of the interviews, which is showcased in \autoref{fig:thematic_mind_map}. The first step of creating the functional specification was to identify common use cases for an AI assessment system which can be used by medical professionals. The specification defines formal tasks, sub-tasks and required capabilities for those use cases. The formalized functionalities were further used as the foundation for an interdependency analysis, which should highlight coactive human-computer-interaction (\textbf{HCI}) design patterns \parencite{johnson_coactive_2014}. Having the functionalities defined and set allows for conception of interaction and interface design, accompanied by the technical system architecture. The design concepts were created in a way that heavily referenced \textcite{people_ai_google_website} and \textcite{clearbox_website}. Having Control Room by Clearbox as a close reference allowed for the leveraging of their knowledge and expertise. Furthermore a design iteration was created by conducting an expert workshop on interaction design with colleagues from Clearbox and the University of Lübeck. The results from interdependency analysis, interaction dialogues and interaction flowcharts were then realized in \autoref{chapter:implementation}.

\section{Use Cases}\label{section:use_cases}
Two common use cases found throughout the user requirements are to be presented as an anchor for further conception and reference. The main difference in these use cases is the underlying user group and therefore the intention of interaction.

\subsection{Understanding through Interaction}
A main use case emerges from \autoref{chapter:analysis}: Medical professionals who want to understand and trust machine learning algorithms through extensive interaction with the training data, visual explanations and comparisons before actually using the system in their daily clinical life. Medical practitioners or clinic managements are therefore enabled to form appropriate expectations and mental model of the model's capabilities and performance, which helps them adopting the model and propagating knowledge and trust to professional peers and patients.

\subsection{Comparison of Models}
An additional use case can be conceived, which revolves around users with a better understanding of machine learning: The interactive comparison of different AI models to be used in the context of medical application development. Using an AI model assessment beyond standard metrics can facilitate better decisions in favor or against a specific machine learning model. The ease of use and high interactivity of such system enables specialized users to explore more intricate facets of DNNs. Combining standard metrics with data exploration, visual perturbation techniques and the ability to experiment freely with the model creates a sandbox environment for testing and comparing different machine learning models and data sets. 

\section{Functionalities}\label{section:functionalities}
Interactive explanations which offer access to information about the training data and the model's functionalities, strengths and limitations are the focus. Furthermore natural alternatives in the form of comparative explanations, as described by \textcite{cai_effects_2019}, shall be supplemented to the presentation of image data. Considering the low expertise of medical professionals in machine learning topics, general explanations of system capabilities in textual form are expected to be beneficial. Additionally the ability to execute input-output experiments with data and the actual model were universally requested. \autoref{table:function_specification} summarizes the specified functionalities. Furthermore these functionalities will be supplemented by visual explanation techniques. A research on different visualization techniques yielded many possible algorithms to further explain the ML model's reasoning. The main resources for this were \textcite{adadi_blackbox_2018,samek_explaining_2021,ribeiro_anchors_2018,arrieta_explainable_2019,ras_explainable_2021}. A wide selection of implementations for visual explanation techniques are available, ranging from perturbation-based to model intrinsic methods as described by \textcite{ras_explainable_2021}. From this lot of techniques, three were found to be particularly interesting based on their features. \autoref{table:visual_explanation_methods} shows the selections and the main properties of those. Moreover \autoref{subsection:explanation_techniques} further characterizes all explanation methods that are thought to be suitable for the assessment system application.

The functionalities, more specifically tasks, as defined in \autoref{table:function_specification} were subject to an interdependency analysis. The goal of the analysis was to define sub-tasks and capabilities required for those, which give insights into the mode of interaction between computer and user. \autoref{table:interdependency_analysis_table} showcases the key aspects of the analysis in form of a interdependency analysis table as conceived by \textcite{johnson_coactive_2014}. The table makes it clear, that the human interacting with the system is highly dependent on the computer - this is no surprise in the context of a explanatory system, where the user seeks out information about a complex model. On the other hand the computer does not depend much on the human counterpart, because it has the prevalence of information, but the computer can still benefit from the human expertise in certain situations. Overall this highlights the importance of interaction design and system collegiality for an AI assessment system.

\begin{table}[htbp]
    \centering
    \begin{tabularx}{\textwidth}{ c X X }
        \toprule
        \# & Functionality & Description \\
        \midrule
        1 & Browse training data for given class & The user is able to explore the whole dataset, which was used for training of the ML model. Additionally it is possible for the user to filter the data based on classification labels. \\ 
        2 & Show examples of false negative, false positive and low confidence & The user is able to explore training data for which the classification resulted in false negative, false positive or low classification confidence \\  
        3 & Group data based on similarities & The user is presented with clustered training data that was algorithmically identified to be similar. \\
        4 & Show data that is very similar to data from another class & The user is presented with comparative explanations which showcase data points that are very similar to other data points but classified differently \\
        5 & Offer overview of general system capabilities & The user is presented with general information and metrics about the ML model in a written and structured way \\
        6 & Show written Explanations & The user is presented with written explanations about the system by leveraging text templates \\
        7 & Input-Output-Experiment & The user is able to feed the ML model with data and predict the models result. The model then also predicts a result, which is then compared with the user's prediction \\
        \bottomrule
    \end{tabularx}
    \caption{Functional Specification}
    \label{table:function_specification}
\end{table}

\begin{table}[htbp]
    \centering
    \begin{tabularx}{\textwidth}{ 
            >{\hsize=.5\hsize\linewidth=\hsize}X
            >{\hsize=.7\hsize\linewidth=\hsize}X
            >{\hsize=1.8\hsize\linewidth=\hsize}X
        }
        \toprule
        Method & Type & Description \\
        \midrule
        Occlusion & Perturbation-based & Replaces rectangular areas in input with baseline reference and computes difference in output; Most useful in models where pixels in contiguous regions are likely to be correlated \\ 
        Anchors & Rule-based & Shows part of the input (super pixels) which are sufficient for the classifier to make the prediction; Builds on the shortcomings of LIME \parencite{ribeiro_why_2016}\\  
        Layerwise Relevance Propagation & Model intrinsic & Propagates the prediction backwards using purposely 
        designed local propagation rules; allows for differentiation between positive and negative influence of input pixels to prediction \\
        \bottomrule
    \end{tabularx}
    \caption{Visual Explanation Methods}
    \label{table:visual_explanation_methods}
\end{table}

\begin{table}[htbp]
    \centering
    \includegraphics[height=\textwidth, angle=90]{img/figures/Interdependency_Analysis_Table.pdf}
    \caption{Interdependency Analysis Table}
    \label{table:interdependency_analysis_table}
\end{table}

\subsection{Explanation Techniques}\label{subsection:explanation_techniques}
Three groups of explanation techniques are created as possible ways to implement the required functionalities (\autoref{table:function_specification}). The groups are divided by their style of interaction and information deliverance. Most of the presented techniques have a direct mapping to the functionalities to be provided by the assessment system, especially \textit{General Description, Metrics, Data Browsing and Experiment}. The visual explanation techniques though, do overlap significantly in their practicality. While they differ drastically in their mode of operation and technical background, they do cater to the same interaction: Showing image regions that are significant to the result of the AI. Because of this and the limited time and resource frame, only one of the three proposed visual explanation types will be implemented.
\subsubsection*{Textual}
\begin{description}[font=\normalfont\itshape]
    \item[General Description:] General descriptions of the system capabilities aim to explain the system on a level, that is accessible to machine learning laymen, such as medical professionals. As described by \textcite{people_ai_google_website} it is beneficial for building trust when the system capabilities are explained instead of the technology itself. This helps the user to build a better mental model, especially when dealing with hyper-complex structures, such as DNNs. The general descriptions shall contain a textual explanations of the model capabilities to ensure a good introduction and appropriate expectations.
    \item[Metrics:] Standard metrics are also used to describe the machine learning model. The metrics used are \textit{Accuracy, Precision, Recall \& F1}. Those metrics were chosen, because they are often used to describe other processes and applications in the medical context and therefore should be familiar and interpretable by the user. 
\end{description}

\subsubsection*{Interactive}
\begin{description}[font=\normalfont\itshape]
    \item[Data Browsing:] The ability to browse the training data of a machine learning model was universally requested by the interviewed professionals. The browsing ability is extended by different filters for the specific functionality. The user shall be able to explore the training data based on the class of the image. Furthermore the data will be algorithmically grouped based on similarities to potentially expose patterns. Additionally the user shall be able to explore data points that belong to the edge cases of the models capabilities, such as false negative and false positives.
    \item[Experiment:] Comparing the machine learning models predictions to the human expertise also was a key aspect of AI interaction in the medical domain as shown by the interviews. Therefore a input-output-experiment shall be implemented, where the user is able to test the AI against its own expertise.
\end{description}

\subsubsection*{Visual}
\begin{description}[font=\normalfont\itshape]
    \item[Occlusion:] A perturbation based approach to compute attribution, involving replacing each contiguous rectangular region with a given baseline, and computing the difference in output. Occlusion is most useful in cases such as images, where pixels in a contiguous rectangular region are likely to be highly correlated \parencite{captum_website,zeiler_visualizing_2013}.
    \item[Anchors:] The algorithm provides model-agnostic and human interpretable explanations suitable for classification models applied to images, text and tabular data. The idea behind anchors is to explain the behaviour of complex models with high-precision rules called anchors. These anchors are locally sufficient conditions to ensure a certain prediction with a high degree of confidence \parencite{ribeiro_anchors_2018}.
    \item[LRP:] The Layer-wise Relevance Propagation (LRP) algorithm explains a classifier's prediction specific to a given data point by attributing (positive \& negative) relevance scores to important components of the input by using the topology of the learned model itself \parencite{lapuschkin_unmasking_2019}.
\end{description}
Referencing the \autoref{chapter:analysis}, the main goal of a visual explanation shall be the validation of the AI model's behavior. Therefore a simple technique, such as Occlusion, which only highlights image regions with a high importance to a result, is reasonable to implement. While LRP does have certain benefits because of its ability to assess positive and negative influences of pixels to the model output, it also comes with a high complexity that manifests as a lot of hyperparameters, making it hard to implement and use properly. Anchors work differently to the other two methods, since it is able to find a hyperpixel that is most influential to a AI's classification. In the context of standardized medical images, that are already preprocessed in a certain way (e.g. X-ray images), the method looses it's edge. The high similarity of images in medical binary classification tasks lessens the expressiveness of Anchors significantly.    

\section{System Architecture}\label{section:system_architecture}
The goal is to create an interactive software application, therefore a suitable system architecture has to be constructed to suite the needs of the users and the usage context. Although, implementing the functionalities from \autoref{section:functionalities} is possible in many different ways. It is possible to realize the assessment system as a classical, offline software application or by leveraging web based tool sets for a possibly distributed cloud solution. Also it is conceivable to implement the system on a middle ground of those two, by creating a software that is build to be ran locally in common web browsers. To stay hardware and software agnostic these three options will be shortly evaluated against each other based on the requirements set by the functionalities.

The common requirements are the ability to store machine learning models and training data for the assessment to be computed. In addition the user has to be presented with a graphical user interface (\textbf{GUI}) to interact with. Having these two components in a close relation can drastically reduce the overhead of implementing the communication between the computational and data storage component with the GUI component. On the other hand such a close relation in an offline system can significantly reduce the flexibility of the implementation regarding GUI and interaction design. Furthermore a single offline application has to take many different execution environments (operating systems) into account, which might be a big downside depending on the actual context of usage. Separating the system into a multi-tier application allows for more modularity and freedom in choosing the actual implementation technology. A multi-tier web application allows for a very specialized choice of tools for the respective component at the cost of a higher complexity and implementation cost. Such web applications have the advantage of being relatively easy to transform into a local application without the need of hosting a server environment. This can be the middle ground between the offline local and the web based distributed application.

Referencing the usage context of the application it is not needed to implement a highly complex distributed application, although Clearbox has shown that it is very much possible to implement a robust cloud based solution. To reduce the scope of implementation for this thesis a middle ground is the most reasonable: Leveraging modern web based tools that are mostly environment agnostic to build a flexible application that could be ran in either the cloud or locally on a single machine. \autoref{fig:system_architecture} shows a possible system architecture for a two-tiered application, where the GUI is separated from the logic and the data store. Such a separation of concerns on the macro level enables the usage of specialized tools for each component. While it is conceivable to move the data store into a separate tier (making it a three-tiered architecture), there are no concrete requirements for using a individual data store technology, such as a dedicated database.

The \textit{frontend} component encompasses the user interface, which will probably be realized with web-based tools as mentioned before. This web-based component facilitates the flexibility of the implementation, as the user will remain mostly hardware and software independent by leveraging common browser technologies. The \textit{backend} will be decoupled from the GUI and therefore can benefit from other technological stacks, optimized for the tasks of machine learning and data science. Additionally the backend will envelop the data and the AI model itself, for providing its services to the frontend. This split allows for a distributed, hardware and software agnostic architecture which can be run either locally or remotely, whereas leveraging the optimal tools for each task.

The communication between the two components will be realized through an \textbf{API}. The protocol used for such a communication will be the standardized \textit{Hypertext Transfer Protocol}, which allows for systems
to be built independently of the data being transferred \parencite{rfc2616}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/figures/architecture.png}
    \caption{Multi Tier System Architecture}
    \label{fig:system_architecture}
\end{figure}

\section{Interaction Design}\label{section:interaction_design}
The interaction design is a key aspect of HCI, as it defines how the user will actually interact with the system and how information will be made accessible to the user. Considering the human factors in the design process is very important as described by \textcite{wickens_2016_engineering} - for this builds upon the insights of the previous chapters, namely the definition of interaction (sub-)tasks and the interdependency analysis, which in turn are based on the general objectives and user needs as described in \autoref{chapter:analysis}.

The conception of functionalities and interaction design was the main topic of the previously mentioned expert workshop, which was conducted with colleagues from the University of Lübeck and Clearbox. The goal was to present, validate and discuss the scope of functionalities (based on the previous research and the user requirements) and the interaction design which utilizes scaffolding and guidance for the main user group. The workshop was therefore an iteration of the previously conceived results and the yielded good feedback on the scope of functionalities and the guidance concept. A main focus of the discussion was type of guidance which was then resolved to a semantic signal manifested by an intention query. Another relevant results of the workshop were: (1) The identification of a possible contradiction between different visual explanation techniques, which were described in \autoref{subsection:explanation_techniques}. This reinforces the idea to limit the visual explanations to one to avoid unnecessary confusion of the user. (2) Feedback from Clearbox's own tests showed the tendency to a more sequentially user experience in contrast to a dashboard centered application. (3) Pre-computation of computationally expensive explanation methods to increase the interaction performance and response time of the application.

A central consideration of the interaction design is to adapt the system to the abilities and needs of the actual users. Referencing \autoref{subsection:interviews}, a main user group has low expertise in machine learning, which in turn means special requirements for the interaction design of an AI assessment system. Therefore the design approach will revolve around a guided interaction version and a unguided interaction version: The guided interaction will leverage a more streamlined approach which facilitates the intentions of the user by asking and then presenting the respective functionality. \textcite{trigg_guided_1988} already proposes local guidance of the user by the author of a system, by using intention queries to determine a suitable path through the content of an application. This mode of interaction aims to relieve the user of mental workload by scaffolding the application and thus avoiding the presentation of all functionalities, and therefore aiding the learning process for machine learning topics \parencite{soloway_learner_1994}. The other mode of interaction will be unguided, as in there will be no intention query and the user will be able to freely explore the whole lot of functionalities offered by the application, which is hypothesize to be beneficial for users with more expertise in machine learning. The baseline for both ways of interaction is the whole array of functionalities as specified in \autoref{table:function_specification}.

To further explore and concretize the interaction design two methods were applied: \textit{Interaction Dialogues} and \textit{Interaction Flowcharts}. Interaction dialogues aim to creatively explore different ways of implementing the functionalities as defined in \autoref{section:functionalities}. Additionally they enable the exploration of other perspectives based on a natural interaction mode (spoken dialogue). The result is then the baseline for the interaction flowcharts, which specify the concrete flow of information in a standardized manner.

\subsection{Interaction Dialogues}\label{subsection:interaction_dialogues}
The idea of using interaction dialogues to creatively explore different ways of interacting with the assessment system came from the cooperation with Clearbox. A colleague specialized in building User Interaction and User Experience proposed this method to be used for developing the interaction design. Using dialogues as the mode of interaction and communication between human and computer allows for a very liberated design process. The goal was to explore different variations of interaction based on the functionalities as defined in \autoref{table:function_specification}. An example dialogue (with H for human and C for computer) for task 1 will be presented:
\begin{displayquote}
    H: I would like to see your training data.\\
    C: Do you want to see all data or just data for a specific class?\\
    H: I want to only see data for class x.\\
    C: Here you go! Do you want to see data of another class, too?\\
    H: Yes please, but give me some time.\\
    C: Of course!
\end{displayquote}

Using the dialogue technique allows for easy exploration of alternative information flows as shown by the next example:
\begin{displayquote}
    H: I would like to see your training data.\\
    C: Here take a look!\\
    H: Wow, that is really a lot!\\
    C: Do you want to filter for a specific class?\\
    H: Yes, please show me only data for class x.\\
    C: There you go.\\
    H: Thank you, and now please show me all data for class y.\\
    C: Sure, here!
\end{displayquote}

These examples clearly show different ways of interacting in order to accomplish the same task: Browsing the training data. This process was applied to all tasks (and sub-tasks) from \autoref{table:interdependency_analysis_table} to gather a lot of different interaction variations, to be then used as a baseline for the following interaction flowcharts. The key takeaways from this process, was that there are various possibilities to realize an interaction. With multiple alternatives per task, some comparisons could be made: Often it seems beneficial for a streamlined interaction design, to present the user with a set of options from the beginning, instead of presenting everything and then reducing the amount of information. Furthermore situations with exhaustive searches can be avoided by presenting the user a limited amount of information. Most tasks had two to three alternatives which were compared against each other in order to find the best dialogue, which was then chosen to be the baseline for the standardized flowcharts.

\subsection{Interaction Flowcharts}\label{subsection:interaction_flowcharts}
Building on the previous chapters the possible human-computer-interactions were formalized into flowcharts. The type of flowchart is defined in \textit{DIN 66001} \parencite{hering_programmablaufplan_1984}. The flowcharts are essential as a reference for the implementation, as they define the details of the information flow between the two parties. The goal is to have interactions that have a clear start and end point, with no dead ends - leveraging flowcharts for this allows for an easy validation of these goals. Another goal was to separate the application into smaller, more manageable pieces, each defined by its own flowchart. \autoref{fig:flowchart_browse_data} shows a flowchart that defines the flow of interaction for the process of browsing classified images (task 1 from \autoref{table:function_specification}), with the background of the presented interaction dialogue. For each of the tasks defined in \autoref{table:function_specification} and \autoref{table:interdependency_analysis_table} such a flowchart was developed as seen in \nameref{appendix:interaction_flowcharts}.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/figures/flowcharts/browse_train_data_for_chosen_class.png}
    \caption{Flowchart - Browse Training Data for a given Class}
    \label{fig:flowchart_browse_data}
\end{figure}

\section{Interface Design}\label{section:interface_design}
Building on the functional specification and the interaction design an interface design was developed to encompass the whole array of functionality into a single application. Similar to the flowcharts the application interface will be split up into single elements of interaction as defined by the tasks and interaction flowcharts. This separation allows for a better overview and a more flexible layout. Additionally inspiration was drawn from the state-of-the-art application from Clearbox as seen in \autoref{section:state_of_the_art}, where a similar approach was chosen. The concept of splitting the application into smaller pieces also facilitates the usability of the whole application, as the user will not be overburdened with all functionalities simultaneously. Instead the user will be able to choose which functionality he wants to utilize. This layout goes hand in hand with the guidance concept, as every functionality will be represented by its own part of the user interface. This is an important aspect to consider, especially regarding the reusability of software components. Furthermore, preventing multiple implementations of the same functionality helps with the comparability of the guided versus non-guided version of the application.

The application is conceived for devices with relatively big screen sizes, such as desktop computers, laptops or tablets as there was no use case found for a mobile application (see \autoref{chapter:analysis}). Additionally the screen size is needed for the user to properly view the image content. As such the layout of the application will be vertically scrollable, optimized for landscape orientations to match the devices and the use case.

\autoref{fig:mockup_part_1}, \autoref{fig:mockup_part_2} and \autoref{fig:mockup_part_3} show a mockup of the application, designed as a single page layout with all functionalities present. The layout presented here corresponds to the unguided version. On the top of the application (\autoref{fig:mockup_part_1}) general information about the model, its capabilities and standard metrics are shown. Further down (\autoref{fig:mockup_part_2}) the data browsing functionalities are depicted. Lastly the the similarities and the experiment functionalities are shown at the bottom of the application (\autoref{fig:mockup_part_3}). Furthermore the order of the components is determined by the level of involvement needed of the user: The beginning is limited to higher level information, which then transforms into a deeper insight into the training data, model limitations and clustering of data, and concludes in the input-output experiment where the user can test its mental model against the actual AI.

The general design of the application revolves around a simple modern layered design in the form of cards with rounded corners. This aims to make the single components easily distinguishable while preventing the introduction of unnecessary visual separations. The color theme of the application is chosen to be neutral with high contrast between text and background while utilizing highlighting colors for important interface components for good readability and easy orientation. Although this mockup heavily references Clearbox' design language, a additional dark mode with inverted colors is considered depending on the user preferences and image content.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/figures/mockups/mockup_1.png}
    \caption{Mockup Part 1}
    \label{fig:mockup_part_1}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/figures/mockups/mockup_2.png}
    \caption{Mockup Part 2}
    \label{fig:mockup_part_2}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/figures/mockups/mockup_3.png}
    \caption{Mockup Part 3}
    \label{fig:mockup_part_3}
\end{figure}

\section{Conclusion on the Conception}
Building on the analysis, the conception conceived possible solutions to meet the requirements of the user, the usage context and the overall goals. By referencing the concrete use cases and user requirements seven functionalities were derieved (\autoref{table:function_specification}), which were then analysed for interdependencies in a HCI scenario (\autoref{table:interdependency_analysis_table}), resulting in a clear specification of tasks, sub tasks and capabilities required to perform those for either party of the interaction. Furthermore The functionalities were described in detail in \autoref{subsection:explanation_techniques} and insights were collected about the mapping of the functionalities to the explanation techniques, while providing a semantic grouping for those techniques. Although many explanations techniques have a direct mapping, the visual explanation was overrepresented and therefore one from three options was chosen. Having the functional specification set, allowed for the development of a system architecture (\autoref{section:interface_design}), which is constructed with flexibility in mind and leveraging modern web based technologies while still meeting the requirements of an AI assessment system. Through a conceptional iteration via an expert workshop the ideas were further discussed and deepened, resulting in interaction dialogues (\autoref{subsection:interaction_dialogues}) for the seven tasks and subsequently in interaction flowcharts (\autoref{subsection:interaction_flowcharts}) defining the concrete flow of information between the interacting parties. Finally those functional components were manifested in an interface design (\autoref{section:interface_design}), which leverages the interaction design and the idea of guided versus non guided interaction styles to provide a easy to understand GUI for users with low expertise in machine learning. While taking clear reference to Clearbox's design concept and language, the concept here aims to provide two different GUIs that build on the same flexible components, each encompassing one functionality, to be later evaluated against each other.

\newpage
\chapter{Implementation}\label{chapter:implementation}
The AI assessment system will be implemented based on the insights and conclusions from \autoref{chapter:conception}. The system to be build has to realise many tasks from the data science and machine learning domains, while it also needs to provide a GUI for user interaction, as described by \autoref{section:system_architecture}: Implementing the AI assessment system prototype as a flexible, hardware agnostic application is a main goal of the system architecture and implementation, while also meeting the interaction and design requirements as described in \autoref{section:interaction_design} and \autoref{section:interface_design}.

Taking these aspects into account, a \textit{Python} based backend implementation is preferable, as the programming language is widely used for data science and machine learning tasks, while also providing tools for web application building. Furthermore Python ranks the most popular programming language as of november 2021 and provides some of the most used and curated software libraries for data science and machine learning among all alternatives \parencite{toibe_index_python_website,numpy_github,pandas_github,pytorch_website}. The flexibility provided by Python allows the backend to be completely implemented in said language and making no compromises on the tools needed.

Being decoupled from the machine learning domain, the possible frontend implementation tool set is much more diverse: Popular frameworks for browser based GUIs are \textit{React}, \textit{Angular} and \textit{Vue} (amongst others), all providing the required functionalities in either \textit{JavaScript} or \textit{TypeScript} in combination with the classic \textit{HTML} and \textit{CSS} technologies.

Referencing \autoref{subsection:existing_apps} and Clearbox, a Python based application leveraging \textit{Streamlit} will be used for implementing the AI assessment system \parencite{streamlit_website}. While there are many different possible alternatives to implement such a flexible distributed system (for example \textit{Django} or \textit{Flask} in combination with \textit{React}), Streamlit comes with a big advantage: The ability to directly transform Python scripts into deployment ready web applications including a GUI. This ability completely invalidates the disadvantage of building a flexible web based application, as it avoids the additional effort of implementing the communication between the presentational and the logic layer of a multi-tier application. Although strictly speaking a Streamlit-based application will not be multi-tiered in its implementation as it only consists of singular python scripts leveraging the Streamlit platform, which in turn hides most of the complexity of implementing a distributed, web based application. Additionally some flexibility in implementing the actual frontend of the application is lost by using such a omnipotent library, as there is no need for a specialized GUI technology. However it is reasonable to limit the implementation complexity of an AI assessment system prototype in the scope of this thesis by leveraging the Streamlit framework.

Also in this conjuncture, it makes sense to limit the implementation complexity of a AI model to be used in the assessment system. Instead of developing a own model, a pre-trained model was chosen. The model used for the implementation of the assessment system prototype is the \texttt{densenet121-res224-rsna} model from \textcite{cohen_torchxrayvision_2021}. The model is part of an open source software library called \textit{TorchXRayVision} for working with chest X-ray datasets and deep learning models. It provides a common interface and common pre-processing chain for a wide set of publicly available chest X-ray datasets. This concrete model was trained to classify X-ray images and therefore detect a pneumonia disease. Additionally the fitting and also publicly available \textit{RSNA Pneumonia Detection Challenge} dataset was used \parencite{rsna_kaggle_website}. Leveraging a pre-trained model and a curated dataset is an important aspect in the implementation of an AI assessment system, as it perfectly resembles an application scenario for the medical domain and therefore supports the implementation of the actual assessment system in the context of this thesis. However, for a complete assessment system implementation, a functionality for importing any machine learning model and dataset would be needed - this was omitted, as it is not essential for the evaluation of AI explanation method effects on users.

The whole source code of the AI assessment system prototype can be found digitally on DVD in \autoref{appendix:dvd_contents} or online on Gitlab\todo{make repo available}. The following sections will reference parts of the source code when needed.

\section{System Architecture Implementation}
Using the Streamlit platform implies some changes to the originally conceived system architecture (see \autoref{fig:system_architecture}). The Streamlit platform allows building a whole web-based application with just Python code, and therefore eliminates the need to implement a separate frontend and the communication between the frontend and the backend. Based on the platform's focus on data science tasks, all functionalities (\autoref{table:function_specification}) can be implemented with the provided GUI elements. \autoref{fig:architecture_implementation} showcases the adapted system architecture, which leverages the Streamlit platform: The frontend shrunk to a thin client, which runs in the browser. The frontend consequently only consumes the service provided by the backend and is responsible for displaying the GUI elements to the user. Furthermore the actual implementation of the GUI elements is already provided by Streamlit: Based on the Python scripts in the backend, GUI elements are generated by Streamlit for the browser to display (see \autoref{section:interface_implementation} for details). The backend is now embedded in the Streamlit platform and makes use of its API to generate a multi-tier web application by using the provided tools. The internal structure of the backend has not changed and still includes a logic component, the AI model and a data store. The communication between the frontend and the backend is realized via the HTTP protocol as described in \autoref{section:system_architecture}.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/figures/architecture_implementation.png}
    \caption{System Architecture Implementation with Streamlit}
    \label{fig:architecture_implementation}
\end{figure}

The implementation was split up into two modules, the logic component which handles the user interaction and uses the Streamlit API and the data component which outsources some common functionality regarding data acquisition, transformation and provision. For a basic separation of concerns, these modules were split into two files, where the logic component makes use of the data component, which in turn makes use of the \textit{scikit-learn} and \textit{TorchXRayVision} libraries. Because of the heavy usage of the Streamlit platform, the total system architecture implementation is simple in comparison to a complete multi-tiered solution built on separate technological stacks.

\autoref{listing:main.py_initialization} and \autoref{listing:data_module_usage} show the utilization of the \textit{TorchXRayVision} library and the interaction of the logic module with the data module to load the AI model and RSNA data set for the assessment system to use. Take note of the \python{@st.cache} annotation, which makes use of the Streamlit API to identify data that can be cached for improved performance. This is especially useful for CSV data, that is readily displayed in the GUI. \autoref{listing:main.py_initialization} and \autoref{listing:data_module_usage} are the foundation for the backend component, which builds on top of the AI model, loaded data and Streamlit API. \autoref{table:software_versions} shows the primary software used for the implementation of the whole system.

\begin{listing}[htpb]
    \begin{minted}{python}
        model_specifier = 'densenet121-res224-rsna'
        model = xrv.models.DenseNet(weights=model_specifier)
        d_rsna = load_rsna_dataset()
        detailed_class_info = load_detailed_rsna_class_info()
        classes = detailed_class_info['class'].unique()
    \end{minted}
    \caption{Initialization of the Model and Dataset}
    \label{listing:main.py_initialization}
\end{listing}

\begin{listing}[htpb]
    \begin{minted}{python}
        @st.cache
        def load_rsna_dataset():
            d_rsna = xrv.datasets.RSNA_Pneumonia_Dataset(
                imgpath='./data/kaggle-pneumonia-jpg/stage_2_train_images_jpg',
                views=["PA", "AP"],
                unique_patients=True,
                transform=transform)
            return d_rsna

        @st.cache
        def load_detailed_rsna_class_info():
            return pd.read_csv('./data/kaggle-pneumonia-jpg/stage_2_detailed_class_info.csv')
    \end{minted}
    \caption{Functions for Image and Meta Data Acquisition}
    \label{listing:data_module_usage}
\end{listing}

\begin{table}[htbp]
    \centering
    \begin{tabularx}{0.3\textwidth}{ l l }
        \toprule
        software & version \\
        \midrule
        Python & 3.9.6 \\
        captum & 0.4.1 \\
        numpy & 1.21.2 \\
        pandas & 1.3.3 \\
        scikit-learn & 1.0.1 \\
        streamlit & 1.2.0 \\
        torch & 1.9.1 \\
        torchxrayvision & 0.0.32 \\
        \bottomrule
    \end{tabularx}
    \caption{Used Software and Versions}
    \label{table:software_versions}
\end{table}

\section{Interface Implementation}\label{section:interface_implementation}
Based on the concepts of \autoref{section:interaction_design} and \autoref{section:interface_design} the interface was implemented using the tools and GUI elements of the Streamlit platform \parencite{streamlit_github}. The mainly used GUI elements were: \texttt{expanders}, \texttt{columns}, \texttt{tables}, \texttt{images}, \texttt{buttons}, \texttt{checkboxes} and \texttt{selectboxes} - all of which are readily provided. To aid the visual impression of the system, specifically for the display of Xray images, a dark interface mode was made the default, while the user still had the option to change to a bright mode.

As conceived in the conception, all functionalities were implemented as differentiated GUI components by using the \texttt{expander} element. \autoref{listing:overview_element} showcases the implementation of such an component, which makes use of the \texttt{expander} element in combination with the \texttt{columns} horizontal layout functionality. Specifying the GUI elements with Streamlit builds upon a very structured and sequential convention: Items are presented in the same order, as they are declared in the Python code (with the exception of column layouts). There are very few possibilities to specify complex layout concepts, which automatically results in a very clean and structured GUI. \autoref{fig:overview_element} shows the rendered GUI element which was specified in \autoref{listing:overview_element}. The special feature of an \texttt{expander} element allows for easy hiding or showing of the encapsulated functionality. \autoref{fig:descriptive_elements} and \autoref{listing:descriptive_elements} show this exact feature: Multiple functionalities are declared successively with the \texttt{expander} element, but only one item is chosen to be presented by the user by clicking on it.

Managing the state of the application is an important aspect, as it allows for the individualization of the user experience. This is particularly relevant for the implementation of the intention query as conceived in \autoref{section:interaction_design}. The GUI components provided by Streamlit are inherently stateful, data-driven and have a simple life cycle that refreshes on every interaction, which leads to a simple development process that is backed by the underlying data. However it is more complicated to manage state, that is not tied to specific elements and therefore exceeds the life cycle of the element. An example for this is the random choice of image samples to be displayed to the user: The random data points are sampled for each user of the application and shall only be regenerated if the user whishes to do so (see \autoref{figure:flowchart_browse_data}). Because of the simple life cycle of Streamlit GUI elements, which resets on every interaction, a separate state for some GUI elements has to be managed as seen in \autoref{listing:managing_state}. Streamlit provides a functionality to persist session state per user, which is then saved on the frontend side - this allows a interactive, stateful user experience which is backed by a common data store.

Another important aspect of the interface implementation is the use of the AI model and visual explanation techniques as conceived in \autoref{subsection:explanation_techniques}. To implement the \texttt{densenet121-res224-rsna} model the \textit{torch} library was used. Loading the AI model with this library allows for direct interaction with it, for example the input-output experiment as described in \autoref{section:functionalities}. Furthermore the usage of the \textit{captum} library allowed for the computation of visual explanations (attribution by occlusion) for the data set at hand. To increase the performance of the GUI, the explanations were generated beforehand and saved separately to the original image data including the same unique identifier. \autoref{listing:occlusion} shows the computation of occlusion samples, where each sample results in an image similar to \autoref{fig:occlusion}. The computed images reveal high value super-pixels that indicate a high relevance for the AI model. These samples are then used alongside the original Xray images in the assessment system, where the user can choose to display the context-bound visual explanation.

\begin{listing}[htpb]
    \begin{minted}{python}
        with st.expander('Overview'):
            st.subheader(f'{model_specifier}'.upper())
            overview_l, overview_r = st.columns(2)
            overview_l.text(f'Import Date: {datetime.date.today()}')
            overview_r.text('Framework: Pytorch')
    \end{minted}
    \caption{Overview GUI Element}
    \label{listing:overview_element}
\end{listing}

\begin{listing}[htpb]
    \begin{minted}{python}
        with st.expander('Overview'):
            # [...]

        with st.expander('Model Description'):
            # [...]

        with st.expander('Capabilities'):
            # [...]

        with st.expander('Standard Metrics'):
            st.subheader('Standard Metrics')
            metrics1, metrics2, metrics3, metrics4 = st.columns(4)
            metrics1.metric(label='Accuracy', value=str(metrics['accuracy'].round(2)))
            metrics2.metric(label='Precision', value=str(metrics['precision'].round(2)))
            metrics3.metric(label='Sensitivity', value=str(metrics['recall'].round(2)))
            metrics4.metric(label='F1', value=str(metrics['f1'].round(2)))
    \end{minted}
    \caption{Descriptive GUI Elements}
    \label{listing:descriptive_elements}
\end{listing}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/screenshots/overview_element.jpg}
    \caption{Overview GUI Element}
    \label{fig:overview_element}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/screenshots/descriptive_elements.jpg}
    \caption{Descriptive GUI Elements}
    \label{fig:descriptive_elements}
\end{figure}

\begin{listing}[htpb]
    \begin{minted}{python}
        def set_browse_indices(high):
            st.session_state.indices = np.random.randint(low=0, high=high, size=10)
        # [...]
        if 'indices' not in st.session_state:
            set_browse_indices(len(dataset.index))
        index_list = st.session_state.indices
        df_samples = dataset.loc[index_list]
    \end{minted}
    \caption{Managing State}
    \label{listing:managing_state}
\end{listing}

\begin{listing}[htpb]
    \begin{minted}{python}
        occlusion = Occlusion(model)

        for index, row in df_predictions.iterrows():
            patient_id = row['patientid']

            patient_index = d_rsna.csv[d_rsna.csv['patientid'] == patient_id].index.values[0]
            print(patient_index)

            model_input = d_rsna[patient_index]['img']
            model_input = np.expand_dims(model_input, axis=0)
            model_input = torch.from_numpy(model_input).float()

            pred_label_idx = model(model_input).argmax()

            attr = occlusion.attribute(model_input,
                                    strides=(1, 30, 30),
                                    target=pred_label_idx,
                                    sliding_window_shapes=(1, 30, 30),
                                    baselines=0,
                                    show_progress=True
                                    )

            plt.imshow(model_input[0, 0, :, :])
            plt.contourf(attr[0, 0, :, :], alpha=0.5)
            plt.colorbar()
            plt.savefig(f'./data/kaggle-pneumonia-jpg/occlusion/{patient_id}.jpg', bbox_inches='tight', dpi=150)
            plt.clf()
    \end{minted}
    \caption{Computing Attribution through Occlusion}
    \label{listing:occlusion}
\end{listing}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/occlusion.jpg}
    \caption{Attribution through Occlusion}
    \label{fig:occlusion}
\end{figure}

\section{Conclusion on the Implementation}
Many of the concepts presented in \autoref{chapter:conception} were realized in the AI assessment system prototype, while some aspects grew to be differently than originally conceived, due to technical compromises. The previous chapter showcased the some important aspects of developing a flexible, multi-tier web application with the focus on the functionalities defined formerly. The technologies used for this implementation are mainly Python and the Streamlit framework, which offer a exceptionally well match for the task at hand. By leveraging the data-science oriented framework, a GUI for common web browsers could be realized directly from the Python scripts working directly on the AI model and data. The complexities of developing a distributed web application could be largely avoided by using the Streamlit platform - this also shows in the adjusted system architecture. However some flexibility in implementing the actual GUI design was lost, due to the already provided GUI elements. Instead of developing custom interactive elements with a specialized frontend technology, the prepackaged elements were used. The functionalities from \autoref{section:functionalities} could all be implemented with the exception of functionality number three. The absence of this functionality is caused by the failure to find a suitable algorithm to find commonalities in XRay images to be applied to this data set. Additionally the actual design of the GUI is also predefined by the Streamlit framework, while still maintaining clear similarities to the concept of \autoref{section:interface_design}.

\newpage
\chapter{Dialogue Samples}\label{chapter:dialogue_samples}
Based on the main use case (see \autoref{section:use_cases}), the realized application will be presented in the following chapter. Leveraging a common use case, allows for an exemplary tour through the AI assessment system. The baseline is the unguided version, which gets supplemented by the guided version. Additionally mobile versions (smartphone \& tablet) will be showcased exemplarily. The application was implemented in english (shown here) and german (used for the evaluation). All versions of the application (english / german / guided / unguided) can be found digitally in \autoref{appendix:dvd_contents} or the Github repository.

\autoref{fig:samples_l_all} shows the starting point for the interaction. Opening the application in the browser leads to this landing page. The page presents the user with all functionalities in the unguided version. Alternatively the guided user gets a slightly different starting point as shown in \autoref{fig:samples_l_overview_guided}, as the guided version hides the functionalities behind the intention query on top. Following the unguided interaction flow, the user is now free to explore all functionalities by clicking on an expander GUI element. A typical starting point for the user are the first three elements as shown in \autoref{fig:samples_l_overview}. \autoref{fig:samples_l_browse} shows a possible next interaction, where the user decides to browse the AI training data. The figure highlights the focus on class-based filtering and a small sample size, that can be regenerated. The user is able to select a data point and view the image data. Additionally the user can supplement the displayed information with metadata as shown in \autoref{fig:samples_l_browse_metadata}. By opening or closing the expander elements, the user has the freedom to shape the vertical layout of the application. Closing the other elements allows for a better overview and navigation as shown in \autoref{fig:samples_l_experiment}, while focusing on the current functionality. The input-output experiment may be a interesting next interaction point for the user, because it was commonly requested as described in \autoref{section:user_analysis}. To further deepen the understanding of the AI, the user might want to see how the AI came to its results. \autoref{fig:samples_l_experiment_occlusion} shows the visual explanation method (see \autoref{subsection:explanation_techniques}), which the user can use to gain additional information about the AI's reasoning. \autoref{fig:samples_l_experiment_guided} shows the alternative guided experiment functionality for comparison. Finally the user might want to explore some more detailed information about the data set and therefore go to the data clustering functionality as depicted by \autoref{fig:samples_l_data_cluster}, where the user can gain insights about statistical patterns recognized in the meta data. Although not all functionalities were used, such can be a exemplary user scenario. Furthermore the application is designed to be responsive in regards to the used device. \autoref{fig:samples_m_limits} shows the previously omitted model limitations functionality on a medium sized device, such as a tablet. Using such a device has the advantage of a possible vertical screen orientation and therefore being able to present more information vertically. \autoref{fig:samples_m_experiment} supports this statement, where the user can interact with the whole experiment functionality in contrast to \autoref{fig:samples_l_experiment}. However further reducing the screen size leads to a cluttered interface: \autoref{fig:samples_s_overview_metrics} and \autoref{fig:samples_s_experiment_browse} show the overview, metrics, browse and experiment functionality on a smartphone.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/screenshots/samples/large/l_all.png}
    \caption{Dialogue Sample - All Functionalities (large display)}
    \label{fig:samples_l_all}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/screenshots/samples/large/l_overview_guided.png}
    \caption{Dialogue Sample - Guided Overview Functionalities (large display)}
    \label{fig:samples_l_overview_guided}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/screenshots/samples/large/l_overview.png}
    \caption{Dialogue Sample - Overview Functionalities (large display)}
    \label{fig:samples_l_overview}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/screenshots/samples/large/l_browse.png}
    \caption{Dialogue Sample - Browse Functionality (large display)}
    \label{fig:samples_l_browse}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/screenshots/samples/large/l_browse_metadata.png}
    \caption{Dialogue Sample - Browse Functionality with Metadata (large display)}
    \label{fig:samples_l_browse_metadata}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/screenshots/samples/large/l_experiment.png}
    \caption{Dialogue Sample - Experiment Functionality (large display)}
    \label{fig:samples_l_experiment}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/screenshots/samples/large/l_experiment_occlusion.png}
    \caption{Dialogue Sample - Experiment Functionality with Attribution by Occlusion (large display)}
    \label{fig:samples_l_experiment_occlusion}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/screenshots/samples/large/l_experiment_guided_active.png}
    \caption{Dialogue Sample - Guided Experiment Functionality (large display)}
    \label{fig:samples_l_experiment_guided}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/screenshots/samples/large/l_data_clusters.png}
    \caption{Dialogue Sample - Data Clustering Functionality (large display)}
    \label{fig:samples_l_data_cluster}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/screenshots/samples/medium/m_limits.PNG}
    \caption{Dialogue Sample - Data Clustering Functionality (medium display)}
    \label{fig:samples_m_limits}
\end{figure}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/screenshots/samples/medium/m_occlusion.PNG}
    \caption{Dialogue Sample - Experiment Functionality (medium display)}
    \label{fig:samples_m_experiment}
\end{figure}

\begin{figure}
    \centering
    \subfigure[]{\includegraphics[width=0.4\textwidth]{img/screenshots/samples/small/s_overview.PNG}} 
    \subfigure[]{\includegraphics[width=0.4\textwidth]{img/screenshots/samples/small/s_metrics.PNG}} 
    \caption{(a) Overview Functionality (small display) (b) Metrics Functionality (small display)}
    \label{fig:samples_s_overview_metrics}
\end{figure}

\begin{figure}
    \centering
    \subfigure[]{\includegraphics[width=0.4\textwidth]{img/screenshots/samples/small/s_experiment.PNG}} 
    \subfigure[]{\includegraphics[width=0.4\textwidth]{img/screenshots/samples/small/s_browse.PNG}} 
    \caption{(a) Experiment Functionality (small display) (b) Browse Functionality (small display)}
    \label{fig:samples_s_experiment_browse}
\end{figure}

\newpage
\chapter{Summative Evaluation}\label{chapter:evaluation}
Following the principles of a human centered development of software, a summative evaluation of the AI assessment prototype was conducted. This is important to assess the efficiency, effectivity and quality of the conceived solutions from conception to implementation. Using scientific questionnaires and a comprehensible procedure is key to gain valuable insights on the effect of the system interaction on the user. Additionally the summative evaluation is the benchmark for the discussion of the research questions. The following sections will give insights into the concrete goal, the used methods, the study design and the results of the evaluation.

\section{Goal}
The summative evaluation is a final assessment of the development presented in \autoref{chapter:implementation}. In the context of the thesis it is important to study the conrete effects of the AI assessment system protoype on the user regarding information processing, trust and workload. These three aspects are directly derived from the research questions and the goals of the thesis (see \autoref{section:goals}). Assessing understandability and trustworthiness of AI models through the usage of an assessment system and evaluation with potential users of such system is the main goal. The focus lies on the effects of interactive explanations of an AI model and the underlying training data on the user groups. The study also aims to generate insights into the differences of the presented interaction styles. The results of the evaluation will create a foundation for further discussion and research on this topic, especially regarding the human factors in the interaction of humans and computers respectively intelligent system, such as AI models.

\section{Methods}
To gain insights on the effects of the AI assessment system on the potential users a on-site experiment was conducted. The study was realised in german, because the place of evaluation was the university of Lübeck, where most participants are only fluid in the german language. To assess the effects of the AI assessment system and the different interaction modes (guided vs. unguided) a mixed study including within-subject and between-subject aspects was developed aiming at approximately 30 subjects. Concretely the effects of the independent variables (interaction with the AI assessment system and guided vs. unguided interaction) on the depentend variables (Subjective Information Processing Awareness (\textbf{SIPA}), Trust and Workload) were studied.

It is imporant to mention the focus on subjective measures in this study, as the main aspects of trustworthiness and explainability of AI models are also highly subjective. Because of this, there was mainly subjective data collected with the exception of one objective performance measure. The complete evaluation questionnaire can be found in \autoref{appendix:evaluation_questionnaire}.

\subsection{Design}
The study was designed to incorporate within-subject and between-subject components in an one-to-one on-site experiment. The within-subject component is the interaction with the system, since all subjects interacted with the assessment system. The between-subject component is the interaction style (guided vs. unguided), as the participants were split into two groups of the same size, one interacting with the guided version while the other interacting with the unguided version. \autoref{fig:Study_design} showcases the study design with the subject components and the data acquisition points: After the content-related introduction the participants completed the first questionnaires, which act as the foundation for comparing the information processing and trust before and after the interaction. During the interaction with the system, the time spend on the interaction was measured as a indication of objective workload (with a cut off at 15 minutes). After the interaction the participants completed the last questionnaires. This design allows for comparison of various subjective characteristics before and after the assessment system interaction while also measuring differences between the interaction styles in an overall duration of approximately 30 minutes.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{img/study_design.png}
    \caption{Study Design}
    \label{fig:Study_design}
\end{figure}

\subsection{Participants}
The participants of the evaluation were chosen to be medical students, as this is largely in line with the main user group as described in \autoref{section:user_analysis}. However medical professionals, such as interviewed in \autoref{subsection:interviews}, are preferable but are particularly difficult to make appointments with. Since the study aimed to evaluate approximately 30 participants in a short time, a high turn around count was important. To further increase the initiative of participating in the study a raffle was set up, drawing four winners rewarded with 50€ each.

Initially it was conceived that besides medical professionals also data scientists should be a user group. However already in \autoref{section:user_analysis} there was a lack of interview partners from the data science domain. Due to this, the whole conception shifted to focus the medical users. Because of this, the evaluation will only include medical students.

Altough the study was announced on multiple channels (university internal platform and research partner contacts) two weeks before the three week long execution period the overall participation was low. In total, \textit{N} = 14 people participated in the study - this may be partially attributed to the currently ongoing pandemic situation. All participants were medical students from the university of Lübeck with an age ranging from 21 to 32, 6 of which were male and 8 female. The lowest semester recorded was 5th and the highest 11th. All participants stated having at least a fundamental knowledge of radiology, but have a very mixed level of affinity for technology interaction. \autoref{table:evaluation_participants} compiles the basic information of the 14 participants.

\begin{table}[htbp]
    \centering
    \begin{tabularx}{0.45\textwidth}{ l c c c }
        \toprule
        & age & semester & ATI score \\
        \midrule
        \textit{N} & 14 & 14 & 14 \\
        \textit{M} & 24.36 & 8.07 & 3.91 \\
        \textit{SD} & 3.34 & 2.36 & 0.83 \\
        \textit{Min} & 21 & 5 & 2.11 \\
        \textit{Max} & 32 & 11 & 5.33 \\
        \bottomrule
    \end{tabularx}
    \caption{Descriptive Statistics of Basic Information on the Evaluation Participants}
    \label{table:evaluation_participants}
\end{table}

\subsection{Setting and Instruments}
\paragraph{Setting}\mbox{} \\
The experiment was conducted on site at the university of Lübeck. For the timeframe of the evaluation the usability lab was provided by the Institute for Multimedia and Interactive Systems (IMIS), where the study was carried out. The room includes two working spaces for one person each equiped with computers and one working space for three people. The co-working space was used for the introduction of the participants, while the single working spaces were used for the interaction with the assessment system and the completition of questionnaires. The actual interaction was carried out on a 2016 MacBook Pro (i5, 16GB RAM, MacOS 12.0.1, 13"), where the whole application was running (backend and frontend). The web browser used for displaying the GUI was Safari (version 15.1). Only one participant at a time was evaluated; Special requirements in the form of a hygiene concept had to be met due to the ongoing pandemic situation.

\paragraph{Application usage}\mbox{} \\
The participants were presented with the application as implemented in \autoref{chapter:implementation} - either with the guided or the unguided version (see \autoref{chapter:dialogue_samples}). The system was configured and the application was already running when the participants arrived. Additionally it was made sure, that the participants could use the peripherals to interact with the GUI in the Safari browser. After the inital contact, the participants were allowed to freely interact with the application for a maximum of 15 minutes.

\paragraph{Questionnaires}\mbox{} \\
The study included questionnaires to be answered before and after the interaction with the system. The questionnaires used in this study were:
\begin{itemize}
    \item Basic Demographic Query
    \item Subjective Information Processing Awareness Scale (SIPA) \parencite{schrills_sipas_2021} (pre \& post)
    \item Facets of System Trustworthiness (\textbf{FOST}) \parencite{franke_advancing_2015} (pre \& post)
    \item Affinity for Technology Interaction (ATI) \parencite{franke_personal_2019} (pre)
    \item Explanation Satisfaction Scale (\textbf{ESS}) \parencite{hoffman_metrics_2019} (post)
    \item NASA Task Load Index (\textbf{NASA-TLX}) \parencite{hart_nasa-task_2006} (post)
\end{itemize}
The whole compiled questionnaire with all sub questionnaires and explanatory texts in german can be found in \autoref{appendix:evaluation_questionnaire}. The questionnaire results were digitalized and aggregated in an excel sheet to include all answers per participant (each participant got a row in the sheet identified by a pseudonym). Per participant the results  of each sub-questionnaire were evaluated by calculating the mean with regards to inverted items, as described by the original authors \parencite{schrills_sipas_2021,franke_personal_2019,franke_advancing_2015,hoffman_metrics_2019,hart_nasa-task_2006}. Each resulting mean value was appended in its own column.

\subsection{Procedure}\label{subsection:procedure}
The participants were invited individually to the study in the usability lab of the university of Lübeck. Firstly the participants were greeted and the necessary covid-19 measures were taken - only vaccinated, recovered or tested people could participate under strict hygienic requirements. After the greeting, the briefing started. The briefing included a spoken and written thematic introduction and a brief overview of the procedure. Additionally the participants got to read an explanation of the AI model used, the standard metrics and the assessment system. When the briefing was completed without open questions, the participants needed to consent to a data privacy agreement. This marks the start of the data acquisition, which takes place on paper (printed version of \autoref{appendix:evaluation_questionnaire}). The first half of the questionnaire included demographic, ATI, SIPA and FOST. SIPA and FOST here are referring to the explanation of the AI model in the briefing. After completing the first half, the interaction with the assessment system was started. The participants were asked to take place in front of the prepared laptop device with the instructions:

\begin{displayquote}
    "Your goal is to gather insights about the model’s training data, strenghts and limitations. Try to understand how the model is working through the interaction with the assessment system. For this you have 15 minutes. If you think that you will not be able to make any further discoveries before the 15 minutes are up, feel free to stop the interaction. If there are any questions or errors, feel free to ask for help."
\end{displayquote}

During the interaction the time was recorded. After the interaction the participants got to complete the second half of the questionnaire including SIPA, FOST, ESS and NASA-TLX. This marks the end of the data acquisition through the written questionnaire. After the completition, the debriefing started with the participants being asked if there were things that went not so well, that struck them or created questions. Furthermore the participants were asked if they have comments on things that they liked about the interaction or general comments. The data from the spoken debriefing was written down to complement the quantitative data. The study ended with a debriefing and a farewell.

\section{Results}\label{section:evaluation_results}
The descriptive statistics of the results are depicted in \autoref{table:evaluation_descriptive} and form the baseline for further analysis. These descriptive statistics can additionally be grouped up by the participant's interaction mode (guided vs. unguided), which is showcased in \autoref{fig:evaluation_descriptive_boxplots}, \autoref{table:evaluation_descriptive_guided} and \autoref{table:evaluation_descriptive_unguided}. When comparing the pretest to the posttest scores, the statistics already suggest that the interaction had effects on the understandability and trust and that those two are connected. The objective time measured for the interaction was no indicator of the workload of the participants, as almost all used the full 15 minutes - for this the NASA-TLX score is more insightful as the unguided group has an overall higher task load index. Also the explanation satisfaction is overall higher in the unguided participant group.

To further evaluate the sampled data a repeated measures analysis of variance (\textbf{ANOVA}) was conducted, in which the subjects are measured more than once to determine whether statistically significant change has occurred \parencite{vogt_dictionary_2011}. Because there are two independent variables (interaction \& guidance) a two-way ANOVA was applied to the dataset from \autoref{table:evaluation_descriptive} for the dependent variables SIPA and trust (FOST). \autoref{table:anova_within_sipa}, \autoref{table:anova_between_sipa}, \autoref{table:anova_within_trust} and \autoref{table:anova_between_trust} showcase the results of the ANOVA. The results show a statistically significant change on the SIPA within-subject effect and the trust within-subject effect. However the between-subject effects do not display a statistically significant change. Although trust increased, SIPA decreased in the posttest compared to the pretest. Also the correlation between SIPA and trust increased through the interaction with the system as depicted in \autoref{table:correlation_matrix_post} compared to \autoref{table:correlation_matrix_pre}. Interestingly ATI does not affect any other variable.

As mentioned in \autoref{subsection:procedure} there was also qualitative data recorded after the interaction. Overall the assessment system was well received and most participants found it interesting to get insights into an AI model. The input-output experiment functionality was commonly stated to be a good functionality. Additionally the data browsing and grouping based on meta data was stated to be very informative. More controversial aspects were the actual performance of the AI model, which most expected to be higher, and therefore were surprised. Furthermore the participants were very mixed about the visual explanation method (attribution by occlusion); some said it helped greatly to understand the reasoning of the AI, while other did not find it useful at all. Aspects that were regarded as negative by some was a missing technological explanation on the concepts of image classifiers. Additionally many participants were somewhat confused by the focus on pneumonia, as many stated that X-ray images can give insights on many more pathologies.

\begin{table}[htbp]
    \centering
    \begin{tabularx}{\textwidth}{ l c c c c c c }
        \toprule
        & SIPA (pre) & FOST (pre) & SIPA (post) & FOST (post) & ESS & NASA-TLX \\
        \midrule
        \textit{N} & 14 & 14 & 14 & 14 & 14 & 14 \\
        \textit{M} & 3.88 & 4.37 & 4.26 & 3.80 & 3.21 & 6.67 \\
        \textit{SD} & 0.77 & 0.99 & 0.91 & 0.81 & 0.76 & 2.03 \\
        \textit{Min} & 2.00 & 1.40 & 1.67 & 1.80 & 1.38 & 4.00 \\
        \textit{Max} & 5.17 & 5.80 & 5.17 & 4.80 & 4.25 & 11.00 \\
        \bottomrule
    \end{tabularx}
    \caption{Descriptive Statistics of all Results}
    \label{table:evaluation_descriptive}
\end{table}

\begin{table}[htbp]
    \centering
    \begin{tabularx}{\textwidth}{ l c c c c c c c }
        \toprule
        & SIPA (pre) & FOST (pre) & SIPA (post) & FOST (post) & ESS & NASA-TLX \\
        \midrule
        \textit{$n_g$} & 7 & 7 & 7 & 7 & 7 & 7 \\
        \textit{M} & 3.67 & 3.94 & 3.82 & 3.44 & 2.73 & 6.34 \\
        \textit{SD} & 1.01 & 1.19 & 1.06 & 0.94 & 0.70 & 1.94 \\
        \textit{Min} & 2.00 & 1.40 & 1.67 & 1.80 & 1.38 & 4.00 \\
        \textit{Max} & 5.17 & 5.00 & 4.50 & 4.20 & 3.50 & 10.20 \\
        \bottomrule
    \end{tabularx}
    \caption{Descriptive Statistics of guided Results}
    \label{table:evaluation_descriptive_guided}
\end{table}

\begin{table}[htbp]
    \centering
    \begin{tabularx}{\textwidth}{ l c c c c c c c }
        \toprule
        & SIPA (pre) & FOST (pre) & SIPA (post) & FOST (post) & ESS & NASA-TLX \\
        \midrule
        \textit{$n_u$} & 7 & 7 & 7 & 7 & 7 & 7 \\
        \textit{M} & 4.10 & 4.80 & 4.71 & 4.17 & 3.70 & 7.00 \\
        \textit{SD} & 0.41 & 0.54 & 0.45 & 0.47 & 0.48 & 2.21 \\
        \textit{Min} & 3.50 & 4.20 & 4.00 & 3.60 & 3.00 & 4.00 \\
        \textit{Max} & 4.67 & 5.80 & 5.17 & 4.80 & 4.25 & 11.00 \\
        \bottomrule
    \end{tabularx}
    \caption{Descriptive Statistics of unguided Results}
    \label{table:evaluation_descriptive_unguided}
\end{table}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{img/figures/boxplots.png}
    \caption{Boxplots of Questionnaire Results}
    \label{fig:evaluation_descriptive_boxplots}
\end{figure}

\begin{table}[htbp]
    \centering
    \begin{tabularx}{0.7\textwidth}{ l c c c c c c }
        \toprule
        & \textit{SS} & \textit{df} & \textit{MS} & \textit{F} & \textit{p} & ${\eta^2}_p$ \\
        \midrule
        Time & 1.016 & 1 & 1.016 & 5.41 & .038* & .311 \\
        Time $\ast$ Guidance & 0.397 & 1 & 0.397 & 2.11 & .172 & .150 \\
        Residual & 2.254 & 12 & 0.188 & & & \\
        \bottomrule
        \multicolumn{3}{l}{\footnotesize * $\textit{p}<.05$, ** $\textit{p}<.01$, *** $\textit{p}<.001$} \\
    \end{tabularx}
    \caption{ANOVA - Within Subjects Effects on SIPA}
    \label{table:anova_within_sipa}
\end{table}

\begin{table}[htbp]
    \centering
    \begin{tabularx}{0.6\textwidth}{ l c c c c c c }
        \toprule
        & \textit{SS} & \textit{df} & \textit{MS} & \textit{F} & \textit{p} & ${\eta^2}_p$ \\
        \midrule
        Guidance & 3.11 & 1 & 3.11 & 2.90 & .114 & .195 \\
        Residual & 12.86 & 12 & 1.07 & & & \\
        \bottomrule
    \end{tabularx}
    \caption{ANOVA - Between Subjects Effects on SIPA}
    \label{table:anova_between_sipa}
\end{table}

\begin{table}[htbp]
    \centering
    \begin{tabularx}{0.75\textwidth}{ l c c c c c c }
        \toprule
        & \textit{SS} & \textit{df} & \textit{MS} & \textit{F} & \textit{p} & ${\eta^2}_p$ \\
        \midrule
        Time & 2.2857 & 1 & 2.2857 & 6.4516 & .026* & .350 \\
        Time $\ast$ Guidance & 0.0229 & 1 & 0.229 & 0.0645 & .804 & .005 \\
        Residual & 2.254 & 12 & 0.188 & & & \\
        \bottomrule
        \multicolumn{3}{l}{\footnotesize * $\textit{p}<.05$, ** $\textit{p}<.01$, *** $\textit{p}<.001$} \\
    \end{tabularx}
    \caption{ANOVA - Within Subjects Effects on Trust}
    \label{table:anova_within_trust}
\end{table}

\begin{table}[htbp]
    \centering
    \begin{tabularx}{0.6\textwidth}{ l c c c c c c }
        \toprule
        & \textit{SS} & \textit{df} & \textit{MS} & \textit{F} & \textit{p} & ${\eta^2}_p$ \\
        \midrule
        Guidance & 4.48 & 1 & 4.48 & 4.24 & .062 & .261 \\
        Residual & 12.67 & 12 & 1.07 & & & \\
        \bottomrule
    \end{tabularx}
    \caption{ANOVA - Between Subjects Effects on Trust}
    \label{table:anova_between_trust}
\end{table}

\begin{table}[htbp]
    \centering
    \begin{tabularx}{0.65\textwidth}{ l l c c c }
        \toprule
        & & SIPA (pre) & FOST (pre) & ATI \\
        \midrule
        \multirow[t]{2}{*}{SIPA (pre)} & \textit{r} & --- & & \\
        & \textit{p} & --- & & \\
        \multirow[t]{2}{*}{FOST (pre)} & \textit{r} & .57* & --- & \\
        & \textit{p} & .03 & --- & \\
        \multirow[t]{2}{*}{ATI} & \textit{r} & .13 & -.04 & --- \\
        & \textit{p} & .66 & .89 & --- \\
        \bottomrule
        \multicolumn{5}{l}{\footnotesize * $\textit{p}<.05$, ** $\textit{p}<.01$, *** $\textit{p}<.001$} \\
    \end{tabularx}
    \caption{Correlation Matrix (pre scores)}
    \label{table:correlation_matrix_pre}
\end{table}

\begin{table}[htbp]
    \centering
    \begin{tabularx}{0.9\textwidth}{ l l c c c c c }
        \toprule
        & & ATI & SIPA (post) & FOST (post) & ESS & NASA-TLX \\
        \midrule
        \multirow[t]{2}{*}{ATI} & \textit{r} & --- & & & & \\
        & \textit{p} & --- & & & & \\
        \multirow[t]{2}{*}{SIPA (post)} & \textit{r} & .10 & --- & & & \\
        & \textit{p} & .739 & --- & & & \\
        \multirow[t]{2}{*}{FOST (post)} & \textit{r} & -.02 & .78** & --- & & \\
        & \textit{p} & .94 & $<.01$ & --- & & \\
        \multirow[t]{2}{*}{ESS} & \textit{r} & -.03 & .71** & .68** & --- & \\
        & \textit{p} & .91 & $<.01$ & $<.01$ & --- & \\
        \multirow[t]{2}{*}{NASA-TLX} & \textit{r} & .50 & -.34 & -.38 & -.29 & --- \\
        & \textit{p} & .07 & .23 & .18 & .31 & --- \\
        \bottomrule
        \multicolumn{7}{l}{\footnotesize * $\textit{p}<.05$, ** $\textit{p}<.01$, *** $\textit{p}<.001$} \\
    \end{tabularx}
    \caption{Correlation Matrix (post scores)}
    \label{table:correlation_matrix_post}
\end{table}

\section{Discussion}\label{section:evaluation_discussion}
The statistical analysis in \autoref{section:evaluation_results} has shown, that the interaction with the assessment system had a significant effect on the understandability (through SIPA) and the perceived trustworthiness (through FOST) of the AI model. However the effects go the opposite way: Understandability increased through the interaction, but trustworthiness decreased. This is a interesting finding, because it highlights a particular effect on the user: Through a better understanding, the user's trust in the model was calibrated and the user perceived the model as less trustworthy. Remarkably, the SIPA and FOST pretest scores, which are based only on the textual explanation of the evaluation questionnaire (see \autoref{appendix:evaluation_questionnaire}) are quite high. The within-subject effects support the answer of the research questions Q1 \& Q2: For this AI model and data set, SIPA and trust are correlated. Additionally the correlation increased through the interaction with the assessment system. Therefore the presented functionalities and explanation methods are suitable to calibrate and possible optimize the trust levels through the interaction (and not only increase it). By supplementing the qualitative data, especially the the input-output experiment and the data browsing functionality were beneficial to the moderate SIPA and trust. On the other hand, the visual explanation was controversially perceived and might not be suitable to explain an AI model to medical professionals. Furthermore the qualitative data shows, that participants from this sample want more technical details about image classifiers - whether this is a task of the assessment system remains questionable.

Comparing the between-subject effects, no statistical significance was found. However a tendency for lower effects on the guided group were observed compared to the unguided group. This leads to the question, whether an open interaction style leads to a higher perceived feeling of understandability. \textcite{sedig_role_2001} already explored the effects of scaffolding on cognition in learnware, and found that HCI artifacts can extend or, inadvertently, limit human cognition and thought processes. There seems to be a connection between the cognitive load in a learning context and the between-subject effects observed in this evaluation. However, because of the missing significance, the effects of guidance and scaffolding in XAI should be further studied with a focus on the distinction between content-related versus non-content-related aspects of the GUI as proposed by \textcite{sedig_role_2001}. Based on these inconclusive results, research question Q3 cannot be clearly answered: Stronger effects in the unguided group suggest, that scaffolding and guidance can impede the perceived understandability - altough the small sample size might negatively influence the results of the variance analysis.

While it is not possible to discriminate the effects of each explanation method used by this study design, the explanation satisfaction and the SIPA posttest score are also correlated. With the limitation of the missing discrimination of the explanation methods used, research question Q4 can be partially answered by the correlation between ESS and SIPA: The display of structured meta data, in addition to the image data, was part of most explanation methods (and the main focus of one method). This leads to the assumption that the presentation of meta data does significantly increase the users understanding of the AI model's operational range and performance. Referencing the qualitative data, many participants even whished for more meta data, such as clinical data of the patient.

Interestingly ATI, a key facet of user personality, does not have any correlation with the other variables. Seemingly the tendency to actively engage in intensive technology interaction has no effects on the information conveyed to the user by the system, as neither SIPA or trust are correlated with it. Altough it is conceivable that the small sample, only consisting of young medical students with relatively high ATI scores, distort this relation.

Lastly the perceived workload was overall higher in the unguided group compared to the guided group. This indicates that the guidance helped with workload, even thought it does not correlate with neither SIPA or trust. However no meaningful measure of objective workload could be observed. A higher perceived workload in the unguided group, combined with the tendency towards a stronger effect on SIPA and trust, might indicate similar psychological effects as described in the \textit{Elaboration Likelihood Model} by \textcite{petty_elaboration_1986}. If one takes the the rather high ATI score of the participants into consideration a connection between a higher need for cognition and a higher effect on SIPA and trust is conceivable and invites to further research.

\section{Conclusion on the Evaluation}
Through a summative evaluation the effects of the conceived implementation on the user could be measured. An on-site experiment study design including within- and between-subject factors delivered statistically significant results. By applying a repeated measure ANOVA it was discovered how the interaction with an AI assessment system affected understandability and trustworthiness of AI models: While SIPA increased, trust decreased. The correlation between these two variables increased after the interaction. The interaction with the assessment system delivered statistically significant effects on SIPA and trust. However, guidance did not yield significant results, even though the effect size suggests that SIPA gets impeded by the guidance. While the sample size was smaller than originally desired, the results remain viable but incomplete; three of four research questions could be addressed with room for further studies, especially on the topic of guidance in XAI.

\newpage
\chapter{Summary and Outlook}
The following sections give a summarized overview of the thesis, its results and outstanding issues, while also discussing directions for further research.

\section{Summary}
Artificial intelligence is used successfully in many application fields, however medical systems cannot benefit as easily from AI based technology. This impediment stems oftentimes from the AI being a "black box". In consequence humans struggle to understand such AI-systems and their output, leading to trust and compliance issues \parencite{eu_com_ai,adadi_blackbox_2018}. Applying human-centered design research on this issue and therefore studying the effect of HCI in the context of medical AI aims to shed light on the requirements for modern AI usage in the medical field.

Through the conception, development and evaluation of an AI assessment system in cooperation with Clearbox, suitable explanation methods were found in order to impact the perceived understandability and trustworthiness of AI models for the specific user group of medical professionals. By studying literature and conducting expert interviews (see \autoref{chapter:analysis}) a scientific basis was created to conceptualize requirements, functionalities, interaction- and interface designs for an interactive AI assessment system (see \autoref{chapter:conception}). Based on this conception a functional prototype was implemented with modern web-based technologies in order to accomdate a flexible usage scenario (see \autoref{chapter:implementation}). By leveraging an open access AI model and data set, the implementation depicts an realistic application scenario for the medical domain. Following the human-centered development process, a summative evaluation of the conceived prototype with $\textit{N}=14$ participants was conducted. The evaluation consisted of an on-site experiment with medical students including within- and between-subject factors. Analysing the evaluation results yielded insights into the effects of the HCI in both subject groups (see \autoref{chapter:evaluation}). Throught the interaction with the assessment system the users were successfully able to increase their subjective understanding of the AI model, while their trust in the AI model was calibrated and decreased. Furthermore, the users interacting in a explorative and unguided manner showed a tendency towards stronger effects on subjective understanding and trust, compared to the more guided users. Moreover the evaluation results gave insights into further psychological aspects of the user and interaction, such as ATI having no relevance in the user sample and a possible connection between perceived workload and effect on subjective understanding and trust, which invites to further studies.

\section{Outstanding Issues}
Originally a more diverse user group was depicted, including medical professionals and data scientists. Already in the user analysis (see \autoref{section:user_analysis}) first issues arised regarding the quantity of information to be gathered on the user group of data scientists. During the further conception this lack of information led to a strong focus on the user group of medical professionals. The focus was maintained until the summative evaluation, where it culminated in a homogeneous medical participant group. Because of the inherent differences between the user groups, the results are only meaningful for medical professionals. Even then, the participants surveyed belonged to a restricted demographic, leading to a more difficult generalizability of the results.

\section{Outlook}
Many interesting effects could be observed during the evaluation of the AI assessment system prototype. However the low sample size led to some inconclusive results (see \autoref{section:evaluation_discussion}). Especially the comparision between guided and unguided interaction only allowed for assumptions regarding the effects on understandability and trustworthiness. However this can be a great starting point for further research, focusing on the concrete mode of HCI in the XAI context. By developing more sophisticated user interfaces and interactive explanation methods, the intricacies of guidance and scaffolding for human-AI-interaction could be further explored. Additionally many other feasible explanation methods, such as the omitted comparative explanations or different visual explanations could be implemented and the effects on the users studied. Complemented with a more diverse participant group and additional measures for understandability and trustworthiness, the presented results can be used as a foundation for a further research on HCI in the XAI context.

\section{Final Conclusion}
With the all pervasive use of artificial intelligence in current technological advances, the XAI research domain grows ever since. Although explaining AI can be done in many ways, a modern interactive version was conceived for this thesis. Leveraging the knowledge of the HCI domain and combining it with current XAI literature, tools and research partners allowed for an insightful process of conceptualizing, implementing and evaluating an interactive AI assessment interface in the context of medical applications. Considering the potential user's needs and requirements throughout the entire development process is an important part of modern application development. Therefore a strong focus on the human aspects yielded expressive evaluation results, which depict clear connections between interactive explanations, understandability and trustworthiness of AI models. However, the concepts of understandability and trustworthiness are very complex and cannot be captured in their entirety by such a study - but reviewing the results of the thesis gives great insights on HCI in the context of XAI and invites for further research.

\clearpage
\include{appendix}
\end{document}
