\documentclass[11pt,a4paper,english]{scrreprt}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage[long,nodayofweek,level,24hr]{datetime} 
\usepackage[skip=5pt,font=footnotesize]{caption}

\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{enumitem}
\usepackage{pdfpages}
\usepackage{xcolor}
\usepackage[bookmarksnumbered]{hyperref}
\hypersetup{pdfborder={0 0 0}, breaklinks=true}
\usepackage{bookmark}
% \addtokomafont{disposition}{\rmfamily}
\usepackage[onehalfspacing]{setspace}

% DISABLE TODO NOTES HERE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{todonotes}
%\usepackage[disable]{todonotes}

% HEADER & FOOTER DEFINITIONS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{fancyhdr}
\input{page-style}

% LITERATURE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[backend=biber,style=apa]{biblatex}
\addbibresource{literatur.bib}


% DEF FOR COVER %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\titelMAEnglish}{Explainable Artificial Intelligence:\\ Designing human-centric assessment system interfaces to increase explainability and trustworthiness of artificial intelligence in medical applications}

\newcommand{\authorMA}{Philipp Dominik Bzdok}

\newcommand{\examinerMA}{Univ.-Prof. Dr. rer. nat. Thomas Franke, Dipl.-Psych.}

\newcommand{\supporterMA}{Tim Schrills, M.Sc.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% DEF FOR COMMENTS, CAN BE DELETED %%%%%%%%%%%%%%%%%%%%%%%%
\newenvironment{comment}
  {\par\medskip
   \begingroup\color{olive}%
   }
 {\endgroup
  \medskip}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\pagenumbering{gobble}
\input{deckblatt}
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

\input{preamble}
\tableofcontents

\newpage
\pagenumbering{arabic}
\chapter{Introduction}\label{chapter:introduction}
The use of modern Artificial Intelligence (\textbf{AI}) techniques is pervasive and can be found in many fields of application, such as digital image processing, search engines and speech recognition \parencite{eu_com_ai}. Other application fields, such as medical diagnosis systems, cannot benefit as easily from AI-based technology compared to recreational domains. This impediment stems oftentimes from the AI being a "black box". In consequence humans struggle to understand such AI-systems and their output, leading to trust and compliance issues \parencite{adadi_blackbox_2018}. These issues are further enhanced in the medical context where decisions, possibly based on AI, can have severe consequences for users, especially patients.

An example for medical human-AI-interaction is the image-based recognition of Deep Vein Thrombosis (\textbf{DVT}) with real time AI support for medical professionals by \textit{Think\-Sono}. The system leverages AI to guide the user through the current gold-standard diagnosis, a compression ultrasound examination, so that it enables any healthcare professional to detect DVT \parencite{thinksono_website}. Closely related in this context is the interdisciplinary research project \textit{CoCoAI}, which aims to explore psychological, ethical and technological implications of human-centered, AI-based applications in the DVT diagnosis and beyond \parencite{cocoai_website}. 

When AI-based systems are used in high risk application contexts, such as medical diagnosis, the aspects of explainability, interpretability and trustworthiness become a primary concern for adoption and use of said system. \textcite{ribeiro_why_2016} already explored the importance of explainability and trust in AI-based systems and postulated that AI systems will not be used if the users have no trust in the model or the results. Even though many machine learning algorithms score high on standard performance metrics, such as precistion, recall or Area Under the Receiver Operating Characteristics (\textbf{AUROC}), user-facing performance may be way worse \parencite{gordon_disagreement_2021}. Understanding the AI's underlying machine learning (\textbf{ML}) model and its predictions is an important step for assessing trust and facilitating effective interaction \parencite{ribeiro_why_2016}. Recent technological advances are realized by \textit{Clearbox AI}, with the focus on trustworthy AI by implementing an AI model assessment \parencite{clearbox_website, eu_trustworthy_ai}. The model assessment can help model owners to identify robustness issues, potential undesired behaviour, and explain errors and uncertainties regarding the model predictions \parencite{clearbox_wp}.

Trust in AI systems is primarily induced by the users' understanding and the general interpretability of the machine learning model and their predictions \parencite{ribeiro_why_2016, ras_explanation_2018}. The wide array of different possible user groups and the complex constructs of understandability and trust demands for a human-centric approach in designing AI assessment systems. Because of the inherent complexity of non-linear machine learning models, especially Deep Neural Networks (\textbf{DNNs}) for image processing, suitable visualization and communication techniques are non-trivial. Additionally to the complex models for image classification, the input data is also more complex as it is unstructured. Non-linear neural networks and unstructured data provide additional challenges for Explainable Artificial Intelligence (\textbf{XAI}), as described in \textcite{keane_how_2019}. XAI is a research field that studies how AI decisions and data driving those decisions can be explained to people in order to provide transparency, enable assessment of accountability, demonstrate fairness, or facilitate understanding \parencite{arrieta_explainable_2019}. XAI plays an important role in the acceptance and finally in the usage of AI-based technology. This is further underlined in the medical context where public authorities set strict regulations on the usage of technological systems and ethic concerns have to be thoughtfully addressed.

In the context of a image-based medical diagnosis system, it is important that the responsible stakeholders, such as medical practitioners, specialized doctors and clinic managements, are enabled to make informed decisions on the usage of AI-based technology, even though their expertise in machine learning and data science is expected to be low. The stakeholders' trust in this system is a primary factor for the widespread use of said technology for real life applications. Therefore, increasing the understanding of the AI model and finding an optimal trust level in the predictions by designing human-centric explanation techniques within the AI model assessment system is a main goal of this work. Additionally it is conceivable that authorities will instantiate auditors for AI-based systems in medical contexts. Having a comprehensible and scientifically proven assessment system could be a big step in the approval and adoption of said system. 

\section{Goals}
The users understanding of the AI model and trust in the model are highly essential as pointed out by \textcite{knapic_explainable_2021}. This holds especially true for medical applications where re-traceable results have to be provided and people acting on these results bear great responsibility. To facilitate understanding and trust the machine learning model has to be interpretable and explainable. In the context of Convolutional Neural Networks (\textbf{CNNs}) interpretability of models can pose a significant concern because of their inherent complexity. Explanations of AI-models can provide insights on the machine's decision process and therefore generate user understanding. This can lead to the model being more interpretable by humans.

Assessing the suitability and performance of a CNN for a specific task by applying standard performance evaluation metrics is problematic, since these can be oblivious to distinguishing the diverse problem solving behaviors of a neural network \parencite{lapuschkin_unmasking_2019}. \textcite{samek_explaining_2021,JMLR:v17:15-618,ribeiro_anchors_2018} give an overview on the technical foundations of XAI and a presentation of practical methods, which will be used in conjunction with human-centric design to explore and evaluate suitable and efficient methods to explain a model's classification.

The goal of this thesis is to design, develop and evaluate interactive AI-assessment-system artifacts for medical professionals and machine learning specialists in a human-centric fashion to facilitate understandability and trustworthiness of AI-models. Developing such a system, with human concerns in focus, leads to following research questions:
\begin{itemize}
    \item[Q1:] How is the stakeholders' (medical professionals, clinic managements or data scientists) subjective information processing awareness linked to trust for a specific model and its predictions in the medical domain?
    \item[Q2:] How can different explanation techniques, ranging from perturbation based approaches to more model intrusive alternatives, increase trust in image classifier models and predictions?
    \item[Q3:] What are the most efficient methods to explain and possibly optimize trust levels in image classification models?
    \item[Q4:] To what extend can structured metadata increase the stakeholder's understanding of a model's operational range and performance?
\end{itemize}

\section{State of the Art}
An AI-assessment-system is currently offered by Clearbox AI. The \textit{AI Control Room} cloud platform enables users to assess, improve and validate ML models and data in accordance with the principles of Trustworthy AI \parencite{clearbox_website,eu_trustworthy_ai}. \textcite{clearbox_website} describes its AI-assessment-system as a "Deep Preâ€‘production Analysis" tool:
\begin{displayquote}
    "AI Control Room automatically generates a model assessment to help model owners to identify robustness issues, potential undesired behaviour, and explain errors and uncertainties regarding the model predictions."
\end{displayquote}

Concretely the product enables users to perform following tasks for AI models working on tabular data:
\begin{description}
    \item [Model behaviour validation:] Validation metrics and plausible causes of error are clearly presented, potential limitations and irreducible uncertainty are identified and local explanations of the model behaviour are generated selecting representative points in the dataset.
    \item [Synthetic data generation:] A generative model can be used to create synthetic data points that preserve the statistical properties of the original dataset. These points can augment the original training set to improve generalization, to increase model robustness, and to oversample specific labels when in the presence of unbalanced data.
    \item [Data-centric analysis:] Generative models perform a probabilistic analysis of the underlying data allowing for robust outliers detection and uncertainty analysis. This information can help you to evaluate data quality.
    \item [Centralised tracking system:] AI Control Room acts as a centralised tracking system to store lineage, versioning, and metadata of your datasets and models. Assessments generated are securely persisted along with models and datasets.
\end{description}

Besides general information and standard metrics (see \autoref{fig:model_assessment_overview}) the assessment system offers varied insights into different aspects of the machine learning model: \autoref{fig:model_assessment_graphs} shows graphs of training and validation precision, recall and calibration, while \autoref{fig:model_assessment_analysis} shows the models strong points and limitations by analyzing the feature distribution in the data. Furthermore, the second half of the model assessment focuses more on the interpretability aspect of machine learning: \autoref{fig:model_assessment_interpret} shows a confusion matrix of possible classification results, which is then extended by example data points, chosen by the assessment system (see \autoref{fig:model_assessment_examples}). These examples can then be further explored to generate understanding of the models inner workings by applying an attribution based explanation technique combined with a decision rule explanation.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/screenshots/model_assessment_overview.png}
    \caption{AI Control Room - Model Assessment Overview with Standard Metrics}
    \label{fig:model_assessment_overview}
\end{figure}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/screenshots/model_assessment_graphs.png}
    \caption{AI Control Room - Precision-Recall and Calibration Graphs}
    \label{fig:model_assessment_graphs}
\end{figure}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/screenshots/model_assessment_analysis.png}
    \caption{AI Control Room - Model String Points and Limitations}
    \label{fig:model_assessment_analysis}
\end{figure}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/screenshots/model_assessment_interpret.png}
    \caption{AI Control Room - Interpretability Assessment}
    \label{fig:model_assessment_interpret}
\end{figure}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/screenshots/model_assessment_examples.png}
    \caption{AI Control Room - Example Data}
    \label{fig:model_assessment_examples}
\end{figure}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/screenshots/model_assessment_explanation.png}
    \caption{AI Control Room - Prediction Explanation for Examples Data}
    \label{fig:model_assessment_explanation}
\end{figure}
\clearpage

\section{Approach}
As already described in the introduction of \autoref{chapter:introduction}, many machine learning algorithms score high on standard performance metrics, but user-facing performance may be way worse \parencite{gordon_disagreement_2021}. This issue is caused by real world applications being very dependent on the actual human-AI-interaction. Following this reasoning, it lends itself to utilize a human-centered design process for creating AI-assessment systems. \autoref{fig:DIN_EN_ISO_9241} shows a standardized process of human-centered design, which was applied in this thesis to conceptualize, implement and evaluate assessment system artifacts. The key take-away is the inclusion of human aspects in all stages of the process. Research on evaluation of AI explanations revealed that there is a big gap between the perceived and actual usefulness of explanations, as described by \textcite{ras_explainable_2021}. This further underlines the need for a human-centered approach in designing AI-assessment-systems.

The thesis' structure will reflect the human-centered approach, which is visualized in \autoref{fig:DIN_EN_ISO_9241}: As already alluded in \autoref{chapter:introduction}, \autoref{chapter:analysis} is about understanding and setting the context of use by conducting literature research and user interviews. Based on the established requirements \autoref{chapter:conception} will describe the conception of functionalities and interaction design. The development of solutions will be described in \autoref{chapter:implementation}, while \autoref{chapter:evaluation} is about the evaluation of the solutions. This general process is embedded in a iterative loop, where intermediate results are evaluated against the requirements and subject to change.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/figures/DIN_EN_ISO_9241-210.png}
    \caption{Human-centered Design Process \parencite{DIN}}
    \label{fig:DIN_EN_ISO_9241}
\end{figure}

\newpage
\chapter{Analysis}\label{chapter:analysis}
\begin{comment}
    \textbf{Context => Problem / Task => User => Organisational:} "Black-box DNNs have become pervasive in todays society and represent a proven and indispencible machine learning tool. Such models for image classification are able to detect various disease patterns with medical imaging and can be used by medical professionals support diagnosis and increase efficiency and effectivity if trusted and used correctly. Medical professionals with low expertise in machine learning need to be enabled to assess ML applications before using it on patients to build trust and facilitate good results."
\end{comment}

Following the human-centered design process, it is important to incorporate the potential users from the beginning. This is also reflected in the analysis, where the context and setting of use has to be understood and set. Besides the human factors, there are also more general and theoretical aspects to be analysed, such as the context of AI in the medical application domain for specific tasks, such as classifying disease patterns with medical imaging.

In the following the different aspects to be analysed will be layed out and described as a foundation for the human-centered design process and the conception of system artifacts.

\section{Data Sources}
Three main data sources where used for the analysis, ranging from general scientific literature about XAI to specially elaborated user interviews and cooperation with developers of an existing application.

\subsection{Scientific Literature}
Literature is the foundation of the analysis. As described by \textcite{mueller_explanation_2019}, the amount of scientific publications on the topic of explanation in intelligent systems has surged in the last 5 years, revealing many important and relevant information on this subject area through openly accessible papers. In the beginning of the thesis (July 2021) a general search on \textit{Google Scholar} was conducted to gain a overview on available publications. A non-exhaustive list of search terms at the time was:
\begin{itemize}
    \item XAI
    \item XAI in Medical Applications
    \item Explainable Artificial Intelligence
    \item Explainable Artificial Intelligence in Medical Applications
    \item Explainable Machine Learning
    \item Interpretable Machine Learning
    \item Explaining Black-Box Machine Learning Models
    \item Explaining DNNs
\end{itemize}
This general research yielded already good results, as there were many relatively new and popular publications on the topic of XAI, such as \textcite{mueller_explanation_2019, ras_explanation_2018, adadi_blackbox_2018}.

The results of the internet research were then further reinforced by academic partners from the University of LÃ¼beck, with whom related research was conducted in the context of the \textit{CoCoAI} project. Leveraging the available resources and support from research partners boosted the yield on relevant scientific literature tenfold. Over the course of multiple months the list of literature grew and is still being maintained in a shared \textit{Zotero} library \parencite{Zotero_website}. The most important scientific papers for this analysis were: \textcite{ras_explanation_2018, arrieta_explainable_2019, ribeiro_why_2016, adadi_blackbox_2018,knapic_explainable_2021, samek_explaining_2021, chiou_trusting_2021}. 

\subsection{Interviews}
Complementing the general research on XAI, specially elaborated user interviews where conducted. These interviews specifically target medical professionals and data scientists. Interviews with the actual user group of a potential solution is fundamental to understanding and setting the context and requirements of use. The participants for the interviews were chosen with following requirements in mind:
\begin{description}
    \item[Medical Professionals:] Has interest and/or knowledge in AI-systems; has worked with or researched AI-systems in the medical domain; can judge the benefits and risks of the use of AI in medical applications.
    \item[Data Scientists / AI Researchers:] Is familiar with the XAI topic; has interest in explainability and trustworthiness of machine learning models; has worked with AI in the medical context.
\end{description}
Participants for the interviews were gathered via academic partners, internet research and word of mouth. In total 16 suitable people from 6 different institutions (among them UKSH, TU MÃ¼nchen and Mevis Frauenhofer Bremen) were contacted about potential interviews. Ten leads were medical professionals while six leads where data scientists or AI researchers. From the total of 16 potential interview partners only four interviews have been conducted. This low yield is due to the time constraint of the individuals, who are mainly full time medical practitioners or researchers. The participants are described in further detail in \autoref{table:interview_participants}.

\begin{table}[htbp]
    \centering
    \begin{tabularx}{\textwidth}{ l l l X X l }
        \toprule
        ID & Age & Gender & Occupation & Education Level & AAII Score \\
        \midrule
        1 & 28 & male & Assistant Physician in Neuroradiology & State Examination & 4.89 \\ 
        2 & 24 & female & Research Associate (ML) & Masters Degree & 5.12 \\ 
        3 & 48 & male & Surgeon & Dr. med. & 5.45 \\ 
        4 & 27 & female & Assistant Physician in Neuroradiology & State Examination & 4.67 \\ 
        \bottomrule
    \end{tabularx}
    \caption{Interview Participants}
    \label{table:interview_participants}
\end{table}

The Interviews were conducted in german and executed as 1 to 1 online interviews. For reference the interviews were recorded if consent was present. Additionally the interviews were supported by a research colleague, who kept protocol. After the interviews the recordings were transcribed for further analysis and the participants were asked to answer the \textit{Affinity for AI Interaction} (\textbf{AAII}) questionnaire, which is a modified version of the \textit{Affinity for Technology Interaction} (\textbf{ATI}) questionnaire \parencite{franke_personal_2019}. ATI aims to determine the tendency to actively engage in intensive technology interaction, as a key personal resource for coping with technology. Analogously the AAII questionnaire aims to determine the tendency to actively engage in AI interaction.

In terms of content, the interviews for medical professionals and data scientists were slightly different as seen in \autoref{table:interview_topics}. The reason for this is the heterogenous expertise on the subject area of machine learning models and potential user requirements. The whole interview guideline can be found in \autoref{appendix:interview_guideline}.
\begin{table}[htbp]
    \centering
    \begin{tabularx}{\textwidth}{ l l }
        \toprule
        Medical Professionals & Data Scientists / AI researchers \\
        \midrule
        Actual Usage of AI & Actual Usage of AI \\ 
        Perspective on AI Usage & Comparison of AI Models \\  
        Trust in AI & Perspective on AI Usage \\
        Potential Problems with AI Usage & Trust in AI \\
        Own Explanation Techniques & Potential Problems with AI Usage \\
        Familiarity with XAI & Own Explanation Techniques \\
        Assessment on Local Explanations & Familiarity with XAI \\
        Information Processing & Need for Local Explanations \\
        Reliability vs. Trust vs. Understanding & Need for Global Explanations \\
         & Information Processing \\
         & Trust-Behavior Connection \\
         & Reliability vs. Trust vs. Understanding \\
        \bottomrule
    \end{tabularx}
    \caption{Interview Topics}
    \label{table:interview_topics}
\end{table}

After gathering the interview data via protocols and transcripts a thematic analysis according to \textcite{braun_thematical_2006} was applied to identify common topics and codes. The thematic analysis is a widely used qualitative analysis method mainly found in the field of psychology and can be used as a primary tool to access data from interviews. Applying this method resulted in a thematic map showcasing common overlapping topics found in the interviews, which can be seen in \autoref{fig:thematic_mind_map}. thematic map serves as the baseline for further context and requirement analysis.

\begin{figure}[htbp]
    \centering
    \includegraphics[height=0.8\textwidth, angle=90]{img/figures/Thematic_Mind_Map.pdf}
    \caption{Thematic Mind Map}
    \label{fig:thematic_mind_map}
\end{figure}

\subsection{Existing Applications}

\section{Context Analysis}
\begin{comment}
Der Kontext bezieht sich auf den \textbf{rÃ¤umlichen} und \textbf{zeitlichen Kontext}, in dem Ihre Entwicklung eingesetzt wird (z. B. \textbf{Informations-Website:} auf Desktop-Computern, Notebooks und MobilgerÃ¤ten sowohl zu Hause als auch unterwegs und zu jeder Uhrzeit vs. S\textbf{achbearbeitungssoftware in der Ã¶ffentlichen Verwaltung:} auf Arbeitsplatz-Computern, im BÃ¼ro, zwischen 9 und 17 Uhr) Aus diesen Kontexten ergeben sich unterschiedliche Nutzungsanforderungen, die fÃ¼r die Konzeption relevant sind. Beziehen Sie diese Informationen bei der weiteren Entwicklung explizit mit ein.

Wenn Sie Aussagen Ã¼ber den Kontext machen, dann legen Sie auch Ihre Quellen offen â€” woher wissen Sie das? Z. B. aus Interviews, der Literatur, etc.
\end{comment}

Text \dots

\section{Problem Analysis}
\begin{comment}
Problem- und Aufgabenanalyse wird hÃ¤ufig kombiniert, kÃ¶nnen aber auch in zwei separate Unterkapitel aufgeteilt werden.

\textbf{Problemanalyse:} Welche(s) Problem(e) adressieren Sie? Machen Sie es konkret. Verwenden Sie empirische Daten, z. B. Ã¼ber die HÃ¤ufigkeit des Problems. Machen Sie es plastisch mit einem typischen Szenario. Weisen Sie in der weiteren Arbeit explizit darauf hin, wie Ihre Entwicklung diese Probleme adressiert.
\end{comment}

Text \dots

\section{Task Analysis}
\begin{comment}
\textbf{Aufgabenanalyse:} Welche Aufgaben sollen die Benutzer/innen (besser) ausfÃ¼hren kÃ¶nnen? Wodurch sind diese gekennzeichnet? Bei kleineren klar definierten Aufgaben bieten sich z. B. HTAs an. Nehmen Sie in der weiteren Arbeit explizit Bezug auf die Aufgaben und die durch die Entwicklung neu vorhandenen MÃ¶glichkeiten bzw. bessere DurchfÃ¼hrung.
\end{comment}

Text \dots

\section{User Analysis}
\begin{comment}
Wer sind die Benutzer/innen Ihrer Entwicklung? Beschreiben Sie die Benutzergruppen oder Benutzerklassen. In \textbf{seltenen(!)} FÃ¤llen kÃ¶nnen sich Personas eignen (sind eher ein Kommunikationstool im Team). Die Zielgruppe mÃ¼ssen Sie im Verlauf Ihrer weiteren Arbeit immer wieder explizit einbinden â€” nutzen Sie sie um Ihre GestaltungslÃ¶sungen immer wieder an deren Eigenschaften zu messen. In Konzeption, Realisierung, und Evaluation muss deutlich werden, dass Sie spezifisch fÃ¼r diese Personen und ihre Besonderheiten entwickelt haben.

Teilen Sie unterschiedliche Benutzerklassen in Unterabschnitte auf, z. B. "2.4.1 Verwaltungsmitarbeiter", "2.4.2 BÃ¼rger". Es sollten nicht mehr als ca. drei Benutzerklassen bedient werden.
\end{comment}

Text \dots

\section{Organisational Analysis}
\begin{comment}
Hier geht es um den \textbf{organisationalen Kontext}, also z. B. um Hierarchien, AbhÃ¤ngig-keiten zwischen den Nutzern/innen und anderen Personen/Organisationen, Rechte in Unternehmen, und andere Rahmenbedingungen, die Sie berÃ¼cksichtigen mÃ¼ssen.

Nehmen Sie als Beispiel Ihre Situation als Studierende bei der Entwicklung einer Studiengangs-App â€” Sie sind im Kontext UniversitÃ¤t mit Dozierenden, UniversitÃ¤tsleitung, Studiengangs-Koordination, entsprechenden Studien- und PrÃ¼fungsordnungen etc. unterwegs. Das mÃ¼ssen Sie bei der Entwicklung berÃ¼cksichtigen.

Oder wenn es um eine Software fÃ¼r eine Klinik geht â€” und die IT den Einsatz von Java verbietet. Wenn Sie das nicht in Erfahrung gebracht haben und dann eine Java LÃ¶sung entwickeln, bekommen Sie erst viel zu spÃ¤t heraus, dass Ihre Entwicklung nicht verwendet werden kann (yup, realer Fall â€” fÃ¼r die Klinik-IT ist das Java-Verbot normal und nicht der Rede wert, die Nutzer wissen davon oft nicht â€” oder gar was Java ist). BTW, versuchen Sie nicht, eine Organisation zu Ã¤ndern â€” Organisationsentwicklung ist grausam und dafÃ¼r werden sie nicht bezahlt.

Auch hier: Nehmen Sie auf diese Punkte explizit in der weiteren Entwicklung Bezug!
\end{comment}

Text \dots

\section{Conclusion on the Analysis}
\begin{comment}
Stellen Sie zum Ende der Analyse in einem eigenen Fazit mit eigener Gewichtung die zentralen Inhalte der Analyse kurz dar. Zur UnterstÃ¼tzung kann eine Tabelle mit einer Priorisierung der Nutzungsanforderungen verwendet werden. Sie legen \textbf{auf Basis der Analyse} begrÃ¼ndet fest, welche Nutzungsanforderungen:

\begin{enumerate}
    \item absolut notwendig erfÃ¼llt werden mÃ¼ssen (mÃ¼ssen Sie lÃ¶sen, sonst kann das Ziel der Anwendung nicht erreicht werden, d.h. sie ist nicht effektiv),
    \item wichtig zu erfÃ¼llen sind (hoher Einfluss auf Effizienz, Zufriedenstellung, Erlernbarkeit),
    \item was "nice-to-have" ist (geringer Einfluss auf Effizienz, Zufriedenstellung, Erlernbarkeit), und
    \item was eher unwichtig ist, aber die Anwendung abrundet (KÃ¼r).
\end{enumerate}

Die Anforderungen mit \textbf{1 mÃ¼ssen} Sie erfÃ¼llen, die Anforderungen mit \textbf{2 sollten} Sie erfÃ¼llen, die Anforderungen mit \textbf{3 kÃ¶nnen} Sie erfÃ¼llen, und die mit \textbf{4 kÃ¶nnen} Sie erfÃ¼llen, wenn Sie wider erwarten am Ende der Entwicklungsphase noch was Zeit haben.

Falls bei der Festlegung der Nutzungsanforderungen viel Entscheidungsspielraum vorliegt â€” z. B. weil die Analyse keine klaren PrioritÃ¤ten nahelegt, dann kÃ¶nnen Sie die Priorisierung auch zu Beginn der Konzeption darstellen. In der Konzeption haben Sie mehr gestalterische Freiheiten als in der Analyse. Ein eigenes Fazit sollten Sie am Ende der Analyse aber in jedem Fall schreiben.
\end{comment}

Text \dots

\newpage
\chapter{Conception}\label{chapter:conception}
\begin{comment}
Die \textbf{Konzeption muss klar aus der Analyse folgen}. Sie kÃ¶nnen und sollten kreative LÃ¶sungen erarbeiten, aber Sie mÃ¼ssen dabei wirklich kreativ sein. Sprich: \textbf{Absichtlich etwas Erschaffen, was neu und nÃ¼tzlich ist}. Machen Sie deutlich wie Ihre kreative LÃ¶sung die Punkte aus der Analyse aufgreift und lÃ¶st. Also nicht: "Jetzt habe ich die Pflicht-Analyse gemacht, kann's abhaken, und mich frei entfalten." Die Analyse liefert die \textbf{Legobausteine} und den \textbf{Rahmen} (Bodenplatte) fÃ¼r die Konzeption â€” die mÃ¼ssen Sie nutzen. Sie kÃ¶nnen natÃ¼rlich iterativ die ihr VerstÃ¤ndnis des Nut-zungskontextes erweitern und die Nutzungsanforderungen updaten!

Stellen Sie zu Beginn der Konzeption einen \textbf{Bezug zur Analyse} her. Geben Sie dann einen Ãœberblick Ã¼ber das Konzeptionskapitel. Wie sind Sie bei der Konzeption vor-gegangen? Welche Aspekte (z. B. FunktionalitÃ¤ten, Systemarchitektur, Interface-Design, etc.) berÃ¼cksichtigen Sie? Verweise auf die Abschnitte (Abschnittsnummern verwenden!).

Zentral fÃ¼r die Konzeption ist die \textbf{formative Evaluation der GestaltungslÃ¶sung anhand der Anforderungen}. Sie wissen ja aus der Analyse wie die Nutzungsanforderungen sind â€” entsprechend immer zuerst selbst die LÃ¶sungen damit bewerten (= evaluieren). Dabei helfen auch Benutzerklassen oder (wenn sinnvoll) Personas. Aber frÃ¼hzeitig die Zielgruppe hinzuziehen, damit diese Ihnen Feedback zu Ihren bisherigen Konzeptionen gibt. Sie brauchen oft mehrere Iterationen bis Sie eine gebrauchstaugliche LÃ¶sung herausarbeiten kÃ¶nnen.

\textbf{Zuerst "Getting the Right Design", dann erst "Get the Design Right":} Viele Arbeiten sind zu unkreativ. Das heiÃŸt nicht anders um anders sein zu wollen (wir sind nicht im Marketing), sondern etwas entwickeln, was neu und nÃ¼tzlich ist. HÃ¤ufig wird mit der ersten Idee gestartet und diese iterativ verbessert. Das Ergebnis ist, dass man sich einen HÃ¼gel hocharbeitet (Abbildung linke Seite). Das fÃ¼hrt zu einem "hill climbing problem" â€” man kommt evtl. oben an, aber Ã¼bersieht, dass es andere LÃ¶sungen gibt, die einen vielleicht weiter fÃ¼hren (Abbildung rechte Seite).

\begin{figure}[htbp]
    \centering
    \begin{subfigure}
        \centering
        \includegraphics{img/Picture2.png}
    \end{subfigure}
    \begin{subfigure}
        \centering
        \includegraphics{img/Picture3.png}
    \end{subfigure}
\end{figure}

"Getting the Design Right" (links) vs. "Getting the Right Design" (rechts). Abbildungen aus \textcite{greenberg_2012}.

Stattdessen erst unterschiedliche Ideen mit sehr geringen Ã„nderungskosten (= billige und grottige Papierskizzen) explorieren, dann erst fÃ¼r die beste Idee entschei-den (auf Basis der Nutzungsanforderungen, Ihren Fertigkeiten und zur VerfÃ¼gung stehenden Zeit)!)

\begin{figure}[htbp]
    \centering
    \includegraphics{img/Picture4.png}
\end{figure}

Abbildung aus \textcite{greenberg_2012}.

Die Konzeption \textbf{kann unterschiedlich aufgebaut sein} und \textbf{nicht alle Unterkapitel sind bei jeder Entwicklung relevant}. Die Struktur muss Sinn ergeben â€” im Zweifelsfall mit dem Betreuer abstimmen. Die Unterkapitel Konzeptionsvorgehen, AnwendungsfÃ¤lle (Use-Cases) und FunktionalitÃ¤ten wird man in den meisten FÃ¤llen benÃ¶tigen. \textbf{MÃ¶gliche Reihenfolgen danach sind:}

\begin{itemize}
    \item \textbf{Iterationsweise:} Bietet sich an, wenn Sie nur einen Aspekt (z. B. nur das Interface) entwickeln. Sie beschreiben in eigenen Unterkapiteln die jeweiligen Iterationen samt formativer Evaluation, aus der sich dann die nÃ¤chste Iteration ableitet.
    \item \textbf{Strukturell gegliedert:} Aufteilung in Unterkapitel (Systemarchitektur, Interface, etc.) und Beschreibung der Iterationen dieser in den jeweiligen Unterkapiteln.
\end{itemize}
\end{comment}

Text \dots

\section{Conceptual Approach}
\begin{comment}
Die Konzeption verlÃ¤uft â€” noch stÃ¤rker als die Analyse â€” iterativ. Die Unterkapitel sind allerdings linear strukturiert. Vergleichbar mit den Datenquellen der Analyse bietet sich das erste Unterkapitel an, das Vorgehen und die eingesetzten Methoden zu beschreiben (deren Ergebnisse sich meist in mehreren Unterkapitel niederschlagen).

\textbf{Methoden} kÃ¶nnen z. B. sein:

\begin{itemize}
    \item \textbf{Analyseworkshop:} NÃ¼tzlich um die Ergebnisse der Analyse zu strukturieren und fÃ¼r die Konzeption aufzubereiten (vgl. \textcite[S. 190-192]{kumar_2013})
    \item \textbf{Design-Sprints:} StÃ¤rker fÃ¼r die Konzeption (vgl. \textcite{knapp_2016})
    \item \textbf{Workshops mit der Zielgruppe:} Generell um Ideen partizipativ zu entwickeln und formativ zu evaluieren.
\end{itemize}

Auch hier: Vorgehen beschreiben, Teilnehmer mit Alter (M, SD), Geschlecht (m/w/d) sowie relevanten Eigenschaften (warum stehen sie fÃ¼r die Zielgruppe?).

In den folgenden Unterkapitel kÃ¶nnen Sie dann auf 3.1 verweisen.
\end{comment}

Text \dots

\section{Use Cases}
\begin{comment}
Use Cases bieten sich an, die Ergebnisse aus der Analyse zusammenzufassen und in einen oder mehrere konkrete AnwendungsfÃ¤lle zu Ã¼bertragen. Die Tabelle mit den Nutzungsanforderungen sollte hierbei helfen. Wie sieht die Situation aus, wenn Ihr System eingesetzt wird? Bei den AnwendungsfÃ¤llen noch nicht zu spezifisch werden. Es geht primÃ¤r darum zu beschrieben, was die Person macht, z. B. verÃ¤nderte ArbeitsablÃ¤ufe, neue Funktionen, etc., noch nicht wie genau dies (technisch, womit realisiert) umgesetzt wird.
\end{comment}

Text \dots

\section{Functionalities}
\begin{comment}
Geben Sie einen Ãœberblick Ã¼ber die geplanten FunktionalitÃ¤ten des Systems. Dazu kÃ¶nnen Sie die AnwendungsfÃ¤lle der Konzeption sowie die Tabelle mit den Nutzungsanforderungen aus der Analyse aufgreifen und den Anforderungen mÃ¶gliche FunktionalitÃ¤ten zuordnen. Priorisieren Sie dann diese FunktionalitÃ¤ten â€” bezÃ¼glich Auswirkungen auf die Gebrauchstauglichkeit des Systems und Umsetzbarkeit dieser Funktionen.
\end{comment}

Text \dots

\section{System Architecture}
\begin{comment}
Mit den FunktionalitÃ¤ten gehen bestimmte Anforderungen an das zugrundeliegende System einher. Stellen Sie hier dar, welche LÃ¶sungen es fÃ¼r die Systemarchitektur gibt, welche Aufteilungen sich eignen (z. B. MVC) und wie man es umsetzen kÃ¶nnte. Je nach Detailgrad bieten sich u.a. UML-Diagramme an.

\textbf{BegrÃ¼ndete Auswahl:} Nehmen Sie die FunktionalitÃ¤ten als Grundlage, das gibt Ihnen vor, mit welchen mÃ¶glichen Architekturen Sie diese FunktionalitÃ¤ten umsetzen kÃ¶nnen. Falls es mehrere MÃ¶glichkeiten gibt, erlÃ¤utern Sie diese und treffen eine be-grÃ¼ndete Auswahl auf Basis dessen StÃ¤rken und SchwÃ¤chen fÃ¼r Ihren Anwendungsfall.

\textbf{Hardware/Software-agnostisch bleiben:} Es geht hier explizit nicht um spezifische Hard- oder Software. Sie bleiben auf der "Datenbank", "Server", etc. Ebene. Welche Datenbank (MySQL, PostgreSQL, SQLite, etc.) oder welcher Server (Apache, Node.js, etc.) verwendet wird kommt in der Realisierung. Auch hier: Nach der Konzeption kann der Leser stoppen und Ihre Konzeption auf seine Weise realisieren.

\textbf{Bezug zum Interface Design beachten:} Systemarchitektur und Interface Design hÃ¤ngen eng zusammen. Stimmen Sie beides aufeinander ab â€” in jeder Iteration.
\end{comment}

Text \dots

\section{Interface Design}
\begin{comment}
Wie kÃ¶nnen die Informationen dargeboten und Angaben der Nutzer erfolgen?

\textbf{Screen-Fetisch hinterfragen:} Viele Medieninformatiker haben einen Fetisch was visuelle Interface betrifft. Diese lassen sich leicht skizzieren, es gibt Tools dafÃ¼r, die man kennt, und man kann was zeigen. Man sieht, was man gemacht hat. FÃ¼r viele Kontexte sind sie allerdings absolut ungeeignet (z. B. um Autofahrer zu energiesparendem Fahren zu bringen). Visuelle Aufmerksamkeit ist extrem kostbar. Probieren Sie deswegen bewusst andere Sinne aus. KÃ¶nnen Sie die Informationen auch auditiv geben (z. B. Abweichung vom idealen Fahrverhalten Ã¼ber die TonhÃ¶he)? Oder haptisch (z. B. Vibration)? Gerade wenn man sich im Laufe der Konzeption in das eigene visuelle Design verliebt kommen oft LÃ¶sungen heraus, die vielleicht schÃ¶n sind, aber auch lebensgefÃ¤hrlich, und schlimmer noch: nicht gebrauchstauglich. FÃ¼r viele Kontexte sind visuelle Interfaces sehr gut geeignet â€” aber fÃ¼r viele auch nicht. Und die Auswahl sollte wie Ã¼blich begrÃ¼ndet erfolgen.

\textbf{Formativ Evaluieren:} Interface-Designs mÃ¼ssen Sie formativ evaluieren. FÃ¼r Sie ist die Interaktion klar (Sie wissen auch als â€” meist â€” einzige Person, was unter der OberflÃ¤che passiert und was die HintergrÃ¼nde sind), fÃ¼r Laien ist das oft unklar. Sie brauchen frÃ¼h Feedback der Zielgruppe. Nutzen Sie es! Dokumentieren Sie am Ende jeder Iteration, was herausgekommen ist und was die Konsequenzen fÃ¼r Ihr Design / die nÃ¤chste Iteration war.

\textbf{Papier Ã¼ber Mock-Up-Tools:} Verwenden Sie Papier um visuelle Interfaces zu gestalten. Wenn Sie nicht zeichnen kÃ¶nnen, um so besser. Bei mit dem Computer ausgearbeiteten Mockups ist die Hemmung der Zielgruppe oft zu hoch, noch grundlegende Aspekte zu Ã¤ndern. Damit bleiben Sie oft in der ersten â€” und oft unkreativen und nicht gebrauchstauglichen â€” LÃ¶sung gefangen. Tools wie Adobe XD sind eine Falle, die Sie zumindest in den ersten Iterationsphasen tunlichst vermeiden sollten.

\textbf{Bei visuellen Interfaces Skizzen der Iterationen darstellen:} Benutzen Sie Graphiken (Skizzen, Mockups) um das Interface und die Interaktion damit zu verdeutlichen.
\end{comment}

Text \dots

\newpage
\begin{comment}
\begin{center}
\huge
Nach der ersten Iteration:\\STOP\\
\normalsize
Nachdem Sie die erste Iteration abgeschlossen haben, nehmen Sie das Blatt Papier, auf dem Sie vor/wÃ¤hrend der Analyse Ihre LÃ¶sungsidee(n) festgehalten haben, wieder heraus. Vergleichen Sie Ihre ursprÃ¼ngliche Idee mit der ersten Iteration.

\textbf{Falls sich Ihre erste Iteration sich stark von der erste Idee unterscheidet:} Prima. Sie haben wÃ¤hrend der Analyse dazu gelernt und den Nutzungskontext besser verstanden. Ohne die Analyse und die Verwendung dieser Informationen in der Konzeption hÃ¤tten Sie das nicht machen kÃ¶nnen.

\textbf{Falls Ihre erste Iteration Ihrer ersten Idee sehr Ã¤hnlich ist:} Entweder Sie kannten sich schon vor der Analyse mit dem Themengebiet sehr gut aus â€” oder (und das ist wahrscheinlicher) â€” sie hÃ¤ngen zu stark an Ihrer ersten Idee und hÃ¶ren der Zielgruppe nicht zu. Auch falls Sie selbst zur Zielgruppe gehÃ¶ren (z. B. eine Anwendung fÃ¼r Studierende), Sie sind nicht "alle Nutzer". Nutzer unterscheiden sich Ã¼blicherweise. Entsprechend wirklich am in sich gehen â€” haben Sie aus der Analyse wirklich nichts dazu gelernt? Was das Problem (oder Sie selbst) wirklich so trivial? Das SchÃ¶ne an kreativen Prozessen ist ja, dass man iterativ besser werden kann und auch auf Ideen kommen kann, auf die man vorher nicht gekommen wÃ¤re. Die Chance sollten Sie nutzen.

BTW, dieser Hinweis heiÃŸt nicht, dass Sie komplett naiv an Ihre Gestaltung herangehen sollten, ohne sich vorher Gedanken zu machen. Aber, wenn Sie eine Analyse wirklich ernst nehmen, dann sollte die Ihnen Sachverhalte aufzeigen, die man als AuÃŸen-stehender nicht sieht. Und Sie sollten auch in der Konzeption basierend auf diesen Informationen LÃ¶sungen entwickeln, die Ã¼ber die ersten simplen Ideen hinausgehen (vgl. confirmation bias). Die neu und nÃ¼tzlich sind.
\end{center}
\end{comment}

\section{Conclusion on the Conception}
\begin{comment}
Wie auch bei der Analyse am Ende der Konzeption ein kurzes Fazit um den Entwicklungsprozess zusammenzufassen und begrÃ¼nden, was die erfolgversprechendste LÃ¶sung (hÃ¶chste Gebrauchstauglichkeit in der formativen Evaluation!) fÃ¼r die Realisierung ist.
\end{comment}

Text \dots

\newpage
\chapter{Implementation}\label{chapter:implementation}
\begin{comment}
Beschreiben Sie, wie Sie das Projekt konkret realisiert haben, mit dem Ziel, anderen (Medien-)Informatikern zu erlauben, Ihre Anwendung weiter zu entwickeln.

\textbf{Test:} Sie nehmen einen Kommilitonen, geben ihm dieses Kapitel und bitten ihm eine Funktion hinzuzufÃ¼gen oder zu verÃ¤ndern. Wenn er das kann, prima. Wenn nicht, besser schreiben oder bessere Kommilitonen suchen.

Verweisen Sie zu Beginn auf die erfolgreichste LÃ¶sung aus der Konzeption. Wie in der Konzeption bieten sich Unterkapitel mit Systemarchitektur und Interface-Design an â€” jetzt allerdings mit konkreten Angaben, welche Programme / Frameworks Sie verwenden, und wie Sie das Interface konkret umsetzen.

Geben Sie zuerst einen allgemeinen Ãœberblick und werden dann konkreter. Als wenn Sie in eine Weltkarte reinzoomen. Erst mal sagen, wie die Welt aufgebaut ist (Kontinente), dann auf Kontinent mit LÃ¤ndern zoomen, dann exemplarisch mal auf ein Land. Leser braucht ein gutes mentales Modell vom Aufbau und den Funktionen, um zu verstehen, wie es technisch umgesetzt wurde.

\textbf{BegrÃ¼nden Sie Ihre Entscheidungen linear:} Z.B.

\begin{enumerate}
    \item "das sind die Anforderungen aus der Analyse/bisheriger Konzeption"
    \item "Programmiersprache / System / etc. x, y, und z erfÃ¼llen diese Anforderungen"
    \item "relevante Kriterien sind a, b, c und d (z. B. Verbreitung, Community, Weiterentwicklung, Preis)"
    \item "Programmiersprache / System / etc. x, y, und z schneiden da so und so ab, x ist insgesamt am besten"
    \item "Programmiersprache / System / SDK / etc. x wird verwendet". Eine Argumentation Ã  la A => B => C => D => E ist nachvollziehbarer als wir nehmen E, weil A => C woraus D folgt, wegen B.
\end{enumerate}

Weiterer Grund: Wenn Sie zuerst die konkrete Software sagen, dann fangen Personen direkt an, die Entscheidung zu hinterfragen (weil sie z. B. eine andere bevorzugen, gibt da genug Religionskriege). Wenn Sie linear argumentieren gehen diese Personen eher bei jedem Schritt mit und enden dann bei der von Ihnen verwendeten Software (sofern sie bei den Auswahlkriterien und der Bewertung mitgehen).

\textbf{Nutzen Sie den DatentrÃ¤ger im Anhang der Arbeit:} Insbesondere in den Programmcode kÃ¶nnen Sie viel auslagern. In der Arbeit muss der Leser aber die Struktur/den Aufbau Ihres Programms verstehen sowie die Logik dahinter. Und er muss wissen, dass es die ErlÃ¤uterungen im Programmcode gibt und wo er diese findet. Entsprechend die Struktur (Dateien/Klassen) deutlichen machen und Verweise auf relevanten Dateien.
\end{comment}

Text \dots

\section{System Architecture Implementation}
\begin{comment}
vgl. Konzeption, jetzt aber wie und womit es konkret umgesetzt wurde (u.U. bieten sich UML-Diagramme an)
\end{comment}

Text \dots

\section{Interface Implementation}
\begin{comment}
vgl. Konzeption, jetzt aber wie und womit es konkret umgesetzt wurde 
\end{comment}

Text \dots

\section{Conclusion on the Implementation}
\begin{comment}
Kurzes Fazit, das u.a. die Frage beantwortet, was technisch konkret umgesetzt wurde und was z. B. aus der Konzeption Ã¼brig geblieben ist und warum.
\end{comment}

Text \dots

\newpage
\chapter{Dialogue Samples}
\begin{comment}
Verdeutlichen Sie die Funktionsweise Ihre Entwicklung bei einer typischen Bedienung (z. B. wie in einem Anwendungsbeispiel / Use Case beschrieben â€” auf den kÃ¶nnen Sie sich ja beziehen).

Die Abbildungen mÃ¼ssen im Text vorher erlÃ¤utert sein und einen Eindruck geben, wie die Entwicklung konkret bedient wird bzw. was sie kann.

\textbf{Das ist das, was bleibt:} Diese Dialogbeispiele sind das einzige, was in ein paar Jahren von Ihrer Anwendung noch sichtbar ist. Dann ist entweder der DatentrÃ¤ger verschwunden oder beschÃ¤digt oder die notwendige Hard- und Software ist nicht mehr lauffÃ¤hig. Geben Sie dem Leser entsprechend einen guten Einblick in das, was Sie tatsÃ¤chlich realisiert haben.

\textbf{Jetzt ein Video aufnehmen:} Schalten Sie den Screenrecorder auf dem Computer (not-falls: QuickTime) oder Smartphone (kann das OS) an und nehmen Sie die typischen Interaktionen einmal auf. Das ist Ihr "Plan B", falls (oft: wenn) im Kolloquium die App nicht bedient werden kann. Das kÃ¶nnen Sie auch gut auf DVD brennen um Lesern die MÃ¶glichkeit geben, sich die Interaktion anzusehen (in der Arbeit darauf hinweisen!). Macht sich auch gut auf Websites (insbesondere dem eigenen Portfolio).
\end{comment}

Text \dots

\newpage
\chapter{Summative Evaluation}\label{chapter:evaluation}
\begin{comment}
Geben Sie zu Beginn der Evaluation einen kurzen Ãœberblick Ã¼ber Ihr Vorgehen. Dazu reichen meist die ZwischenÃ¼berschriften mit ein oder zwei SÃ¤tzen, was Sie konkret gemacht haben. Also nicht "In Design wird das Design beschrieben" sondern "Das experimentelle Vorgehen wird im Abschnitt Design dargestellt".
\end{comment}

Text \dots

\section{Goal}
\begin{comment}
Die summative Evaluation ist eine abschlieÃŸende Bewertung Ihrer Entwicklung. Ziel ist, unter dem Strich zu sehen, wie gebrauchstauglich Ihr System ist (nicht mehr eine iterative Verbesserung wie in der formativen Evaluation der Konzeption). Da-fÃ¼r mÃ¼ssen Sie klare Kriterien ableiten, was eine "gute" bzw. "schlechte" Bewertung nach sich ziehen wÃ¼rde. Meist sind das die klassischen Gebrauchstauglichkeitskriterien (EffektivitÃ¤t, Effizienz, Erlernbarkeit, Zufriedenstellung), wobei diese Ã¼ber das Ziel Ihrer Anwendung konkretisiert werden (EffektivitÃ¤t bei einer Lernapp ist konkret gemessen etwas anderes als EffektivitÃ¤t bei einem digitalen Depressionstagebuch).

Hier verdeutlichen Sie entsprechend, welche Fragen die Evaluation beantworten soll.
\end{comment}

Text \dots

\section{Methods}
\begin{comment}
Im Methodenteil zeigen Sie, was wie evaluiert wurde. Der Methodenteil muss anderen Entwicklern die MÃ¶glichkeit geben, Ihr Evaluationsvorgehen zu wiederholen um Ihre Ergebnisse zu Ã¼berprÃ¼fen. 
\end{comment}

Text \dots

\subsection{Design}
\begin{comment}
Kurze Beschreibung des Versuchs- oder Evaluationsdesigns, dass man das Vorgehen einordnen kann. Also z. B. "es wurde ein Usability Test durchgefÃ¼hrt", oder "es wurde ein Experiment mit between-subjects design durchgefÃ¼hrt, bei dem die Kontrollgruppe die bisherige App verwendet hat und die Experimentalgruppe die neu entwickelte App".
\end{comment}

Text \dots

\subsection{Participants}
\begin{comment}
Kurze Beschreibung der Teilnehmer mit relevanten Angaben. In jedem Fall die Anzahl, oft noch Geschlecht, Alter, Beruf, Vorerfahrung, etc. Bei weniger als zehn Teilnehmern bietet sich eine Tabelle zur schnellen Ãœbersicht an. Verweisen Sie bei individuellen Ergebnissen (z.B. Zitaten aus FragebÃ¶gen oder Interviews) auf die Teilnehmer-Nummer.

Achtung: Niemals die Namen der Teilnehmer erwÃ¤hnen! Die Teilnehmer stehen stellvertretend fÃ¼r die Zielgruppe. Wer sie konkret sind ist irrelevant. BegrÃ¼nden Sie, warum Sie diese Personen ausgewÃ¤hlt haben (spiegeln die Nutzergruppe gut wider) und wo/wie Sie diese rekrutiert haben.
\end{comment}

Text \dots

\subsection{Setting and Instruments}
\begin{comment}
Beschreiben Sie die notwendigen Materialien bei der Evaluation. Dazu gehÃ¶rt â€” mit Ãœberschriften klar ausgewiesen â€” das Setting (wo wurde die Evaluation durchgefÃ¼hrt), Ihre Entwicklung (Verweis auf Dialogbeispiele), und Ihre Erhebungsmethoden (FragebÃ¶gen, InterviewleitfÃ¤den, etc.).

Falls Sie etablierte FragebÃ¶gen verwenden (z.B. ATI) reicht die entsprechende Zitation mit einer kurzen Beschreibung. Bei lÃ¤ngeren FragebÃ¶gen oder InterviewleitfÃ¤den nennen Sie kurz die Abschnitte (z.B. soziodemographische Daten, Technikerfahrung, etc.) und verweisen Sie auf die vollstÃ¤ndigen FragebÃ¶gen im Anhang.

\textbf{ACHTUNG:} Zeitliche Reihenfolge ist hier egal. Hier geht es nach Gliederungspunkten wie Instrumente (FragebÃ¶gen, Interviews, etc.). Die zeitliche Reihenfolge wird in der Prozedur dargestellt.
\end{comment}

\paragraph{Setting}\mbox{} \\
Text \dots

\paragraph{Verwendete Anwendung}\mbox{} \\
Text \dots

\paragraph{FragebÃ¶gen}\mbox{} \\
Text \dots

\subsection{Procedure}
\begin{comment}
Beschreiben Sie chronologisch den Ablauf der Evaluation, von der BegrÃ¼ÃŸung bis zur Verabschiedung. Verweisen Sie dabei auf die anderen Abschnitte (v.a. Setting und Instrumente) und fÃ¼hren Sie nichts Neues mehr ein. Dieser Abschnitt ist der einzige Abschnitt, in dem Sie die zeitliche Reihenfolge klar einhalten mÃ¼ssen, alle anderen sind inhaltlich strukturiert.
\end{comment}

Text \dots

\begin{comment}
Nach dem Lesen der Methode muss deutlich geworden sein, wie Sie Ihre Evaluationsfragen messbar gemacht haben. Wie haben Sie z.B. Benutzerzufriedenheit oder Effizienz gemessen? Die Leser mÃ¼ssen sich aufgrund des Methodenteils in die Lage der Teilnehmer versetzen kÃ¶nnen und ein mentales Modell Ihrer Evaluation bilden kÃ¶nnen.
\end{comment}

\section{Results}
\begin{comment}
In den Ergebnissen zeigen Sie wie die Ergebnisse analysiert wurden um Ihre Evaluationsfragen zu beantworten. Nicht einfach die Daten auflisten, sondern stellen Sie die Ergebnisse strukturiert dar und betonen Sie die wichtigen Aspekte. Gliedern Sie die Ergebnisse nach Ihren Forschungsfragen / Fragestellungen (nicht zeitlich oder nach Erhebungsmethoden wie FragebÃ¶gen vs. Beobachtung). Zu Beginn (falls relevant) sollten Sie Ã¼berprÃ¼fen, ob die Anwendung auch wirklich so verwendet wurde, wie sie verwendet werden sollte (manipulation check). Haben die Personen also z. B. wirklich die Aufgaben mit der App gelÃ¶st oder haben sie die App schnell beiseite gelegt.

Stellen Sie die Befunde / Ergebnisse von Analysen / Evaluationen / etc. immer so neutral und so objektiv wie mÃ¶glich dar â€” ohne Ihre subjektive Interpretation oder Bewertung. Also keine Begriffe wie "lediglich", "nur", "kÃ¶nnte / wÃ¼rde / sollte / etc." oder Bewertungen wie "hat gut / nicht gut geklappt". Diese Bewertungen gehÃ¶rt in die Diskussion.

Ãœberlegen Sie sich mit welchen Tabellen und Abbildungen Sie die Ergebnisse gut darstellen kÃ¶nnen. Bei aggregierten Messwerten (auf mindestens Intervallskalenniveau) immer Mittelwerte (M), Standardabweichungen (SD) und die Anzahl der Messdaten (Personen, n) angeben. Bei ordinalskalierten Daten entsprechend Median, etc. (ja, Statistik war wichtig).

Statistische Tests korrekt angeben (siehe z.B. Pallant, 2010).

Pallant, J. (2007). SPSS Survival Manual (3rd ed.). Open University Press.

Abbildungen mÃ¼ssen in sich verstÃ¤ndlich sein (was abgebildet ist). Das heiÃŸt, die Achsen eindeutig beschriften, Skala (z.B. Likert-Skala von 1 starke Ablehnung bis 7 starke Zustimmung) in die Legende. 3D-Graphiken vermeiden â€” diese bieten oft keinen Mehrwert (vgl. Field, 2016).

Field, A. (2016). An Adventure in Statistics. Sage.

Am Ende des Ergebnis-Abschnitts muss deutlich geworden sein, wie Ihre Entwicklung von den Teilnehmenden eingesetzt wurde (hoffentlich wie geplant), was die Hauptergebnisse waren, sofern aufgrund der StichprobengrÃ¶ÃŸe mÃ¶glich welche Werte sich statistisch signifikant voneinander unterscheiden und was die statistischen Ergebnisse in den Variablen bedeuten (z. B. positive Korrelation zwischen A und B, dass Personen, die A besser bewertet haben auch B besser bewertet haben; aber keine Bewertung ob das gut oder schlecht ist). Falls Sie konkrete Ziele hatten (z. B. "SUS-Wert von x") dann sagen Sie, ob dieses Ziel erreicht wurde oder nicht (das ist keine Wertung, die in die Diskussion gehÃ¶ren wÃ¼rde, sondern ein grÃ¶ÃŸer, gleich oder kleiner was eindeutig ist).
\end{comment}

Text \dots

\section{Discussion}
\begin{comment}
In der Diskussion erklÃ¤ren und interpretieren Sie die Ergebnisse. Welche Schlussfolgerungen ziehen Sie daraus? Was sind die praktischen Konsequenzen fÃ¼r die (weitere) Entwicklung? Hier dÃ¼rfen Sie selbst die Ergebnisse bewerten â€” auf Basis von einer kritischen Reflektion.
In der Diskussion keine neuen Ergebnisse aus der Analyse / Evaluation / etc. einfÃ¼hren. Die Beweisaufnahme ist mit Ende des Ergebnisteils abgeschlossen. Es geht hier auch nicht um eine Mystery-Geschichte mit Spannungsbogen, sondern um klar nachvollziehbare Argumente. Neue Informationen aus der Literatur verwenden um die (v.a. Ã¼berraschende) Ergebnisse zu interpretieren ist dagegen mÃ¶glich.
\end{comment}

Text \dots

\section{Conclusion on the Evaluation}
\begin{comment}
Fassen Sie die Evaluation kurz zusammen â€” insbesondere was die zentralen Ergebnisse waren. Unterm Strich: Wie gut hat's geklappt?
\end{comment}

Text \dots

\newpage
\chapter{Summary and Outlook}
\begin{comment}
Kurze EinfÃ¼hrung, was in den folgenden Unterkapiteln behandelt wird.
\end{comment}

Text \dots

\section{Summary}
\begin{comment}
Fassen Sie die zentralen Schritte und Ergebnisse Ihrer Arbeit kurz zusammen. Personen mit wenig Zeit mÃ¼ssen aus dieser Darstellung die Kernpunkte Ihrer Arbeit mit-nehmen und Ihren Arbeitsaufwand und Erfolg bewerten kÃ¶nnen.

Ist Ã¤hnlich wie die Zusammenfassung zu Beginn der Arbeit, aber etwas lÃ¤nger (1 bis maximal 2 Seiten) mit Verweisen auf die entsprechenden Kapitel/Abschnitte und Sie kÃ¶nnen beim Leser etwas mehr voraussetzen (hat es gelesen oder kann wegen den Verweisen direkt dahin springen).
\end{comment}

Text \dots

\section{Outstanding Issues}
\begin{comment}
\textbf{Offene Punkte = Versprochene aber nicht umgesetzte Punkte:} Falls Schritte explizit geplant wurden (ExposÃ©! Pflichtenheft!), aber nicht realisiert werden konnten, dann diese hier klar darstellen und diskutieren.

MÃ¶gliche Features, die Sie nicht vor Beginn versprochen haben, gehÃ¶ren in den Ausblick.
\end{comment}

Text \dots

\section{Outlook}
\begin{comment}
Ideen, welche weiteren Entwicklungen oder Untersuchungen folgen sollten, oder was man noch umsetzen kÃ¶nnte, gehÃ¶ren in den Ausblick.

Versprochene aber nicht umgesetzte Elemente in die Offenen Punkte.

Bitte keine Allgemeinheiten ("kÃ¶nnen noch Features hinzugefÃ¼gt werden" oder "kÃ¶nnte besser evaluiert werden") sondern konkrete Beschreibungen und BegrÃ¼ndungen der Relevanz dieser Schritte.
\end{comment}

Text \dots

\section{Final Conclusion}
\begin{comment}
Die Arbeit, die Ergebnisse und weitere mÃ¶gliche Schritte kurz kritisch (ehrlich und konstruktiv, aber nicht selbstkreuzigend) reflektieren und positiv enden. Maximal eine halbe bis 3/4 Seite.
Ist kurz und hier kÃ¶nnen Sie von der Arbeit zurÃ¼cktreten und auch den Leser aus dem Text ziehen.

Das ist nur zum Test: \textcite{ford} \textcite{hadoop}
\end{comment}

Text \dots

\clearpage
\include{appendix}
\end{document}
