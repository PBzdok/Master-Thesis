\documentclass[11pt,a4paper,english]{scrreprt}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage[long,nodayofweek,level,24hr]{datetime} 
\usepackage[skip=5pt,font=footnotesize]{caption}

\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{enumitem}
\usepackage{pdfpages}
\usepackage{xcolor}
\usepackage[bookmarksnumbered]{hyperref}
\hypersetup{pdfborder={0 0 0}, breaklinks=true}
\usepackage{bookmark}
% \addtokomafont{disposition}{\rmfamily}
\usepackage[onehalfspacing]{setspace}

% DISABLE TODO NOTES HERE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{todonotes}
%\usepackage[disable]{todonotes}

% HEADER & FOOTER DEFINITIONS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{fancyhdr}
\input{page-style}

% LITERATURE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[backend=biber,style=apa]{biblatex}
\addbibresource{literatur.bib}


% DEF FOR COVER %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\titelMAEnglish}{Explainable Artificial Intelligence:\\ Designing human-centric assessment system interfaces to increase explainability and trustworthiness of artificial intelligence in medical applications}

\newcommand{\authorMA}{Philipp Dominik Bzdok}

\newcommand{\examinerMA}{Univ.-Prof. Dr. rer. nat. Thomas Franke, Dipl.-Psych.}

\newcommand{\supporterMA}{Tim Schrills, M.Sc.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% DEF FOR COMMENTS, CAN BE DELETED %%%%%%%%%%%%%%%%%%%%%%%%
\newenvironment{comment}
  {\par\medskip
   \begingroup\color{olive}%
   }
 {\endgroup
  \medskip}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\pagenumbering{gobble}
\input{deckblatt}
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

\input{preamble}
\tableofcontents

\newpage
\pagenumbering{arabic}
\chapter{Introduction}\label{chapter:introduction}
The use of modern Artificial Intelligence (\textbf{AI}) techniques is pervasive and can be found in many fields of application, such as digital image processing, search engines and speech recognition \parencite{eu_com_ai}. Other application fields, such as medical diagnosis systems, cannot benefit as easily from AI-based technology compared to recreational domains. This impediment stems oftentimes from the AI being a "black box". In consequence humans struggle to understand such AI-systems and their output, leading to trust and compliance issues \parencite{adadi_blackbox_2018}. These issues are further enhanced in the medical context where decisions, possibly based on AI, can have severe consequences for users, especially patients.

An example for medical human-AI-interaction is the image-based recognition of Deep Vein Thrombosis (\textbf{DVT}) with real time AI support for medical professionals by \textit{Think\-Sono}. The system leverages AI to guide the user through the current gold-standard diagnosis, a compression ultrasound examination, so that it enables any healthcare professional to detect DVT \parencite{thinksono_website}. Closely related in this context is the interdisciplinary research project \textit{CoCoAI}, which aims to explore psychological, ethical and technological implications of human-centered, AI-based applications in the DVT diagnosis and beyond \parencite{cocoai_website}. 

When AI-based systems are used in high risk application contexts, such as medical diagnosis, the aspects of explainability, interpretability and trustworthiness become a primary concern for adoption and use of said system. \textcite{ribeiro_why_2016} already explored the importance of explainability and trust in AI-based systems and postulated that AI systems will not be used if the users have no trust in the model or the results. Even though many machine learning algorithms score high on standard performance metrics, such as precistion, recall or Area Under the Receiver Operating Characteristics (\textbf{AUROC}), user-facing performance may be way worse \parencite{gordon_disagreement_2021}. Understanding the AI's underlying machine learning (\textbf{ML}) model and its predictions is an important step for assessing trust and facilitating effective interaction \parencite{ribeiro_why_2016}. Recent technological advances are realized by \textit{Clearbox AI}, with the focus on trustworthy AI by implementing an AI model assessment \parencite{clearbox_website, eu_trustworthy_ai}. The model assessment can help model owners to identify robustness issues, potential undesired behaviour, and explain errors and uncertainties regarding the model predictions \parencite{clearbox_wp}.

Trust in AI systems is primarily induced by the users' understanding and the general interpretability of the machine learning model and their predictions \parencite{ribeiro_why_2016, ras_explanation_2018}. The wide array of different possible user groups and the complex constructs of understandability and trust demands for a human-centric approach in designing AI assessment systems. Because of the inherent complexity of non-linear machine learning models, especially Deep Neural Networks (\textbf{DNNs}) for image processing, suitable visualization and communication techniques are non-trivial. Additionally to the complex models for image classification, the input data is also more complex as it is unstructured. Non-linear neural networks and unstructured data provide additional challenges for Explainable Artificial Intelligence (\textbf{XAI}), as described in \textcite{keane_how_2019}. XAI is a research field that studies how AI decisions and data driving those decisions can be explained to people in order to provide transparency, enable assessment of accountability, demonstrate fairness, or facilitate understanding \parencite{arrieta_explainable_2019}. XAI plays an important role in the acceptance and finally in the usage of AI-based technology. This is further underlined in the medical context where public authorities set strict regulations on the usage of technological systems and ethic concerns have to be thoughtfully addressed.

In the context of a image-based medical diagnosis system, it is important that the responsible stakeholders, such as medical practitioners, specialized doctors and clinic managements, are enabled to make informed decisions on the usage of AI-based technology, even though their expertise in machine learning and data science is expected to be low. The stakeholders' trust in this system is a primary factor for the widespread use of said technology for real life applications. Therefore, increasing the understanding of the AI model and finding an optimal trust level in the predictions by designing human-centric explanation techniques within the AI model assessment system is a main goal of this work. Additionally it is conceivable that authorities will instantiate auditors for AI-based systems in medical contexts. Having a comprehensible and scientifically proven assessment system could be a big step in the approval and adoption of said system. 

\section{Goals}
The users understanding of the AI model and trust in the model are highly essential as pointed out by \textcite{knapic_explainable_2021}. This holds especially true for medical applications where re-traceable results have to be provided and people acting on these results bear great responsibility. To facilitate understanding and trust the machine learning model has to be interpretable and explainable. In the context of Convolutional Neural Networks (\textbf{CNNs}) interpretability of models can pose a significant concern because of their inherent complexity. Explanations of AI-models can provide insights on the machine's decision process and therefore generate user understanding. This can lead to the model being more interpretable by humans.

Assessing the suitability and performance of a CNN for a specific task by applying standard performance evaluation metrics is problematic, since these can be oblivious to distinguishing the diverse problem solving behaviors of a neural network \parencite{lapuschkin_unmasking_2019}. \textcite{samek_explaining_2021,JMLR:v17:15-618,ribeiro_anchors_2018} give an overview on the technical foundations of XAI and a presentation of practical methods, which will be used in conjunction with human-centric design to explore and evaluate suitable and efficient methods to explain a model's classification.

The goal of this thesis is to design, develop and evaluate interactive AI-assessment-system artifacts for medical professionals and machine learning specialists in a human-centric fashion to facilitate understandability and trustworthiness of AI-models. Developing such a system, with human concerns in focus, leads to following research questions:
\begin{itemize}
    \item[Q1:] How is the stakeholders' (medical professionals, clinic managements or data scientists) subjective information processing awareness linked to trust for a specific model and its predictions in the medical domain?
    \item[Q2:] How can different explanation techniques, ranging from perturbation based approaches to more model intrusive alternatives, increase trust in image classifier models and predictions?
    \item[Q3:] What are the most efficient methods to explain and possibly optimize trust levels in image classification models?
    \item[Q4:] To what extend can structured metadata increase the stakeholder's understanding of a model's operational range and performance?
\end{itemize}

\section{State of the Art}
An AI-assessment-system is currently offered by Clearbox AI. The \textit{AI Control Room} cloud platform enables users to assess, improve and validate ML models and data in accordance with the principles of Trustworthy AI \parencite{clearbox_website,eu_trustworthy_ai}. \textcite{clearbox_website} describes its AI-assessment-system as a "Deep Pre‑production Analysis" tool:
\begin{displayquote}
    "AI Control Room automatically generates a model assessment to help model owners to identify robustness issues, potential undesired behaviour, and explain errors and uncertainties regarding the model predictions."
\end{displayquote}

Concretely the product enables users to perform following tasks for AI models working on tabular data:
\begin{description}
    \item [Model behaviour validation:] Validation metrics and plausible causes of error are clearly presented, potential limitations and irreducible uncertainty are identified and local explanations of the model behaviour are generated selecting representative points in the dataset.
    \item [Synthetic data generation:] A generative model can be used to create synthetic data points that preserve the statistical properties of the original dataset. These points can augment the original training set to improve generalization, to increase model robustness, and to oversample specific labels when in the presence of unbalanced data.
    \item [Data-centric analysis:] Generative models perform a probabilistic analysis of the underlying data allowing for robust outliers detection and uncertainty analysis. This information can help you to evaluate data quality.
    \item [Centralised tracking system:] AI Control Room acts as a centralised tracking system to store lineage, versioning, and metadata of your datasets and models. Assessments generated are securely persisted along with models and datasets.
\end{description}

Besides general information and standard metrics (see \autoref{fig:model_assessment_overview}) the assessment system offers varied insights into different aspects of the machine learning model: \autoref{fig:model_assessment_graphs} shows graphs of training and validation precision, recall and calibration, while \autoref{fig:model_assessment_analysis} shows the models strong points and limitations by analyzing the feature distribution in the data. Furthermore, the second half of the model assessment focuses more on the interpretability aspect of machine learning: \autoref{fig:model_assessment_interpret} shows a confusion matrix of possible classification results, which is then extended by example data points, chosen by the assessment system (see \autoref{fig:model_assessment_examples}). These examples can then be further explored to generate understanding of the models inner workings by applying an attribution based explanation technique combined with a decision rule explanation.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/screenshots/model_assessment_overview.png}
    \caption{AI Control Room - Model Assessment Overview with Standard Metrics}
    \label{fig:model_assessment_overview}
\end{figure}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/screenshots/model_assessment_graphs.png}
    \caption{AI Control Room - Precision-Recall and Calibration Graphs}
    \label{fig:model_assessment_graphs}
\end{figure}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/screenshots/model_assessment_analysis.png}
    \caption{AI Control Room - Model String Points and Limitations}
    \label{fig:model_assessment_analysis}
\end{figure}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/screenshots/model_assessment_interpret.png}
    \caption{AI Control Room - Interpretability Assessment}
    \label{fig:model_assessment_interpret}
\end{figure}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/screenshots/model_assessment_examples.png}
    \caption{AI Control Room - Example Data}
    \label{fig:model_assessment_examples}
\end{figure}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/screenshots/model_assessment_explanation.png}
    \caption{AI Control Room - Prediction Explanation for Examples Data}
    \label{fig:model_assessment_explanation}
\end{figure}
\clearpage

\section{Approach}
As already described in the introduction of \autoref{chapter:introduction}, many machine learning algorithms score high on standard performance metrics, but user-facing performance may be way worse \parencite{gordon_disagreement_2021}. This issue is caused by real world applications being very dependent on the actual human-AI-interaction. Following this reasoning, it lends itself to utilize a human-centered design process for creating AI-assessment systems. \autoref{fig:DIN_EN_ISO_9241} shows a standardized process of human-centered design, which was applied in this thesis to conceptualize, implement and evaluate assessment system artifacts. The key take-away is the inclusion of human aspects in all stages of the process. Research on evaluation of AI explanations revealed that there is a big gap between the perceived and actual usefulness of explanations, as described by \textcite{ras_explainable_2021}. This further underlines the need for a human-centered approach in designing AI-assessment-systems.

The thesis' structure will reflect the human-centered approach, which is visualized in \autoref{fig:DIN_EN_ISO_9241}: As already alluded in \autoref{chapter:introduction}, \autoref{chapter:analysis} is about understanding and setting the context of use by conducting literature research and user interviews. Based on the established requirements \autoref{chapter:conception} will describe the conception of functionalities and interaction design. The development of solutions will be described in \autoref{chapter:implementation}, while \autoref{chapter:evaluation} is about the evaluation of the solutions. This general process is embedded in a iterative loop, where intermediate results are evaluated against the requirements and subject to change.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/figures/DIN_EN_ISO_9241-210.png}
    \caption{Human-centered Design Process \parencite{DIN}}
    \label{fig:DIN_EN_ISO_9241}
\end{figure}

\newpage
\chapter{Analysis}\label{chapter:analysis}
\begin{comment}
    \textbf{Context => Problem / Task => User => Organisational:} "Black-box DNNs have become pervasive in todays society and represent a proven and indispencible machine learning tool. Such models for image classification are able to detect various disease patterns with medical imaging and can be used by medical professionals support diagnosis and increase efficiency and effectivity if trusted and used correctly. Medical professionals with low expertise in machine learning need to be enabled to assess ML applications before using it on patients to build trust and facilitate good results."
\end{comment}

Following the human-centered design process, it is important to incorporate the potential users from the beginning. This is also reflected in the analysis, where the context and setting of use has to be understood and set. Besides the human factors, there are also more general and theoretical aspects to be analysed, such as the context of AI in the medical application domain for specific tasks, such as classifying disease patterns with medical imaging.

In the following the different aspects to be analysed will be layed out and described as a foundation for the human-centered design process and the conception of system artifacts.

\section{Data Sources}
Three main data sources where used for the analysis, ranging from general scientific literature about XAI to specially elaborated user interviews and cooperation with developers of an existing application.

\subsection{Scientific Literature}
Literature is the foundation of the analysis. As described by \textcite{mueller_explanation_2019}, the amount of scientific publications on the topic of explanation in intelligent systems has surged in the last 5 years, revealing many important and relevant information on this subject area through openly accessible papers. In the beginning of the thesis (July 2021) a general search on \textit{Google Scholar} was conducted to gain a overview on available publications. A non-exhaustive list of search terms at the time was:
\begin{itemize}
    \item XAI
    \item XAI in Medical Applications
    \item Explainable Artificial Intelligence
    \item Explainable Artificial Intelligence in Medical Applications
    \item Explainable Machine Learning
    \item Interpretable Machine Learning
    \item Explaining Black-Box Machine Learning Models
    \item Explaining DNNs
\end{itemize}
This general research yielded already good results, as there were many relatively new and popular publications on the topic of XAI, such as \textcite{mueller_explanation_2019, ras_explanation_2018, adadi_blackbox_2018}.

The results of the internet research were then further reinforced by academic partners from the University of Lübeck, with whom related research was conducted in the context of the \textit{CoCoAI} project. Leveraging the available resources and support from research partners boosted the yield on relevant scientific literature tenfold. Over the course of multiple months the list of literature grew and is still being maintained in a shared \textit{Zotero} library \parencite{Zotero_website}. The most important scientific papers for this analysis were: \textcite{ras_explanation_2018, arrieta_explainable_2019, ribeiro_why_2016, adadi_blackbox_2018,knapic_explainable_2021, samek_explaining_2021, chiou_trusting_2021}. 

\subsection{Interviews}
Complementing the general research on XAI, specially elaborated user interviews where conducted. These interviews specifically target medical professionals and data scientists. Interviews with the actual user group of a potential solution is fundamental to understanding and setting the context and requirements of use. The participants for the interviews were chosen with following requirements in mind:
\begin{description}
    \item[Medical Professionals:] Has interest and/or knowledge in AI-systems; has worked with or researched AI-systems in the medical domain; can judge the benefits and risks of the use of AI in medical applications.
    \item[Data Scientists / AI Researchers:] Is familiar with the XAI topic; has interest in explainability and trustworthiness of machine learning models; has worked with AI in the medical context.
\end{description}
Participants for the interviews were gathered via academic partners, internet research and word of mouth. In total 16 suitable people from 6 different institutions (among them UKSH, TU München and Mevis Frauenhofer Bremen) were contacted about potential interviews. Ten leads were medical professionals while six leads where data scientists or AI researchers. From the total of 16 potential interview partners only four interviews have been conducted. This low yield is due to the time constraint of the individuals, who are mainly full time medical practitioners or researchers. The participants are described in further detail in \autoref{table:interview_participants}.

\begin{table}[htbp]
    \centering
    \begin{tabularx}{\textwidth}{ l l l X X l }
        \toprule
        ID & Age & Gender & Occupation & Education Level & AAII Score \\
        \midrule
        1 & 28 & male & Assistant Physician in Neuroradiology & State Examination & 4.89 \\ 
        2 & 24 & female & Research Associate (ML) & Masters Degree & 5.12 \\ 
        3 & 48 & male & Surgeon & Dr. med. & 5.45 \\ 
        4 & 27 & female & Assistant Physician in Neuroradiology & State Examination & 4.67 \\ 
        \bottomrule
    \end{tabularx}
    \caption{Interview Participants}
    \label{table:interview_participants}
\end{table}

The Interviews were conducted in german and executed as 1 to 1 online interviews. For reference the interviews were recorded if consent was present. Additionally the interviews were supported by a research colleague, who kept protocol. After the interviews the recordings were transcribed for further analysis and the participants were asked to answer the \textit{Affinity for AI Interaction} (\textbf{AAII}) questionnaire, which is a modified version of the \textit{Affinity for Technology Interaction} (\textbf{ATI}) questionnaire \parencite{franke_personal_2019}. ATI aims to determine the tendency to actively engage in intensive technology interaction, as a key personal resource for coping with technology. Analogously the AAII questionnaire aims to determine the tendency to actively engage in AI interaction.

In terms of content, the interviews for medical professionals and data scientists were slightly different as seen in \autoref{table:interview_topics}. The reason for this is the heterogenous expertise on the subject area of machine learning models and potential user requirements. The whole interview guideline can be found in \autoref{appendix:interview_guideline}.
\begin{table}[htbp]
    \centering
    \begin{tabularx}{\textwidth}{ l l }
        \toprule
        Medical Professionals & Data Scientists / AI researchers \\
        \midrule
        Actual Usage of AI & Actual Usage of AI \\ 
        Perspective on AI Usage & Comparison of AI Models \\  
        Trust in AI & Perspective on AI Usage \\
        Potential Problems with AI Usage & Trust in AI \\
        Own Explanation Techniques & Potential Problems with AI Usage \\
        Familiarity with XAI & Own Explanation Techniques \\
        Assessment on Local Explanations & Familiarity with XAI \\
        Information Processing & Need for Local Explanations \\
        Reliability vs. Trust vs. Understanding & Need for Global Explanations \\
         & Information Processing \\
         & Trust-Behavior Connection \\
         & Reliability vs. Trust vs. Understanding \\
        \bottomrule
    \end{tabularx}
    \caption{Interview Topics}
    \label{table:interview_topics}
\end{table}

After gathering the interview data via protocols and transcripts a thematic analysis according to \textcite{braun_thematical_2006} was applied to identify common topics and codes. The thematic analysis is a widely used qualitative analysis method mainly found in the field of psychology and can be used as a primary tool to access data from interviews. Applying this method resulted in a thematic map showcasing common overlapping topics found in the interviews, which can be seen in \autoref{fig:thematic_mind_map}. thematic map serves as the baseline for further context and requirement analysis.

\begin{figure}[htbp]
    \centering
    \includegraphics[height=0.8\textwidth, angle=90]{img/figures/Thematic_Mind_Map.pdf}
    \caption{Thematic Mind Map}
    \label{fig:thematic_mind_map}
\end{figure}

\subsection{Existing Applications}

\section{Context Analysis}
\begin{comment}
Der Kontext bezieht sich auf den \textbf{räumlichen} und \textbf{zeitlichen Kontext}, in dem Ihre Entwicklung eingesetzt wird (z. B. \textbf{Informations-Website:} auf Desktop-Computern, Notebooks und Mobilgeräten sowohl zu Hause als auch unterwegs und zu jeder Uhrzeit vs. S\textbf{achbearbeitungssoftware in der öffentlichen Verwaltung:} auf Arbeitsplatz-Computern, im Büro, zwischen 9 und 17 Uhr) Aus diesen Kontexten ergeben sich unterschiedliche Nutzungsanforderungen, die für die Konzeption relevant sind. Beziehen Sie diese Informationen bei der weiteren Entwicklung explizit mit ein.

Wenn Sie Aussagen über den Kontext machen, dann legen Sie auch Ihre Quellen offen — woher wissen Sie das? Z. B. aus Interviews, der Literatur, etc.
\end{comment}

Text \dots

\section{Problem Analysis}
\begin{comment}
Problem- und Aufgabenanalyse wird häufig kombiniert, können aber auch in zwei separate Unterkapitel aufgeteilt werden.

\textbf{Problemanalyse:} Welche(s) Problem(e) adressieren Sie? Machen Sie es konkret. Verwenden Sie empirische Daten, z. B. über die Häufigkeit des Problems. Machen Sie es plastisch mit einem typischen Szenario. Weisen Sie in der weiteren Arbeit explizit darauf hin, wie Ihre Entwicklung diese Probleme adressiert.
\end{comment}

Text \dots

\section{Task Analysis}
\begin{comment}
\textbf{Aufgabenanalyse:} Welche Aufgaben sollen die Benutzer/innen (besser) ausführen können? Wodurch sind diese gekennzeichnet? Bei kleineren klar definierten Aufgaben bieten sich z. B. HTAs an. Nehmen Sie in der weiteren Arbeit explizit Bezug auf die Aufgaben und die durch die Entwicklung neu vorhandenen Möglichkeiten bzw. bessere Durchführung.
\end{comment}

Text \dots

\section{User Analysis}
\begin{comment}
Wer sind die Benutzer/innen Ihrer Entwicklung? Beschreiben Sie die Benutzergruppen oder Benutzerklassen. In \textbf{seltenen(!)} Fällen können sich Personas eignen (sind eher ein Kommunikationstool im Team). Die Zielgruppe müssen Sie im Verlauf Ihrer weiteren Arbeit immer wieder explizit einbinden — nutzen Sie sie um Ihre Gestaltungslösungen immer wieder an deren Eigenschaften zu messen. In Konzeption, Realisierung, und Evaluation muss deutlich werden, dass Sie spezifisch für diese Personen und ihre Besonderheiten entwickelt haben.

Teilen Sie unterschiedliche Benutzerklassen in Unterabschnitte auf, z. B. "2.4.1 Verwaltungsmitarbeiter", "2.4.2 Bürger". Es sollten nicht mehr als ca. drei Benutzerklassen bedient werden.
\end{comment}

Text \dots

\section{Organisational Analysis}
\begin{comment}
Hier geht es um den \textbf{organisationalen Kontext}, also z. B. um Hierarchien, Abhängig-keiten zwischen den Nutzern/innen und anderen Personen/Organisationen, Rechte in Unternehmen, und andere Rahmenbedingungen, die Sie berücksichtigen müssen.

Nehmen Sie als Beispiel Ihre Situation als Studierende bei der Entwicklung einer Studiengangs-App — Sie sind im Kontext Universität mit Dozierenden, Universitätsleitung, Studiengangs-Koordination, entsprechenden Studien- und Prüfungsordnungen etc. unterwegs. Das müssen Sie bei der Entwicklung berücksichtigen.

Oder wenn es um eine Software für eine Klinik geht — und die IT den Einsatz von Java verbietet. Wenn Sie das nicht in Erfahrung gebracht haben und dann eine Java Lösung entwickeln, bekommen Sie erst viel zu spät heraus, dass Ihre Entwicklung nicht verwendet werden kann (yup, realer Fall — für die Klinik-IT ist das Java-Verbot normal und nicht der Rede wert, die Nutzer wissen davon oft nicht — oder gar was Java ist). BTW, versuchen Sie nicht, eine Organisation zu ändern — Organisationsentwicklung ist grausam und dafür werden sie nicht bezahlt.

Auch hier: Nehmen Sie auf diese Punkte explizit in der weiteren Entwicklung Bezug!
\end{comment}

Text \dots

\section{Conclusion on the Analysis}
\begin{comment}
Stellen Sie zum Ende der Analyse in einem eigenen Fazit mit eigener Gewichtung die zentralen Inhalte der Analyse kurz dar. Zur Unterstützung kann eine Tabelle mit einer Priorisierung der Nutzungsanforderungen verwendet werden. Sie legen \textbf{auf Basis der Analyse} begründet fest, welche Nutzungsanforderungen:

\begin{enumerate}
    \item absolut notwendig erfüllt werden müssen (müssen Sie lösen, sonst kann das Ziel der Anwendung nicht erreicht werden, d.h. sie ist nicht effektiv),
    \item wichtig zu erfüllen sind (hoher Einfluss auf Effizienz, Zufriedenstellung, Erlernbarkeit),
    \item was "nice-to-have" ist (geringer Einfluss auf Effizienz, Zufriedenstellung, Erlernbarkeit), und
    \item was eher unwichtig ist, aber die Anwendung abrundet (Kür).
\end{enumerate}

Die Anforderungen mit \textbf{1 müssen} Sie erfüllen, die Anforderungen mit \textbf{2 sollten} Sie erfüllen, die Anforderungen mit \textbf{3 können} Sie erfüllen, und die mit \textbf{4 können} Sie erfüllen, wenn Sie wider erwarten am Ende der Entwicklungsphase noch was Zeit haben.

Falls bei der Festlegung der Nutzungsanforderungen viel Entscheidungsspielraum vorliegt — z. B. weil die Analyse keine klaren Prioritäten nahelegt, dann können Sie die Priorisierung auch zu Beginn der Konzeption darstellen. In der Konzeption haben Sie mehr gestalterische Freiheiten als in der Analyse. Ein eigenes Fazit sollten Sie am Ende der Analyse aber in jedem Fall schreiben.
\end{comment}

Text \dots

\newpage
\chapter{Conception}\label{chapter:conception}
\begin{comment}
Die \textbf{Konzeption muss klar aus der Analyse folgen}. Sie können und sollten kreative Lösungen erarbeiten, aber Sie müssen dabei wirklich kreativ sein. Sprich: \textbf{Absichtlich etwas Erschaffen, was neu und nützlich ist}. Machen Sie deutlich wie Ihre kreative Lösung die Punkte aus der Analyse aufgreift und löst. Also nicht: "Jetzt habe ich die Pflicht-Analyse gemacht, kann's abhaken, und mich frei entfalten." Die Analyse liefert die \textbf{Legobausteine} und den \textbf{Rahmen} (Bodenplatte) für die Konzeption — die müssen Sie nutzen. Sie können natürlich iterativ die ihr Verständnis des Nut-zungskontextes erweitern und die Nutzungsanforderungen updaten!

Stellen Sie zu Beginn der Konzeption einen \textbf{Bezug zur Analyse} her. Geben Sie dann einen Überblick über das Konzeptionskapitel. Wie sind Sie bei der Konzeption vor-gegangen? Welche Aspekte (z. B. Funktionalitäten, Systemarchitektur, Interface-Design, etc.) berücksichtigen Sie? Verweise auf die Abschnitte (Abschnittsnummern verwenden!).

Zentral für die Konzeption ist die \textbf{formative Evaluation der Gestaltungslösung anhand der Anforderungen}. Sie wissen ja aus der Analyse wie die Nutzungsanforderungen sind — entsprechend immer zuerst selbst die Lösungen damit bewerten (= evaluieren). Dabei helfen auch Benutzerklassen oder (wenn sinnvoll) Personas. Aber frühzeitig die Zielgruppe hinzuziehen, damit diese Ihnen Feedback zu Ihren bisherigen Konzeptionen gibt. Sie brauchen oft mehrere Iterationen bis Sie eine gebrauchstaugliche Lösung herausarbeiten können.

\textbf{Zuerst "Getting the Right Design", dann erst "Get the Design Right":} Viele Arbeiten sind zu unkreativ. Das heißt nicht anders um anders sein zu wollen (wir sind nicht im Marketing), sondern etwas entwickeln, was neu und nützlich ist. Häufig wird mit der ersten Idee gestartet und diese iterativ verbessert. Das Ergebnis ist, dass man sich einen Hügel hocharbeitet (Abbildung linke Seite). Das führt zu einem "hill climbing problem" — man kommt evtl. oben an, aber übersieht, dass es andere Lösungen gibt, die einen vielleicht weiter führen (Abbildung rechte Seite).

\begin{figure}[htbp]
    \centering
    \begin{subfigure}
        \centering
        \includegraphics{img/Picture2.png}
    \end{subfigure}
    \begin{subfigure}
        \centering
        \includegraphics{img/Picture3.png}
    \end{subfigure}
\end{figure}

"Getting the Design Right" (links) vs. "Getting the Right Design" (rechts). Abbildungen aus \textcite{greenberg_2012}.

Stattdessen erst unterschiedliche Ideen mit sehr geringen Änderungskosten (= billige und grottige Papierskizzen) explorieren, dann erst für die beste Idee entschei-den (auf Basis der Nutzungsanforderungen, Ihren Fertigkeiten und zur Verfügung stehenden Zeit)!)

\begin{figure}[htbp]
    \centering
    \includegraphics{img/Picture4.png}
\end{figure}

Abbildung aus \textcite{greenberg_2012}.

Die Konzeption \textbf{kann unterschiedlich aufgebaut sein} und \textbf{nicht alle Unterkapitel sind bei jeder Entwicklung relevant}. Die Struktur muss Sinn ergeben — im Zweifelsfall mit dem Betreuer abstimmen. Die Unterkapitel Konzeptionsvorgehen, Anwendungsfälle (Use-Cases) und Funktionalitäten wird man in den meisten Fällen benötigen. \textbf{Mögliche Reihenfolgen danach sind:}

\begin{itemize}
    \item \textbf{Iterationsweise:} Bietet sich an, wenn Sie nur einen Aspekt (z. B. nur das Interface) entwickeln. Sie beschreiben in eigenen Unterkapiteln die jeweiligen Iterationen samt formativer Evaluation, aus der sich dann die nächste Iteration ableitet.
    \item \textbf{Strukturell gegliedert:} Aufteilung in Unterkapitel (Systemarchitektur, Interface, etc.) und Beschreibung der Iterationen dieser in den jeweiligen Unterkapiteln.
\end{itemize}
\end{comment}

Text \dots

\section{Conceptual Approach}
\begin{comment}
Die Konzeption verläuft — noch stärker als die Analyse — iterativ. Die Unterkapitel sind allerdings linear strukturiert. Vergleichbar mit den Datenquellen der Analyse bietet sich das erste Unterkapitel an, das Vorgehen und die eingesetzten Methoden zu beschreiben (deren Ergebnisse sich meist in mehreren Unterkapitel niederschlagen).

\textbf{Methoden} können z. B. sein:

\begin{itemize}
    \item \textbf{Analyseworkshop:} Nützlich um die Ergebnisse der Analyse zu strukturieren und für die Konzeption aufzubereiten (vgl. \textcite[S. 190-192]{kumar_2013})
    \item \textbf{Design-Sprints:} Stärker für die Konzeption (vgl. \textcite{knapp_2016})
    \item \textbf{Workshops mit der Zielgruppe:} Generell um Ideen partizipativ zu entwickeln und formativ zu evaluieren.
\end{itemize}

Auch hier: Vorgehen beschreiben, Teilnehmer mit Alter (M, SD), Geschlecht (m/w/d) sowie relevanten Eigenschaften (warum stehen sie für die Zielgruppe?).

In den folgenden Unterkapitel können Sie dann auf 3.1 verweisen.
\end{comment}

Text \dots

\section{Use Cases}
\begin{comment}
Use Cases bieten sich an, die Ergebnisse aus der Analyse zusammenzufassen und in einen oder mehrere konkrete Anwendungsfälle zu übertragen. Die Tabelle mit den Nutzungsanforderungen sollte hierbei helfen. Wie sieht die Situation aus, wenn Ihr System eingesetzt wird? Bei den Anwendungsfällen noch nicht zu spezifisch werden. Es geht primär darum zu beschrieben, was die Person macht, z. B. veränderte Arbeitsabläufe, neue Funktionen, etc., noch nicht wie genau dies (technisch, womit realisiert) umgesetzt wird.
\end{comment}

Text \dots

\section{Functionalities}
\begin{comment}
Geben Sie einen Überblick über die geplanten Funktionalitäten des Systems. Dazu können Sie die Anwendungsfälle der Konzeption sowie die Tabelle mit den Nutzungsanforderungen aus der Analyse aufgreifen und den Anforderungen mögliche Funktionalitäten zuordnen. Priorisieren Sie dann diese Funktionalitäten — bezüglich Auswirkungen auf die Gebrauchstauglichkeit des Systems und Umsetzbarkeit dieser Funktionen.
\end{comment}

Text \dots

\section{System Architecture}
\begin{comment}
Mit den Funktionalitäten gehen bestimmte Anforderungen an das zugrundeliegende System einher. Stellen Sie hier dar, welche Lösungen es für die Systemarchitektur gibt, welche Aufteilungen sich eignen (z. B. MVC) und wie man es umsetzen könnte. Je nach Detailgrad bieten sich u.a. UML-Diagramme an.

\textbf{Begründete Auswahl:} Nehmen Sie die Funktionalitäten als Grundlage, das gibt Ihnen vor, mit welchen möglichen Architekturen Sie diese Funktionalitäten umsetzen können. Falls es mehrere Möglichkeiten gibt, erläutern Sie diese und treffen eine be-gründete Auswahl auf Basis dessen Stärken und Schwächen für Ihren Anwendungsfall.

\textbf{Hardware/Software-agnostisch bleiben:} Es geht hier explizit nicht um spezifische Hard- oder Software. Sie bleiben auf der "Datenbank", "Server", etc. Ebene. Welche Datenbank (MySQL, PostgreSQL, SQLite, etc.) oder welcher Server (Apache, Node.js, etc.) verwendet wird kommt in der Realisierung. Auch hier: Nach der Konzeption kann der Leser stoppen und Ihre Konzeption auf seine Weise realisieren.

\textbf{Bezug zum Interface Design beachten:} Systemarchitektur und Interface Design hängen eng zusammen. Stimmen Sie beides aufeinander ab — in jeder Iteration.
\end{comment}

Text \dots

\section{Interface Design}
\begin{comment}
Wie können die Informationen dargeboten und Angaben der Nutzer erfolgen?

\textbf{Screen-Fetisch hinterfragen:} Viele Medieninformatiker haben einen Fetisch was visuelle Interface betrifft. Diese lassen sich leicht skizzieren, es gibt Tools dafür, die man kennt, und man kann was zeigen. Man sieht, was man gemacht hat. Für viele Kontexte sind sie allerdings absolut ungeeignet (z. B. um Autofahrer zu energiesparendem Fahren zu bringen). Visuelle Aufmerksamkeit ist extrem kostbar. Probieren Sie deswegen bewusst andere Sinne aus. Können Sie die Informationen auch auditiv geben (z. B. Abweichung vom idealen Fahrverhalten über die Tonhöhe)? Oder haptisch (z. B. Vibration)? Gerade wenn man sich im Laufe der Konzeption in das eigene visuelle Design verliebt kommen oft Lösungen heraus, die vielleicht schön sind, aber auch lebensgefährlich, und schlimmer noch: nicht gebrauchstauglich. Für viele Kontexte sind visuelle Interfaces sehr gut geeignet — aber für viele auch nicht. Und die Auswahl sollte wie üblich begründet erfolgen.

\textbf{Formativ Evaluieren:} Interface-Designs müssen Sie formativ evaluieren. Für Sie ist die Interaktion klar (Sie wissen auch als — meist — einzige Person, was unter der Oberfläche passiert und was die Hintergründe sind), für Laien ist das oft unklar. Sie brauchen früh Feedback der Zielgruppe. Nutzen Sie es! Dokumentieren Sie am Ende jeder Iteration, was herausgekommen ist und was die Konsequenzen für Ihr Design / die nächste Iteration war.

\textbf{Papier über Mock-Up-Tools:} Verwenden Sie Papier um visuelle Interfaces zu gestalten. Wenn Sie nicht zeichnen können, um so besser. Bei mit dem Computer ausgearbeiteten Mockups ist die Hemmung der Zielgruppe oft zu hoch, noch grundlegende Aspekte zu ändern. Damit bleiben Sie oft in der ersten — und oft unkreativen und nicht gebrauchstauglichen — Lösung gefangen. Tools wie Adobe XD sind eine Falle, die Sie zumindest in den ersten Iterationsphasen tunlichst vermeiden sollten.

\textbf{Bei visuellen Interfaces Skizzen der Iterationen darstellen:} Benutzen Sie Graphiken (Skizzen, Mockups) um das Interface und die Interaktion damit zu verdeutlichen.
\end{comment}

Text \dots

\newpage
\begin{comment}
\begin{center}
\huge
Nach der ersten Iteration:\\STOP\\
\normalsize
Nachdem Sie die erste Iteration abgeschlossen haben, nehmen Sie das Blatt Papier, auf dem Sie vor/während der Analyse Ihre Lösungsidee(n) festgehalten haben, wieder heraus. Vergleichen Sie Ihre ursprüngliche Idee mit der ersten Iteration.

\textbf{Falls sich Ihre erste Iteration sich stark von der erste Idee unterscheidet:} Prima. Sie haben während der Analyse dazu gelernt und den Nutzungskontext besser verstanden. Ohne die Analyse und die Verwendung dieser Informationen in der Konzeption hätten Sie das nicht machen können.

\textbf{Falls Ihre erste Iteration Ihrer ersten Idee sehr ähnlich ist:} Entweder Sie kannten sich schon vor der Analyse mit dem Themengebiet sehr gut aus — oder (und das ist wahrscheinlicher) — sie hängen zu stark an Ihrer ersten Idee und hören der Zielgruppe nicht zu. Auch falls Sie selbst zur Zielgruppe gehören (z. B. eine Anwendung für Studierende), Sie sind nicht "alle Nutzer". Nutzer unterscheiden sich üblicherweise. Entsprechend wirklich am in sich gehen — haben Sie aus der Analyse wirklich nichts dazu gelernt? Was das Problem (oder Sie selbst) wirklich so trivial? Das Schöne an kreativen Prozessen ist ja, dass man iterativ besser werden kann und auch auf Ideen kommen kann, auf die man vorher nicht gekommen wäre. Die Chance sollten Sie nutzen.

BTW, dieser Hinweis heißt nicht, dass Sie komplett naiv an Ihre Gestaltung herangehen sollten, ohne sich vorher Gedanken zu machen. Aber, wenn Sie eine Analyse wirklich ernst nehmen, dann sollte die Ihnen Sachverhalte aufzeigen, die man als Außen-stehender nicht sieht. Und Sie sollten auch in der Konzeption basierend auf diesen Informationen Lösungen entwickeln, die über die ersten simplen Ideen hinausgehen (vgl. confirmation bias). Die neu und nützlich sind.
\end{center}
\end{comment}

\section{Conclusion on the Conception}
\begin{comment}
Wie auch bei der Analyse am Ende der Konzeption ein kurzes Fazit um den Entwicklungsprozess zusammenzufassen und begründen, was die erfolgversprechendste Lösung (höchste Gebrauchstauglichkeit in der formativen Evaluation!) für die Realisierung ist.
\end{comment}

Text \dots

\newpage
\chapter{Implementation}\label{chapter:implementation}
\begin{comment}
Beschreiben Sie, wie Sie das Projekt konkret realisiert haben, mit dem Ziel, anderen (Medien-)Informatikern zu erlauben, Ihre Anwendung weiter zu entwickeln.

\textbf{Test:} Sie nehmen einen Kommilitonen, geben ihm dieses Kapitel und bitten ihm eine Funktion hinzuzufügen oder zu verändern. Wenn er das kann, prima. Wenn nicht, besser schreiben oder bessere Kommilitonen suchen.

Verweisen Sie zu Beginn auf die erfolgreichste Lösung aus der Konzeption. Wie in der Konzeption bieten sich Unterkapitel mit Systemarchitektur und Interface-Design an — jetzt allerdings mit konkreten Angaben, welche Programme / Frameworks Sie verwenden, und wie Sie das Interface konkret umsetzen.

Geben Sie zuerst einen allgemeinen Überblick und werden dann konkreter. Als wenn Sie in eine Weltkarte reinzoomen. Erst mal sagen, wie die Welt aufgebaut ist (Kontinente), dann auf Kontinent mit Ländern zoomen, dann exemplarisch mal auf ein Land. Leser braucht ein gutes mentales Modell vom Aufbau und den Funktionen, um zu verstehen, wie es technisch umgesetzt wurde.

\textbf{Begründen Sie Ihre Entscheidungen linear:} Z.B.

\begin{enumerate}
    \item "das sind die Anforderungen aus der Analyse/bisheriger Konzeption"
    \item "Programmiersprache / System / etc. x, y, und z erfüllen diese Anforderungen"
    \item "relevante Kriterien sind a, b, c und d (z. B. Verbreitung, Community, Weiterentwicklung, Preis)"
    \item "Programmiersprache / System / etc. x, y, und z schneiden da so und so ab, x ist insgesamt am besten"
    \item "Programmiersprache / System / SDK / etc. x wird verwendet". Eine Argumentation à la A => B => C => D => E ist nachvollziehbarer als wir nehmen E, weil A => C woraus D folgt, wegen B.
\end{enumerate}

Weiterer Grund: Wenn Sie zuerst die konkrete Software sagen, dann fangen Personen direkt an, die Entscheidung zu hinterfragen (weil sie z. B. eine andere bevorzugen, gibt da genug Religionskriege). Wenn Sie linear argumentieren gehen diese Personen eher bei jedem Schritt mit und enden dann bei der von Ihnen verwendeten Software (sofern sie bei den Auswahlkriterien und der Bewertung mitgehen).

\textbf{Nutzen Sie den Datenträger im Anhang der Arbeit:} Insbesondere in den Programmcode können Sie viel auslagern. In der Arbeit muss der Leser aber die Struktur/den Aufbau Ihres Programms verstehen sowie die Logik dahinter. Und er muss wissen, dass es die Erläuterungen im Programmcode gibt und wo er diese findet. Entsprechend die Struktur (Dateien/Klassen) deutlichen machen und Verweise auf relevanten Dateien.
\end{comment}

Text \dots

\section{System Architecture Implementation}
\begin{comment}
vgl. Konzeption, jetzt aber wie und womit es konkret umgesetzt wurde (u.U. bieten sich UML-Diagramme an)
\end{comment}

Text \dots

\section{Interface Implementation}
\begin{comment}
vgl. Konzeption, jetzt aber wie und womit es konkret umgesetzt wurde 
\end{comment}

Text \dots

\section{Conclusion on the Implementation}
\begin{comment}
Kurzes Fazit, das u.a. die Frage beantwortet, was technisch konkret umgesetzt wurde und was z. B. aus der Konzeption übrig geblieben ist und warum.
\end{comment}

Text \dots

\newpage
\chapter{Dialogue Samples}
\begin{comment}
Verdeutlichen Sie die Funktionsweise Ihre Entwicklung bei einer typischen Bedienung (z. B. wie in einem Anwendungsbeispiel / Use Case beschrieben — auf den können Sie sich ja beziehen).

Die Abbildungen müssen im Text vorher erläutert sein und einen Eindruck geben, wie die Entwicklung konkret bedient wird bzw. was sie kann.

\textbf{Das ist das, was bleibt:} Diese Dialogbeispiele sind das einzige, was in ein paar Jahren von Ihrer Anwendung noch sichtbar ist. Dann ist entweder der Datenträger verschwunden oder beschädigt oder die notwendige Hard- und Software ist nicht mehr lauffähig. Geben Sie dem Leser entsprechend einen guten Einblick in das, was Sie tatsächlich realisiert haben.

\textbf{Jetzt ein Video aufnehmen:} Schalten Sie den Screenrecorder auf dem Computer (not-falls: QuickTime) oder Smartphone (kann das OS) an und nehmen Sie die typischen Interaktionen einmal auf. Das ist Ihr "Plan B", falls (oft: wenn) im Kolloquium die App nicht bedient werden kann. Das können Sie auch gut auf DVD brennen um Lesern die Möglichkeit geben, sich die Interaktion anzusehen (in der Arbeit darauf hinweisen!). Macht sich auch gut auf Websites (insbesondere dem eigenen Portfolio).
\end{comment}

Text \dots

\newpage
\chapter{Summative Evaluation}\label{chapter:evaluation}
\begin{comment}
Geben Sie zu Beginn der Evaluation einen kurzen Überblick über Ihr Vorgehen. Dazu reichen meist die Zwischenüberschriften mit ein oder zwei Sätzen, was Sie konkret gemacht haben. Also nicht "In Design wird das Design beschrieben" sondern "Das experimentelle Vorgehen wird im Abschnitt Design dargestellt".
\end{comment}

Text \dots

\section{Goal}
\begin{comment}
Die summative Evaluation ist eine abschließende Bewertung Ihrer Entwicklung. Ziel ist, unter dem Strich zu sehen, wie gebrauchstauglich Ihr System ist (nicht mehr eine iterative Verbesserung wie in der formativen Evaluation der Konzeption). Da-für müssen Sie klare Kriterien ableiten, was eine "gute" bzw. "schlechte" Bewertung nach sich ziehen würde. Meist sind das die klassischen Gebrauchstauglichkeitskriterien (Effektivität, Effizienz, Erlernbarkeit, Zufriedenstellung), wobei diese über das Ziel Ihrer Anwendung konkretisiert werden (Effektivität bei einer Lernapp ist konkret gemessen etwas anderes als Effektivität bei einem digitalen Depressionstagebuch).

Hier verdeutlichen Sie entsprechend, welche Fragen die Evaluation beantworten soll.
\end{comment}

Text \dots

\section{Methods}
\begin{comment}
Im Methodenteil zeigen Sie, was wie evaluiert wurde. Der Methodenteil muss anderen Entwicklern die Möglichkeit geben, Ihr Evaluationsvorgehen zu wiederholen um Ihre Ergebnisse zu überprüfen. 
\end{comment}

Text \dots

\subsection{Design}
\begin{comment}
Kurze Beschreibung des Versuchs- oder Evaluationsdesigns, dass man das Vorgehen einordnen kann. Also z. B. "es wurde ein Usability Test durchgeführt", oder "es wurde ein Experiment mit between-subjects design durchgeführt, bei dem die Kontrollgruppe die bisherige App verwendet hat und die Experimentalgruppe die neu entwickelte App".
\end{comment}

Text \dots

\subsection{Participants}
\begin{comment}
Kurze Beschreibung der Teilnehmer mit relevanten Angaben. In jedem Fall die Anzahl, oft noch Geschlecht, Alter, Beruf, Vorerfahrung, etc. Bei weniger als zehn Teilnehmern bietet sich eine Tabelle zur schnellen Übersicht an. Verweisen Sie bei individuellen Ergebnissen (z.B. Zitaten aus Fragebögen oder Interviews) auf die Teilnehmer-Nummer.

Achtung: Niemals die Namen der Teilnehmer erwähnen! Die Teilnehmer stehen stellvertretend für die Zielgruppe. Wer sie konkret sind ist irrelevant. Begründen Sie, warum Sie diese Personen ausgewählt haben (spiegeln die Nutzergruppe gut wider) und wo/wie Sie diese rekrutiert haben.
\end{comment}

Text \dots

\subsection{Setting and Instruments}
\begin{comment}
Beschreiben Sie die notwendigen Materialien bei der Evaluation. Dazu gehört — mit Überschriften klar ausgewiesen — das Setting (wo wurde die Evaluation durchgeführt), Ihre Entwicklung (Verweis auf Dialogbeispiele), und Ihre Erhebungsmethoden (Fragebögen, Interviewleitfäden, etc.).

Falls Sie etablierte Fragebögen verwenden (z.B. ATI) reicht die entsprechende Zitation mit einer kurzen Beschreibung. Bei längeren Fragebögen oder Interviewleitfäden nennen Sie kurz die Abschnitte (z.B. soziodemographische Daten, Technikerfahrung, etc.) und verweisen Sie auf die vollständigen Fragebögen im Anhang.

\textbf{ACHTUNG:} Zeitliche Reihenfolge ist hier egal. Hier geht es nach Gliederungspunkten wie Instrumente (Fragebögen, Interviews, etc.). Die zeitliche Reihenfolge wird in der Prozedur dargestellt.
\end{comment}

\paragraph{Setting}\mbox{} \\
Text \dots

\paragraph{Verwendete Anwendung}\mbox{} \\
Text \dots

\paragraph{Fragebögen}\mbox{} \\
Text \dots

\subsection{Procedure}
\begin{comment}
Beschreiben Sie chronologisch den Ablauf der Evaluation, von der Begrüßung bis zur Verabschiedung. Verweisen Sie dabei auf die anderen Abschnitte (v.a. Setting und Instrumente) und führen Sie nichts Neues mehr ein. Dieser Abschnitt ist der einzige Abschnitt, in dem Sie die zeitliche Reihenfolge klar einhalten müssen, alle anderen sind inhaltlich strukturiert.
\end{comment}

Text \dots

\begin{comment}
Nach dem Lesen der Methode muss deutlich geworden sein, wie Sie Ihre Evaluationsfragen messbar gemacht haben. Wie haben Sie z.B. Benutzerzufriedenheit oder Effizienz gemessen? Die Leser müssen sich aufgrund des Methodenteils in die Lage der Teilnehmer versetzen können und ein mentales Modell Ihrer Evaluation bilden können.
\end{comment}

\section{Results}
\begin{comment}
In den Ergebnissen zeigen Sie wie die Ergebnisse analysiert wurden um Ihre Evaluationsfragen zu beantworten. Nicht einfach die Daten auflisten, sondern stellen Sie die Ergebnisse strukturiert dar und betonen Sie die wichtigen Aspekte. Gliedern Sie die Ergebnisse nach Ihren Forschungsfragen / Fragestellungen (nicht zeitlich oder nach Erhebungsmethoden wie Fragebögen vs. Beobachtung). Zu Beginn (falls relevant) sollten Sie überprüfen, ob die Anwendung auch wirklich so verwendet wurde, wie sie verwendet werden sollte (manipulation check). Haben die Personen also z. B. wirklich die Aufgaben mit der App gelöst oder haben sie die App schnell beiseite gelegt.

Stellen Sie die Befunde / Ergebnisse von Analysen / Evaluationen / etc. immer so neutral und so objektiv wie möglich dar — ohne Ihre subjektive Interpretation oder Bewertung. Also keine Begriffe wie "lediglich", "nur", "könnte / würde / sollte / etc." oder Bewertungen wie "hat gut / nicht gut geklappt". Diese Bewertungen gehört in die Diskussion.

Überlegen Sie sich mit welchen Tabellen und Abbildungen Sie die Ergebnisse gut darstellen können. Bei aggregierten Messwerten (auf mindestens Intervallskalenniveau) immer Mittelwerte (M), Standardabweichungen (SD) und die Anzahl der Messdaten (Personen, n) angeben. Bei ordinalskalierten Daten entsprechend Median, etc. (ja, Statistik war wichtig).

Statistische Tests korrekt angeben (siehe z.B. Pallant, 2010).

Pallant, J. (2007). SPSS Survival Manual (3rd ed.). Open University Press.

Abbildungen müssen in sich verständlich sein (was abgebildet ist). Das heißt, die Achsen eindeutig beschriften, Skala (z.B. Likert-Skala von 1 starke Ablehnung bis 7 starke Zustimmung) in die Legende. 3D-Graphiken vermeiden — diese bieten oft keinen Mehrwert (vgl. Field, 2016).

Field, A. (2016). An Adventure in Statistics. Sage.

Am Ende des Ergebnis-Abschnitts muss deutlich geworden sein, wie Ihre Entwicklung von den Teilnehmenden eingesetzt wurde (hoffentlich wie geplant), was die Hauptergebnisse waren, sofern aufgrund der Stichprobengröße möglich welche Werte sich statistisch signifikant voneinander unterscheiden und was die statistischen Ergebnisse in den Variablen bedeuten (z. B. positive Korrelation zwischen A und B, dass Personen, die A besser bewertet haben auch B besser bewertet haben; aber keine Bewertung ob das gut oder schlecht ist). Falls Sie konkrete Ziele hatten (z. B. "SUS-Wert von x") dann sagen Sie, ob dieses Ziel erreicht wurde oder nicht (das ist keine Wertung, die in die Diskussion gehören würde, sondern ein größer, gleich oder kleiner was eindeutig ist).
\end{comment}

Text \dots

\section{Discussion}
\begin{comment}
In der Diskussion erklären und interpretieren Sie die Ergebnisse. Welche Schlussfolgerungen ziehen Sie daraus? Was sind die praktischen Konsequenzen für die (weitere) Entwicklung? Hier dürfen Sie selbst die Ergebnisse bewerten — auf Basis von einer kritischen Reflektion.
In der Diskussion keine neuen Ergebnisse aus der Analyse / Evaluation / etc. einführen. Die Beweisaufnahme ist mit Ende des Ergebnisteils abgeschlossen. Es geht hier auch nicht um eine Mystery-Geschichte mit Spannungsbogen, sondern um klar nachvollziehbare Argumente. Neue Informationen aus der Literatur verwenden um die (v.a. überraschende) Ergebnisse zu interpretieren ist dagegen möglich.
\end{comment}

Text \dots

\section{Conclusion on the Evaluation}
\begin{comment}
Fassen Sie die Evaluation kurz zusammen — insbesondere was die zentralen Ergebnisse waren. Unterm Strich: Wie gut hat's geklappt?
\end{comment}

Text \dots

\newpage
\chapter{Summary and Outlook}
\begin{comment}
Kurze Einführung, was in den folgenden Unterkapiteln behandelt wird.
\end{comment}

Text \dots

\section{Summary}
\begin{comment}
Fassen Sie die zentralen Schritte und Ergebnisse Ihrer Arbeit kurz zusammen. Personen mit wenig Zeit müssen aus dieser Darstellung die Kernpunkte Ihrer Arbeit mit-nehmen und Ihren Arbeitsaufwand und Erfolg bewerten können.

Ist ähnlich wie die Zusammenfassung zu Beginn der Arbeit, aber etwas länger (1 bis maximal 2 Seiten) mit Verweisen auf die entsprechenden Kapitel/Abschnitte und Sie können beim Leser etwas mehr voraussetzen (hat es gelesen oder kann wegen den Verweisen direkt dahin springen).
\end{comment}

Text \dots

\section{Outstanding Issues}
\begin{comment}
\textbf{Offene Punkte = Versprochene aber nicht umgesetzte Punkte:} Falls Schritte explizit geplant wurden (Exposé! Pflichtenheft!), aber nicht realisiert werden konnten, dann diese hier klar darstellen und diskutieren.

Mögliche Features, die Sie nicht vor Beginn versprochen haben, gehören in den Ausblick.
\end{comment}

Text \dots

\section{Outlook}
\begin{comment}
Ideen, welche weiteren Entwicklungen oder Untersuchungen folgen sollten, oder was man noch umsetzen könnte, gehören in den Ausblick.

Versprochene aber nicht umgesetzte Elemente in die Offenen Punkte.

Bitte keine Allgemeinheiten ("können noch Features hinzugefügt werden" oder "könnte besser evaluiert werden") sondern konkrete Beschreibungen und Begründungen der Relevanz dieser Schritte.
\end{comment}

Text \dots

\section{Final Conclusion}
\begin{comment}
Die Arbeit, die Ergebnisse und weitere mögliche Schritte kurz kritisch (ehrlich und konstruktiv, aber nicht selbstkreuzigend) reflektieren und positiv enden. Maximal eine halbe bis 3/4 Seite.
Ist kurz und hier können Sie von der Arbeit zurücktreten und auch den Leser aus dem Text ziehen.

Das ist nur zum Test: \textcite{ford} \textcite{hadoop}
\end{comment}

Text \dots

\clearpage
\include{appendix}
\end{document}
