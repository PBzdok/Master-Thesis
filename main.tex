\documentclass[11pt,a4paper,english]{scrreprt}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage[long,nodayofweek,level,24hr]{datetime} 
\usepackage[skip=5pt,font=footnotesize]{caption}

\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{enumitem}
\usepackage{pdfpages}
\usepackage{xcolor}
\usepackage[bookmarksnumbered]{hyperref}
\hypersetup{pdfborder={0 0 0}, breaklinks=true}
\usepackage{bookmark}
% \addtokomafont{disposition}{\rmfamily}
\usepackage[onehalfspacing]{setspace}

% DISABLE TODO NOTES HERE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{todonotes}
%\usepackage[disable]{todonotes}

% HEADER & FOOTER DEFINITIONS %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{fancyhdr}
\input{page-style}

% LITERATURE %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[backend=biber,style=apa]{biblatex}
\addbibresource{literatur.bib}


% DEF FOR COVER %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\titelMAEnglish}{Explainable Artificial Intelligence:\\ Designing human-centric assessment system interfaces to increase explainability and trustworthiness of artificial intelligence in medical applications}

\newcommand{\authorMA}{Philipp Dominik Bzdok}

\newcommand{\examinerMA}{Univ.-Prof. Dr. rer. nat. Thomas Franke, Dipl.-Psych.}

\newcommand{\supporterMA}{Tim Schrills, M.Sc.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% DEF FOR COMMENTS, CAN BE DELETED %%%%%%%%%%%%%%%%%%%%%%%%
\newenvironment{comment}
  {\par\medskip
   \begingroup\color{olive}%
   }
 {\endgroup
  \medskip}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\pagenumbering{gobble}
\input{deckblatt}
\setlength{\parindent}{0em}
\setlength{\parskip}{1em}

\input{preamble}
\tableofcontents

\newpage
\pagenumbering{arabic}
\chapter{Introduction}\label{chapter:introduction}
The use of modern Artificial Intelligence (\textbf{AI}) techniques is pervasive and can be found in many fields of application, such as digital image processing, search engines and speech recognition \parencite{eu_com_ai}. Other application fields, such as medical diagnosis systems, cannot benefit as easily from AI-based technology compared to recreational domains. This impediment stems oftentimes from the AI being a "black box". In consequence humans struggle to understand such AI-systems and their output, leading to trust and compliance issues \parencite{adadi_blackbox_2018}. These issues are further enhanced in the medical context where decisions, possibly based on AI, can have severe consequences for users, especially patients.

An example for medical human-AI-interaction is the image-based recognition of Deep Vein Thrombosis (\textbf{DVT}) with real time AI support for medical professionals by \textit{Think\-Sono}. The system leverages AI to guide the user through the current gold-standard diagnosis, a compression ultrasound examination, so that it enables any healthcare professional to detect DVT \parencite{thinksono_website}. Closely related in this context is the interdisciplinary research project \textit{CoCoAI}, which aims to explore psychological, ethical and technological implications of human-centered, AI-based applications in the DVT diagnosis and beyond \parencite{cocoai_website}. 

When AI-based systems are used in high risk application contexts, such as medical diagnosis, the aspects of explainability, interpretability and trustworthiness become a primary concern for adoption and use of said system. \textcite{ribeiro_why_2016} already explored the importance of explainability and trust in AI-based systems and postulated that AI systems will not be used if the users have no trust in the model or the results. Even though many machine learning algorithms score high on standard performance metrics, such as precistion, recall or Area Under the Receiver Operating Characteristics (\textbf{AUROC}), user-facing performance may be way worse \parencite{gordon_disagreement_2021}. Understanding the AI's underlying machine learning (\textbf{ML}) model and its predictions is an important step for assessing trust and facilitating effective interaction \parencite{ribeiro_why_2016}. Recent technological advances are realized by \textit{Clearbox AI}, with the focus on trustworthy AI by implementing an AI model assessment \parencite{clearbox_website, eu_trustworthy_ai}. The model assessment can help model owners to identify robustness issues, potential undesired behaviour, and explain errors and uncertainties regarding the model predictions \parencite{clearbox_wp}.

Trust in AI systems is primarily induced by the users' understanding and the general interpretability of the machine learning model and their predictions \parencite{ribeiro_why_2016, ras_explanation_2018}. The wide array of different possible user groups and the complex constructs of understandability and trust demands for a human-centric approach in designing AI assessment systems. Because of the inherent complexity of non-linear machine learning models, especially Deep Neural Networks (\textbf{DNNs}) for image processing, suitable visualization and communication techniques are non-trivial. Additionally to the complex models for image classification, the input data is also more complex as it is unstructured. Non-linear neural networks and unstructured data provide additional challenges for Explainable Artificial Intelligence (\textbf{XAI}), as described in \textcite{keane_how_2019}. XAI is a research field that studies how AI decisions and data driving those decisions can be explained to people in order to provide transparency, enable assessment of accountability, demonstrate fairness, or facilitate understanding \parencite{arrieta_explainable_2019}. XAI plays an important role in the acceptance and finally in the usage of AI-based technology. This is further underlined in the medical context where public authorities set strict regulations on the usage of technological systems and ethic concerns have to be thoughtfully addressed.

In the context of a image-based medical diagnosis system, it is important that the responsible stakeholders, such as medical practitioners, specialized doctors and clinic managements, are enabled to make informed decisions on the usage of AI-based technology, even though their expertise in machine learning and data science is expected to be low. The stakeholders' trust in this system is a primary factor for the widespread use of said technology for real life applications. Therefore, increasing the understanding of the AI model and finding an optimal trust level in the predictions by designing human-centric explanation techniques within the AI model assessment system is a main goal of this work. Additionally it is conceivable that authorities will instantiate auditors for AI-based systems in medical contexts. Having a comprehensible and scientifically proven assessment system could be a big step in the approval and adoption of said system. 

\section{Goals}
The users understanding of the AI model and trust in the model are highly essential as pointed out by \textcite{knapic_explainable_2021}. This holds especially true for medical applications where re-traceable results have to be provided and people acting on these results bear great responsibility. To facilitate understanding and trust the machine learning model has to be interpretable and explainable. In the context of Convolutional Neural Networks (\textbf{CNNs}) interpretability of models can pose a significant concern because of their inherent complexity. Explanations of AI-models can provide insights on the machine's decision process and therefore generate user understanding. This can lead to the model being more interpretable by humans.

Assessing the suitability and performance of a CNN for a specific task by applying standard performance evaluation metrics is problematic, since these can be oblivious to distinguishing the diverse problem solving behaviors of a neural network \parencite{lapuschkin_unmasking_2019}. \textcite{samek_explaining_2021,JMLR:v17:15-618,ribeiro_anchors_2018} give an overview on the technical foundations of XAI and a presentation of practical methods, which will be used in conjunction with human-centric design to explore and evaluate suitable and efficient methods to explain a model's classification.

The goal of this thesis is to design, develop and evaluate interactive AI-assessment-system artifacts for medical professionals and machine learning specialists in a human-centric fashion to facilitate understandability and trustworthiness of AI-models. Developing such a system, with human concerns in focus, leads to following research questions:
\begin{itemize}
    \item[Q1:] How is the stakeholders' (medical professionals, clinic managements or data scientists) subjective information processing awareness linked to trust for a specific model and its predictions in the medical domain?
    \item[Q2:] How can different explanation techniques, ranging from perturbation based approaches to more model intrusive alternatives, increase trust in image classifier models and predictions?
    \item[Q3:] What are the most efficient methods to explain and possibly optimize trust levels in image classification models?
    \item[Q4:] To what extend can structured metadata increase the stakeholder's understanding of a model's operational range and performance?
\end{itemize}

\section{State of the Art}\label{section:state_of_the_art}
An AI-assessment-system is currently offered by Clearbox AI. The \textit{AI Control Room} cloud platform enables users to assess, improve and validate ML models and data in accordance with the principles of Trustworthy AI \parencite{clearbox_website,eu_trustworthy_ai}. \textcite{clearbox_website} describes its AI-assessment-system as a "Deep Preâ€‘production Analysis" tool:
\begin{displayquote}
    "AI Control Room automatically generates a model assessment to help model owners to identify robustness issues, potential undesired behaviour, and explain errors and uncertainties regarding the model predictions."
\end{displayquote}

Concretely the product enables users to perform following tasks for AI models working on tabular data:
\begin{description}
    \item [Model behaviour validation:] Validation metrics and plausible causes of error are clearly presented, potential limitations and irreducible uncertainty are identified and local explanations of the model behaviour are generated selecting representative points in the dataset.
    \item [Synthetic data generation:] A generative model can be used to create synthetic data points that preserve the statistical properties of the original dataset. These points can augment the original training set to improve generalization, to increase model robustness, and to oversample specific labels when in the presence of unbalanced data.
    \item [Data-centric analysis:] Generative models perform a probabilistic analysis of the underlying data allowing for robust outliers detection and uncertainty analysis. This information can help you to evaluate data quality.
    \item [Centralised tracking system:] AI Control Room acts as a centralised tracking system to store lineage, versioning, and metadata of your datasets and models. Assessments generated are securely persisted along with models and datasets.
\end{description}

Besides general information and standard metrics (see \autoref{fig:model_assessment_overview}) the assessment system offers varied insights into different aspects of the machine learning model: \autoref{fig:model_assessment_graphs} shows graphs of training and validation precision, recall and calibration, while \autoref{fig:model_assessment_analysis} shows the models strong points and limitations by analyzing the feature distribution in the data. Furthermore, the second half of the model assessment focuses more on the interpretability aspect of machine learning: \autoref{fig:model_assessment_interpret} shows a confusion matrix of possible classification results, which is then extended by example data points, chosen by the assessment system (see \autoref{fig:model_assessment_examples}). These examples can then be further explored to generate understanding of the models inner workings by applying an attribution based explanation technique combined with a decision rule explanation.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/screenshots/model_assessment_overview.png}
    \caption{AI Control Room - Model Assessment Overview with Standard Metrics}
    \label{fig:model_assessment_overview}
\end{figure}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/screenshots/model_assessment_graphs.png}
    \caption{AI Control Room - Precision-Recall and Calibration Graphs}
    \label{fig:model_assessment_graphs}
\end{figure}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/screenshots/model_assessment_analysis.png}
    \caption{AI Control Room - Model String Points and Limitations}
    \label{fig:model_assessment_analysis}
\end{figure}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/screenshots/model_assessment_interpret.png}
    \caption{AI Control Room - Interpretability Assessment}
    \label{fig:model_assessment_interpret}
\end{figure}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/screenshots/model_assessment_examples.png}
    \caption{AI Control Room - Example Data}
    \label{fig:model_assessment_examples}
\end{figure}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/screenshots/model_assessment_explanation.png}
    \caption{AI Control Room - Prediction Explanation for Examples Data}
    \label{fig:model_assessment_explanation}
\end{figure}
\clearpage

\section{Approach}
As already described in the introduction of \autoref{chapter:introduction}, many machine learning algorithms score high on standard performance metrics, but user-facing performance may be way worse \parencite{gordon_disagreement_2021}. This issue is caused by real world applications being very dependent on the actual human-AI-interaction. Following this reasoning, it lends itself to utilize a human-centered design process for creating AI-assessment systems. \autoref{fig:DIN_EN_ISO_9241} shows a standardized process of human-centered design, which was applied in this thesis to conceptualize, implement and evaluate assessment system artifacts. The key take-away is the inclusion of human aspects in all stages of the process. Research on evaluation of AI explanations revealed that there is a big gap between the perceived and actual usefulness of explanations, as described by \textcite{ras_explainable_2021}. This further underlines the need for a human-centered approach in designing AI-assessment-systems.

The thesis' structure will reflect the human-centered approach, which is visualized in \autoref{fig:DIN_EN_ISO_9241}: As already alluded in \autoref{chapter:introduction}, \autoref{chapter:analysis} is about understanding and setting the context of use by conducting literature research and user interviews. Based on the established requirements \autoref{chapter:conception} will describe the conception of functionalities and interaction design. The development of solutions will be described in \autoref{chapter:implementation}, while \autoref{chapter:evaluation} is about the evaluation of the solutions. This general process is embedded in a iterative loop, where intermediate results are evaluated against the requirements and subject to change.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{img/figures/DIN_EN_ISO_9241-210.png}
    \caption{Human-centered Design Process \parencite{DIN}}
    \label{fig:DIN_EN_ISO_9241}
\end{figure}

\newpage
\chapter{Analysis}\label{chapter:analysis}
Following the human-centered design process, it is important to incorporate the potential users from the beginning. This is also reflected in the analysis, where the context and setting of use has to be understood and set. Besides the human factors, there are also more general and theoretical aspects to be analysed, such as the context of AI in the medical application domain for specific tasks, such as classifying disease patterns with medical imaging.

In the following the context, task, problem and users will be described and analysed as a foundation for the human-centered design process and the following conception of AI assessment system artifacts.

\section{Data Sources}
Three main data sources where used for the analysis, ranging from general scientific literature about XAI to specially elaborated user interviews and cooperation with developers of an existing application.

\subsection{Scientific Literature}
Literature is the foundation of the analysis. As described by \textcite{mueller_explanation_2019}, the amount of scientific publications on the topic of explanation in intelligent systems has surged in the last 5 years, revealing many important and relevant information on this subject area through openly accessible papers. In the beginning of the thesis (July 2021) a general search on \textit{Google Scholar} was conducted to gain a overview on available publications. A non-exhaustive list of search terms at the time was:
\begin{itemize}
    \item XAI
    \item XAI in Medical Applications
    \item Explainable Artificial Intelligence
    \item Explainable Artificial Intelligence in Medical Applications
    \item Explainable Machine Learning
    \item Interpretable Machine Learning
    \item Explaining Black-Box Machine Learning Models
    \item Explaining DNNs
\end{itemize}
This general research yielded already good results, as there were many relatively new and popular publications on the topic of XAI, such as \textcite{mueller_explanation_2019, ras_explanation_2018, adadi_blackbox_2018, hoffman_metrics_2019}.

The results of the internet research were then further reinforced by academic partners from the University of LÃ¼beck, with whom related research was conducted in the context of the \textit{CoCoAI} project. Leveraging the available resources and support from research partners boosted the yield on relevant scientific literature tenfold. Over the course of multiple months the list of literature grew and is still being maintained in a shared \textit{Zotero} library \parencite{zotero_website}. The most important scientific papers for this analysis were: \textcite{ras_explanation_2018, arrieta_explainable_2019, ribeiro_why_2016, adadi_blackbox_2018,knapic_explainable_2021, samek_explaining_2021, chiou_trusting_2021, hoffman_metrics_2019}. 

\subsection{Interviews}\label{subsection:interviews}
Complementing the general research on XAI, specially elaborated user interviews where conducted. These interviews specifically target medical professionals and data scientists. Interviews with the actual user group of a potential solution is key to understanding and setting the context and requirements of use. The participants for the interviews were chosen with following requirements in mind:
\begin{description}
    \item[Medical Professionals:] Has interest and/or knowledge in AI-systems; has worked with or researched AI-systems in the medical domain; can judge the benefits and risks of the use of AI in medical applications.
    \item[Data Scientists / AI Researchers:] Is familiar with the XAI topic; has interest in explainability and trustworthiness of machine learning models; has worked with AI in the medical context.
\end{description}
Participants for the interviews were gathered via academic partners, internet research and word of mouth. In total 16 suitable people from 6 different institutions (among them UKSH, TU MÃ¼nchen and Mevis Frauenhofer Bremen) were contacted about potential interviews. Ten leads were medical professionals while six leads where data scientists or AI researchers. From the total of 16 potential interview partners only four interviews have been conducted. This low yield is due to the time constraint of the individuals, who are mainly full time medical practitioners or researchers. The participants are described in further detail in \autoref{table:interview_participants}.

\begin{table}[htbp]
    \centering
    \begin{tabularx}{\textwidth}{ l l l X X l }
        \toprule
        ID & Age & Gender & Occupation & Education Level & AAII Score \\
        \midrule
        1 & 28 & male & Assistant Physician in Neuroradiology & State Examination & 4.89 \\ 
        2 & 24 & female & Research Associate (ML) & Masters Degree & 5.12 \\ 
        3 & 48 & male & Surgeon & Dr. med. & 5.45 \\ 
        4 & 27 & female & Assistant Physician in Neuroradiology & State Examination & 4.67 \\ 
        \bottomrule
    \end{tabularx}
    \caption{Interview Participants}
    \label{table:interview_participants}
\end{table}

The Interviews were conducted in german and executed as 1 to 1 online interviews. For reference the interviews were recorded if consent was present. Additionally the interviews were supported by a research colleague, who kept protocol. After the interviews the recordings were transcribed for further analysis and the participants were asked to answer the \textit{Affinity for AI Interaction} (\textbf{AAII}) questionnaire, which is a modified version of the \textit{Affinity for Technology Interaction} (\textbf{ATI}) questionnaire \parencite{franke_personal_2019}. ATI aims to determine the tendency to actively engage in intensive technology interaction, as a key personal resource for coping with technology. Analogously the AAII questionnaire aims to determine the tendency to actively engage in AI interaction.

In terms of content, the interviews for medical professionals and data scientists were slightly different as seen in \autoref{table:interview_topics}. The reason for this is the heterogenous expertise on the subject area of machine learning models and potential user requirements. The whole interview guideline can be found in \autoref{appendix:interview_guideline}.
\begin{table}[htbp]
    \centering
    \begin{tabularx}{\textwidth}{ l l }
        \toprule
        Medical Professionals & Data Scientists / AI researchers \\
        \midrule
        Actual Usage of AI & Actual Usage of AI \\ 
        Perspective on AI Usage & Comparison of AI Models \\  
        Trust in AI & Perspective on AI Usage \\
        Potential Problems with AI Usage & Trust in AI \\
        Own Explanation Techniques & Potential Problems with AI Usage \\
        Familiarity with XAI & Own Explanation Techniques \\
        Assessment on Local Explanations & Familiarity with XAI \\
        Information Processing & Need for Local Explanations \\
        Reliability vs. Trust vs. Understanding & Need for Global Explanations \\
         & Information Processing \\
         & Trust-Behavior Connection \\
         & Reliability vs. Trust vs. Understanding \\
        \bottomrule
    \end{tabularx}
    \caption{Interview Topics}
    \label{table:interview_topics}
\end{table}

After gathering the interview data via protocols and transcripts a thematic analysis according to \textcite{braun_thematical_2006} was applied to identify common topics and codes. The thematic analysis is a widely used qualitative analysis method mainly found in the field of psychology and can be used as a primary tool to access data from interviews. Applying this method resulted in a thematic map showcasing common overlapping topics found in the interviews, which can be seen in \autoref{fig:thematic_mind_map}. The thematic map serves as the baseline for further context and requirement analysis.

\begin{figure}[htbp]
    \centering
    \includegraphics[height=0.8\textwidth, angle=90]{img/figures/Thematic_Mind_Map.pdf}
    \caption{Thematic Mind Map}
    \label{fig:thematic_mind_map}
\end{figure}

\subsection{Existing Applications}
A scientific cooperation with Clearbox AI allowed us to access a additional source of information, valuable for the analysis and conception. As already described in \autoref{section:state_of_the_art} Clearbox developed an AI model assessment solution among other things. During the period of cooperation regular meetings were held with the CTO and other employees. These meetings were used for knowledge exchange on the subject of XAI literature, previous experiences, feedback and conceptional workshops.

The enormous previous experience of Clearbox is a great resource of information for the thesis. Many aspects of analysis and conception were supported by the regular, bi-weekly meetings. In particular, resources such as \textcite{people_ai_google_website, captum_website,streamlit_website, lapuschkin_unmasking_2019} were supplied by Clearbox. Furthermore the (beta) access to the Clearbox AI \textit{Control Room} cloud platform and the communication of user feedback was invaluable to gather information on the analysis and conception of an assessment system for image-based AI models.

\section{Context Analysis}
Black-box DNNs have become pervasive in todays society and represent a proven and indispensible machine learning tool. While these machine learning models can easily be used in recreational non-risky contexts, this does not hold true for the medical domain where decisions based on the results of a machine learning model can bear great risks for users and patients. This issue stems from the lack of interpretability and trustworthiness of DNNs \parencite{adadi_blackbox_2018}. DNNs architectures are inherently hard to understand and therefore interpretability of and trust in the results of such neural networks are a challenge.

Creating a solution to explain AI models a priori to the use can help setting the right expectations towards the AI model. Consequently the users of such a explanatory system gain the ability to build a fair mental model before using the AI, which in turn can support the formation of appropriate levels of trust \parencite{hoffman_metrics_2019}. This is beneficial to the user and facilitates efficient usage of the model in production \parencite{hoffman_metrics_2019, people_ai_google_website}.

Explaining the model a priori also enables the solution to build computationally complex explanations, which can depend on datasets of thousands of images. Supplying the user with explanations potentially based on the whole dataset can also be beneficial: Statistical analysis and clustering of the data and metadata can support the understanding of model limitations and edge cases, while exemplary local explanations can be statistically distributed to gain a better overview of global behavior of the model. Combining different types of a priori explanations improve the coverage of the key attributes of explanations - understandability, feeling of satisfaction, sufficiency of detail, completeness, usefulness, accuracy, and trustworthiness - which were described by \textcite{hoffman_metrics_2019}.

The aspects mentioned above are also reflected in the interview data. The following list showcases translated quotes from interview partners regarding the need for (a priori) explanations for AI models aimed at the medical domain:
\begin{displayquote}
    "I think you have to be critical and look at the results carefully - is the result at all plausible?"
\end{displayquote}
\begin{displayquote}
    "What is the information that is interesting for the system or that is decisive for the decisions? What information is rather irrelevant?"
\end{displayquote}
\begin{displayquote}
    "If the explanations for the model are based on any data that are meaningless from my clinical experience, such as a stroke being pinned down by bone shapes. That would shake my confidence in the machine, even though it might give reliable results."
\end{displayquote}
\begin{displayquote}
    "If, for example, there is an outlier data point in a specific case and you are not quite sure why it is like this or how you should interpret it, then something like this [local explanation] is great, so that you can understand why the result is like this or like this."
\end{displayquote}
\begin{displayquote}
    "I've always been interested in exactly how this works, how much training data it's based on, what's behind it, why the system decides the way it does."
\end{displayquote}
\begin{displayquote}
    "[...] and you might also learn what the machine pays attention to, which I personally could also pay attention to when I look at the picture. That would certainly strengthen my confidence in the application."
\end{displayquote}

\section{Problem and Task Analysis}
The following problem scenario summarizes the starting point of the problem and task analysis: The general medical practitioner Dr. med. Mustermann wants to offer thrombosis diagnosis in his office. He is not specialized in vein examination and thus has just basic knowledge and also no necessary equipment. In the past this has lead to him referring patients with venous disorders to a specialized clinic. Through a colleague he was made aware of "\textit{AutoDVT}", a AI-based software developed by ThinkSono, which can help him offer DVT diagnosis in his office. The AutoDVT system works with image-based machine learning and can support medical professionals in real time with identifying DVT. Since Dr. med. Mustermann has very little knowledge of AI and machine learning he is very skeptical towards this innovative but foreign software system. Although he sees the immediate benefits of using a system which supports him with the examination of DVT, his trust in the systemâ€™s predictions is very low and he fears relying on the AIâ€™s assessment. AI-based technology is a black box for him, which he does not fully understand. The predictions of the systems are opaque to him and thus lead to rejection of the system.

DNNs for image classification are able to detect various disease patterns with medical imaging and can be used by medical professionals to support diagnosis and possibly increase efficiency and effectivity if trusted and used correctly \parencite{adadi_blackbox_2018,knapic_explainable_2021}. However, the reality looks different: Medical professionals bear the responsibility for their decisions regarding the patient and thus are rather relucant about using AI based systems - even though the AI could outperform them in image classification tasks. Most of the time decisions are based on personal experience, which was developed over a long period of time. This sentiment is reflected in the interview statements of medical professionals:
\begin{displayquote}
    "One risk, I believe, is also that you hand over responsibility to the machine."
\end{displayquote}
\begin{displayquote}
    "The classic risk of simply relying on what the algorithm says."
\end{displayquote}
\begin{displayquote}
    "The doctor with his expert knowledge will always compare this with his knowledge and experience, is this correct, what is the probability, is this in the range?"
\end{displayquote}

Modern DNN based image classificators, such as the \texttt{XRV\--DenseNet121\--densenet121\--res224\--all} model from \textcite{cohen_limits_2020}, can provide very good results in the prediction of pathologies. For example, the prediction accuracy for pneumonia is benchmarked as 86\% \parencite{torchxrayvision_github}. The use of such model or comparitive ones could benefit medical professionals in many ways as described by the interview partners:
\begin{displayquote}
    "It [AI system] facilitates standardized findings in particular"
\end{displayquote}
\begin{displayquote}
    "It [AI system] might give you a little peace of mind that you haven't missed anything."
\end{displayquote}
\begin{displayquote}
    "I always think to myself that this is based on CT gray levels, i.e. on these density values, and I always think to myself that it makes sense that a computer can distinguish these density levels better than my eyes."
\end{displayquote}

The benefits of using a DNN based image classificator to support medical professionals in detection and diagnosis of diseases are clear. For users to leverage these benefits trust in such a system must be given, which cannot be generated by mere accuracy metrics \parencite{samek_explaining_2021}. To overcome the hesitation of using AI in medical applications, a AI assessment system can be used to facilitate understanding of and trust in the algorithms. Stakeholders, such as practitioners or clinic managements, can use such a system to gain customized insights into the model and data prior to using it in everyday clinical practice.

\section{User Analysis}
Trustworthiness and explainability of AI models only makes sense considering the potential user groups. For an assessment system that aims to explain AI models in the medical domain those user groups are: (1) Medical professionals, such as practitioners and clinic managements and (2) data scientists and AI researchers developing AI models. Naturally those two groups differ greatly from each other. While medical professionals have great expertise in various fields of examination, diagnosis and communication of pathologies, they also are expected to have little knowledge in computer sciences and machine learning. Data scientists on the other hand do have great knowledge of computer sciences, machine learning and neural network architectures, but lack the concrete medical expertise. Following the human-centered process the needs, requirements and whishes in regard to an AI assessment system of those user groups were analysed, mainly referencing the interviews from \autoref{subsection:interviews} and the resulting thematic map (see \autoref{fig:thematic_mind_map}).

\subsection{Medical professionals}
The user group of medical professionals is a very heterogenous one, which is to be expected from a professional field with many different specializations. This was also found out in the interviews. Surveying practitioners of different ages showed, that especially the younger ones, working in neuroradiology, are open towards using AI in their daily routine or that they are already using it. Great initiative was shown by two assistant physicians, who also took courses on machine learning in the medical domain during their studies. Then again the older, but way more experienced surgeon has stated that he has much less contact with AI in clinical practice. All interviewed medical professionals showed interest in medical AI in research projects and were positive on the benefits of AI, especially computer vision tasks. The most cited benefits were the great ability of machine learning to identify pathologies in medical images, the take over of redundant tasks, the backup for diagnosis and the handling of computationally expensive tasks. The medical professionals were also wary and timid about using AI. This stance was stated to be mainly routed in the missing experience in using and trusting these systems. The black-box character of DNNs was stated to be a central issue: Not being able to re-trace the decisions of the AI and having to give away responsibility lead to trust and compliance issues, which was already stated by \textcite{ras_explainable_2021}. Depending on standard metrics was also stated to be insufficient, as the interviewed experts showed interest in the training data set and active comparison of the AI's results with their own experiences. While standard metrics, such as accuracy, sensitivity and specificity were important to the interviewees, the critical evaluation of the results and the validation of the behavior were whished for by every one of them.

As \autoref{table:interview_participants} shows, all interviewees had a high affinity for AI interaction. This is a important aspect to consider, since it shows their tendency to actively engage in interaction with AI while also being interested in it. This fact explains that the physicians were so interested in the explainability and comprehensibility of AI models. The interviewees stated that the explanation of AI decision and therefore the understanding of the model is important to them. Also the training data and its quality was a very common topic amongst all participants. Interestingly it was also stated, that understanding and trusting the model is important to being able to propagate the knowledge and trust to fellow medical practitioners and also patients.

Even though the interest in the functionality of machine learning models was big, the medical practitioners admitted that they have little knowledge on this subject and are limited regarding understanding the technical complexities. However they also stated that there is ongoing collaboration with AI researchers and software engineers for research purposes.

\subsection{Data scientists and AI researchers}
This potential group of AI assessment system users stand in great contrast to the previously mentioned one. Data scientists and AI researchers have a good understanding of the complexities and inner workings of machine learning algorithms. Therefore the requirements and needs of this user group are expected to be very different from the medical professionals. As \autoref{table:interview_participants} shows, unfortunately only one AI researcher could be interviewed during the analysis.

The interviewee stated experience with many kinds of neural networks, while also being familiar with clustering, featuring and interpretation tools. The perspective of AI researchers on interpretability and trustworthiness seems to be also quite different. Important aspects mentioned were: Performance metrics do not guarantee usability in real applications; separation of system results and system architecture; experimental validation; relations to developers; reviewing code. Understanding the complexities of such AI systems was a key aspect as stated by the interviewee. For this literature and study experience were mentioned to be crucial. Even some experimentation with heatmap-based explanation tools were used to understand AI models better.

While data scientists and AI researcher are different to medical professionals in their expertise, some overlap was found in the interviews: The interviewee stated to generate trust by doing exemplary input-output experiments, screening the training data set and reviewing own expectations. Another common topic was the insufficiency of standard metrics for assessing algorithm performance.

An explicit topic was the use of local explanations. The interviewee stated interest in local explanations as they are needed for improving their own understanding and for publications as proof that an algorithm works. Global explanations were not distinguished from local explanations by the interview partner, since the goal seemed to be the same: Determining if a model has weaknesses and to what it can be safely applied. Generating a benchmark for comparability of models to enable better adoption was wished for. Furthermore it was stated that theory should be researched further for understanding in addition to empiricism, which is also stated by \textcite{people_ai_google_website}:
\begin{displayquote}
    "When is an explanation really meaningful? Explaining everything is difficult, but finding out when explanation should be given."
\end{displayquote}

\section{Conclusion on the Analysis}
While the use of DNN based image classifiers for medical applications has many benefits, actual adoption is impeded by the black box character of such systems. The potential users, such as medical practitioners have trust and compliance issues. Even though the users see the immediate benefits of such AI based systems, especially in computer vision tasks, the issue with handing over responsibility to a system they do not understand prevails.

Insufficient standard metrics shall be supplemented with more interactive explanations with a focus on the comprehensibility of complex AI models. Promoting the formation of an appropriate mental model and therefore trust in the abilities of such systems is a key aspect which was identified by the analysis. The use of a priori explanations via an assessment system where the users can experiment with different AI models in a protected environment covers many requirements of the users: Screening the training data, exploring model strengths and limitations, analyzing visual explanations for images and doing input-output experiments. These are the core requirements for an assessment system to increase explainability and trustworthiness of AI in medical, image-based applications. Other relevant findings suggest that such an assessment system needs to actively consider the intention of the user, since it can vary greatly depending on the person's background: Medical professionals with low expertise in machine learning may need to have a more guided user experience, while experts on the subject of machine learning may prefer a more open interaction style. Furthermore it is conceivable that statistical clustering, based on (training) metadata distributions can improve the explanation satisfaction by offering a balanced access to huge datasets in a way that is not susceptible to biases.

\newpage
\chapter{Conception}\label{chapter:conception}
The conception of the AI assessment system follows from the requirements specified in the analysis. Based on the thesis objectives and user needs, functionalities have been derived. Core aspects to be adopted from the analysis are interactive exploration of data, visual explanation of attribution, comparative explanations, input-output experiments and interaction guidance. Before diving into the details of functional conception (\autoref{section:functionalities}), interaction design (\autoref{section:interaction_design}), and system architecture (\autoref{section:system_architecture}), an overview of the conceptual approach (\autoref{section:conceptual_approach}) and foundational use cases (\autoref{section:use_cases}) will be given.

\section{Conceptual Approach}\label{section:conceptual_approach}
Based on \autoref{chapter:analysis} and the primary objectives the conception follows the already known design process. To begin with, building upon the previously gathered information, a functional specification was created. This specification relies heavily on the thematic analysis of the interviews, which is showcased in \autoref{fig:thematic_mind_map}. The first step of creating the functional specification was to identify common use cases for an AI assessment system which can be used by medical professionals. The specification defines formal tasks, sub-tasks and required capabilities for those use cases. The formalized functionalities were further used as the foundation for an interdependency analysis, which should highlight coactive human-computer-interaction design patterns \parencite{johnson_coactive_2014}. Having the functionalities defined and set allows for conception of interaction and interface design, accompanied by the technical system architecture. The design concepts were created in a design process that heavily referenced \textcite{people_ai_google_website} and \textcite{clearbox_website}. Having Control Room by Clearbox as a reference allowed the leveraging of their expertise and their user feedback. Furthermore an design iteration was created by conducting an expert workshop on interaction design. The results from interdependency analysis, interaction dialogues and interaction flowcharts were then realized in \autoref{chapter:implementation}.

\section{Use Cases}\label{section:use_cases}
\begin{comment}
Use Cases bieten sich an, die Ergebnisse aus der Analyse zusammenzufassen und in einen oder mehrere konkrete AnwendungsfÃ¤lle zu Ã¼bertragen. Die Tabelle mit den Nutzungsanforderungen sollte hierbei helfen. Wie sieht die Situation aus, wenn Ihr System eingesetzt wird? Bei den AnwendungsfÃ¤llen noch nicht zu spezifisch werden. Es geht primÃ¤r darum zu beschrieben, was die Person macht, z. B. verÃ¤nderte ArbeitsablÃ¤ufe, neue Funktionen, etc., noch nicht wie genau dies (technisch, womit realisiert) umgesetzt wird.
\end{comment}

Text \dots

\section{Functionalities}\label{section:functionalities}
\begin{comment}
Geben Sie einen Ãœberblick Ã¼ber die geplanten FunktionalitÃ¤ten des Systems. Dazu kÃ¶nnen Sie die AnwendungsfÃ¤lle der Konzeption sowie die Tabelle mit den Nutzungsanforderungen aus der Analyse aufgreifen und den Anforderungen mÃ¶gliche FunktionalitÃ¤ten zuordnen. Priorisieren Sie dann diese FunktionalitÃ¤ten â€” bezÃ¼glich Auswirkungen auf die Gebrauchstauglichkeit des Systems und Umsetzbarkeit dieser Funktionen.
\end{comment}

Text \dots

\section{System Architecture}\label{section:system_architecture}
\begin{comment}
Mit den FunktionalitÃ¤ten gehen bestimmte Anforderungen an das zugrundeliegende System einher. Stellen Sie hier dar, welche LÃ¶sungen es fÃ¼r die Systemarchitektur gibt, welche Aufteilungen sich eignen (z. B. MVC) und wie man es umsetzen kÃ¶nnte. Je nach Detailgrad bieten sich u.a. UML-Diagramme an.

\textbf{BegrÃ¼ndete Auswahl:} Nehmen Sie die FunktionalitÃ¤ten als Grundlage, das gibt Ihnen vor, mit welchen mÃ¶glichen Architekturen Sie diese FunktionalitÃ¤ten umsetzen kÃ¶nnen. Falls es mehrere MÃ¶glichkeiten gibt, erlÃ¤utern Sie diese und treffen eine be-grÃ¼ndete Auswahl auf Basis dessen StÃ¤rken und SchwÃ¤chen fÃ¼r Ihren Anwendungsfall.

\textbf{Hardware/Software-agnostisch bleiben:} Es geht hier explizit nicht um spezifische Hard- oder Software. Sie bleiben auf der "Datenbank", "Server", etc. Ebene. Welche Datenbank (MySQL, PostgreSQL, SQLite, etc.) oder welcher Server (Apache, Node.js, etc.) verwendet wird kommt in der Realisierung. Auch hier: Nach der Konzeption kann der Leser stoppen und Ihre Konzeption auf seine Weise realisieren.

\textbf{Bezug zum Interface Design beachten:} Systemarchitektur und Interface Design hÃ¤ngen eng zusammen. Stimmen Sie beides aufeinander ab â€” in jeder Iteration.
\end{comment}

Text \dots

\section{Interaction Design}\label{section:interaction_design}
\begin{comment}
Wie kÃ¶nnen die Informationen dargeboten und Angaben der Nutzer erfolgen?

\textbf{Screen-Fetisch hinterfragen:} Viele Medieninformatiker haben einen Fetisch was visuelle Interface betrifft. Diese lassen sich leicht skizzieren, es gibt Tools dafÃ¼r, die man kennt, und man kann was zeigen. Man sieht, was man gemacht hat. FÃ¼r viele Kontexte sind sie allerdings absolut ungeeignet (z. B. um Autofahrer zu energiesparendem Fahren zu bringen). Visuelle Aufmerksamkeit ist extrem kostbar. Probieren Sie deswegen bewusst andere Sinne aus. KÃ¶nnen Sie die Informationen auch auditiv geben (z. B. Abweichung vom idealen Fahrverhalten Ã¼ber die TonhÃ¶he)? Oder haptisch (z. B. Vibration)? Gerade wenn man sich im Laufe der Konzeption in das eigene visuelle Design verliebt kommen oft LÃ¶sungen heraus, die vielleicht schÃ¶n sind, aber auch lebensgefÃ¤hrlich, und schlimmer noch: nicht gebrauchstauglich. FÃ¼r viele Kontexte sind visuelle Interfaces sehr gut geeignet â€” aber fÃ¼r viele auch nicht. Und die Auswahl sollte wie Ã¼blich begrÃ¼ndet erfolgen.

\textbf{Formativ Evaluieren:} Interface-Designs mÃ¼ssen Sie formativ evaluieren. FÃ¼r Sie ist die Interaktion klar (Sie wissen auch als â€” meist â€” einzige Person, was unter der OberflÃ¤che passiert und was die HintergrÃ¼nde sind), fÃ¼r Laien ist das oft unklar. Sie brauchen frÃ¼h Feedback der Zielgruppe. Nutzen Sie es! Dokumentieren Sie am Ende jeder Iteration, was herausgekommen ist und was die Konsequenzen fÃ¼r Ihr Design / die nÃ¤chste Iteration war.

\textbf{Papier Ã¼ber Mock-Up-Tools:} Verwenden Sie Papier um visuelle Interfaces zu gestalten. Wenn Sie nicht zeichnen kÃ¶nnen, um so besser. Bei mit dem Computer ausgearbeiteten Mockups ist die Hemmung der Zielgruppe oft zu hoch, noch grundlegende Aspekte zu Ã¤ndern. Damit bleiben Sie oft in der ersten â€” und oft unkreativen und nicht gebrauchstauglichen â€” LÃ¶sung gefangen. Tools wie Adobe XD sind eine Falle, die Sie zumindest in den ersten Iterationsphasen tunlichst vermeiden sollten.

\textbf{Bei visuellen Interfaces Skizzen der Iterationen darstellen:} Benutzen Sie Graphiken (Skizzen, Mockups) um das Interface und die Interaktion damit zu verdeutlichen.
\end{comment}

Text \dots

\section{Interface Design}\label{section:interface_design}

Text \dots

\newpage
\begin{comment}
\begin{center}
\huge
Nach der ersten Iteration:\\STOP\\
\normalsize
Nachdem Sie die erste Iteration abgeschlossen haben, nehmen Sie das Blatt Papier, auf dem Sie vor/wÃ¤hrend der Analyse Ihre LÃ¶sungsidee(n) festgehalten haben, wieder heraus. Vergleichen Sie Ihre ursprÃ¼ngliche Idee mit der ersten Iteration.

\textbf{Falls sich Ihre erste Iteration sich stark von der erste Idee unterscheidet:} Prima. Sie haben wÃ¤hrend der Analyse dazu gelernt und den Nutzungskontext besser verstanden. Ohne die Analyse und die Verwendung dieser Informationen in der Konzeption hÃ¤tten Sie das nicht machen kÃ¶nnen.

\textbf{Falls Ihre erste Iteration Ihrer ersten Idee sehr Ã¤hnlich ist:} Entweder Sie kannten sich schon vor der Analyse mit dem Themengebiet sehr gut aus â€” oder (und das ist wahrscheinlicher) â€” sie hÃ¤ngen zu stark an Ihrer ersten Idee und hÃ¶ren der Zielgruppe nicht zu. Auch falls Sie selbst zur Zielgruppe gehÃ¶ren (z. B. eine Anwendung fÃ¼r Studierende), Sie sind nicht "alle Nutzer". Nutzer unterscheiden sich Ã¼blicherweise. Entsprechend wirklich am in sich gehen â€” haben Sie aus der Analyse wirklich nichts dazu gelernt? Was das Problem (oder Sie selbst) wirklich so trivial? Das SchÃ¶ne an kreativen Prozessen ist ja, dass man iterativ besser werden kann und auch auf Ideen kommen kann, auf die man vorher nicht gekommen wÃ¤re. Die Chance sollten Sie nutzen.

BTW, dieser Hinweis heiÃŸt nicht, dass Sie komplett naiv an Ihre Gestaltung herangehen sollten, ohne sich vorher Gedanken zu machen. Aber, wenn Sie eine Analyse wirklich ernst nehmen, dann sollte die Ihnen Sachverhalte aufzeigen, die man als AuÃŸen-stehender nicht sieht. Und Sie sollten auch in der Konzeption basierend auf diesen Informationen LÃ¶sungen entwickeln, die Ã¼ber die ersten simplen Ideen hinausgehen (vgl. confirmation bias). Die neu und nÃ¼tzlich sind.
\end{center}
\end{comment}

\section{Conclusion on the Conception}
\begin{comment}
Wie auch bei der Analyse am Ende der Konzeption ein kurzes Fazit um den Entwicklungsprozess zusammenzufassen und begrÃ¼nden, was die erfolgversprechendste LÃ¶sung (hÃ¶chste Gebrauchstauglichkeit in der formativen Evaluation!) fÃ¼r die Realisierung ist.
\end{comment}

Text \dots

\newpage
\chapter{Implementation}\label{chapter:implementation}
\begin{comment}
Beschreiben Sie, wie Sie das Projekt konkret realisiert haben, mit dem Ziel, anderen (Medien-)Informatikern zu erlauben, Ihre Anwendung weiter zu entwickeln.

\textbf{Test:} Sie nehmen einen Kommilitonen, geben ihm dieses Kapitel und bitten ihm eine Funktion hinzuzufÃ¼gen oder zu verÃ¤ndern. Wenn er das kann, prima. Wenn nicht, besser schreiben oder bessere Kommilitonen suchen.

Verweisen Sie zu Beginn auf die erfolgreichste LÃ¶sung aus der Konzeption. Wie in der Konzeption bieten sich Unterkapitel mit Systemarchitektur und Interface-Design an â€” jetzt allerdings mit konkreten Angaben, welche Programme / Frameworks Sie verwenden, und wie Sie das Interface konkret umsetzen.

Geben Sie zuerst einen allgemeinen Ãœberblick und werden dann konkreter. Als wenn Sie in eine Weltkarte reinzoomen. Erst mal sagen, wie die Welt aufgebaut ist (Kontinente), dann auf Kontinent mit LÃ¤ndern zoomen, dann exemplarisch mal auf ein Land. Leser braucht ein gutes mentales Modell vom Aufbau und den Funktionen, um zu verstehen, wie es technisch umgesetzt wurde.

\textbf{BegrÃ¼nden Sie Ihre Entscheidungen linear:} Z.B.

\begin{enumerate}
    \item "das sind die Anforderungen aus der Analyse/bisheriger Konzeption"
    \item "Programmiersprache / System / etc. x, y, und z erfÃ¼llen diese Anforderungen"
    \item "relevante Kriterien sind a, b, c und d (z. B. Verbreitung, Community, Weiterentwicklung, Preis)"
    \item "Programmiersprache / System / etc. x, y, und z schneiden da so und so ab, x ist insgesamt am besten"
    \item "Programmiersprache / System / SDK / etc. x wird verwendet". Eine Argumentation Ã  la A => B => C => D => E ist nachvollziehbarer als wir nehmen E, weil A => C woraus D folgt, wegen B.
\end{enumerate}

Weiterer Grund: Wenn Sie zuerst die konkrete Software sagen, dann fangen Personen direkt an, die Entscheidung zu hinterfragen (weil sie z. B. eine andere bevorzugen, gibt da genug Religionskriege). Wenn Sie linear argumentieren gehen diese Personen eher bei jedem Schritt mit und enden dann bei der von Ihnen verwendeten Software (sofern sie bei den Auswahlkriterien und der Bewertung mitgehen).

\textbf{Nutzen Sie den DatentrÃ¤ger im Anhang der Arbeit:} Insbesondere in den Programmcode kÃ¶nnen Sie viel auslagern. In der Arbeit muss der Leser aber die Struktur/den Aufbau Ihres Programms verstehen sowie die Logik dahinter. Und er muss wissen, dass es die ErlÃ¤uterungen im Programmcode gibt und wo er diese findet. Entsprechend die Struktur (Dateien/Klassen) deutlichen machen und Verweise auf relevanten Dateien.
\end{comment}

Text \dots

\section{System Architecture Implementation}
\begin{comment}
vgl. Konzeption, jetzt aber wie und womit es konkret umgesetzt wurde (u.U. bieten sich UML-Diagramme an)
\end{comment}

Text \dots

\section{Interface Implementation}
\begin{comment}
vgl. Konzeption, jetzt aber wie und womit es konkret umgesetzt wurde 
\end{comment}

Text \dots

\section{Conclusion on the Implementation}
\begin{comment}
Kurzes Fazit, das u.a. die Frage beantwortet, was technisch konkret umgesetzt wurde und was z. B. aus der Konzeption Ã¼brig geblieben ist und warum.
\end{comment}

Text \dots

\newpage
\chapter{Dialogue Samples}
\begin{comment}
Verdeutlichen Sie die Funktionsweise Ihre Entwicklung bei einer typischen Bedienung (z. B. wie in einem Anwendungsbeispiel / Use Case beschrieben â€” auf den kÃ¶nnen Sie sich ja beziehen).

Die Abbildungen mÃ¼ssen im Text vorher erlÃ¤utert sein und einen Eindruck geben, wie die Entwicklung konkret bedient wird bzw. was sie kann.

\textbf{Das ist das, was bleibt:} Diese Dialogbeispiele sind das einzige, was in ein paar Jahren von Ihrer Anwendung noch sichtbar ist. Dann ist entweder der DatentrÃ¤ger verschwunden oder beschÃ¤digt oder die notwendige Hard- und Software ist nicht mehr lauffÃ¤hig. Geben Sie dem Leser entsprechend einen guten Einblick in das, was Sie tatsÃ¤chlich realisiert haben.

\textbf{Jetzt ein Video aufnehmen:} Schalten Sie den Screenrecorder auf dem Computer (not-falls: QuickTime) oder Smartphone (kann das OS) an und nehmen Sie die typischen Interaktionen einmal auf. Das ist Ihr "Plan B", falls (oft: wenn) im Kolloquium die App nicht bedient werden kann. Das kÃ¶nnen Sie auch gut auf DVD brennen um Lesern die MÃ¶glichkeit geben, sich die Interaktion anzusehen (in der Arbeit darauf hinweisen!). Macht sich auch gut auf Websites (insbesondere dem eigenen Portfolio).
\end{comment}

Text \dots

\newpage
\chapter{Summative Evaluation}\label{chapter:evaluation}
\begin{comment}
Geben Sie zu Beginn der Evaluation einen kurzen Ãœberblick Ã¼ber Ihr Vorgehen. Dazu reichen meist die ZwischenÃ¼berschriften mit ein oder zwei SÃ¤tzen, was Sie konkret gemacht haben. Also nicht "In Design wird das Design beschrieben" sondern "Das experimentelle Vorgehen wird im Abschnitt Design dargestellt".
\end{comment}

Text \dots

\section{Goal}
\begin{comment}
Die summative Evaluation ist eine abschlieÃŸende Bewertung Ihrer Entwicklung. Ziel ist, unter dem Strich zu sehen, wie gebrauchstauglich Ihr System ist (nicht mehr eine iterative Verbesserung wie in der formativen Evaluation der Konzeption). Da-fÃ¼r mÃ¼ssen Sie klare Kriterien ableiten, was eine "gute" bzw. "schlechte" Bewertung nach sich ziehen wÃ¼rde. Meist sind das die klassischen Gebrauchstauglichkeitskriterien (EffektivitÃ¤t, Effizienz, Erlernbarkeit, Zufriedenstellung), wobei diese Ã¼ber das Ziel Ihrer Anwendung konkretisiert werden (EffektivitÃ¤t bei einer Lernapp ist konkret gemessen etwas anderes als EffektivitÃ¤t bei einem digitalen Depressionstagebuch).

Hier verdeutlichen Sie entsprechend, welche Fragen die Evaluation beantworten soll.
\end{comment}

Text \dots

\section{Methods}
\begin{comment}
Im Methodenteil zeigen Sie, was wie evaluiert wurde. Der Methodenteil muss anderen Entwicklern die MÃ¶glichkeit geben, Ihr Evaluationsvorgehen zu wiederholen um Ihre Ergebnisse zu Ã¼berprÃ¼fen. 
\end{comment}

Text \dots

\subsection{Design}
\begin{comment}
Kurze Beschreibung des Versuchs- oder Evaluationsdesigns, dass man das Vorgehen einordnen kann. Also z. B. "es wurde ein Usability Test durchgefÃ¼hrt", oder "es wurde ein Experiment mit between-subjects design durchgefÃ¼hrt, bei dem die Kontrollgruppe die bisherige App verwendet hat und die Experimentalgruppe die neu entwickelte App".
\end{comment}

Text \dots

\subsection{Participants}
\begin{comment}
Kurze Beschreibung der Teilnehmer mit relevanten Angaben. In jedem Fall die Anzahl, oft noch Geschlecht, Alter, Beruf, Vorerfahrung, etc. Bei weniger als zehn Teilnehmern bietet sich eine Tabelle zur schnellen Ãœbersicht an. Verweisen Sie bei individuellen Ergebnissen (z.B. Zitaten aus FragebÃ¶gen oder Interviews) auf die Teilnehmer-Nummer.

Achtung: Niemals die Namen der Teilnehmer erwÃ¤hnen! Die Teilnehmer stehen stellvertretend fÃ¼r die Zielgruppe. Wer sie konkret sind ist irrelevant. BegrÃ¼nden Sie, warum Sie diese Personen ausgewÃ¤hlt haben (spiegeln die Nutzergruppe gut wider) und wo/wie Sie diese rekrutiert haben.
\end{comment}

Text \dots

\subsection{Setting and Instruments}
\begin{comment}
Beschreiben Sie die notwendigen Materialien bei der Evaluation. Dazu gehÃ¶rt â€” mit Ãœberschriften klar ausgewiesen â€” das Setting (wo wurde die Evaluation durchgefÃ¼hrt), Ihre Entwicklung (Verweis auf Dialogbeispiele), und Ihre Erhebungsmethoden (FragebÃ¶gen, InterviewleitfÃ¤den, etc.).

Falls Sie etablierte FragebÃ¶gen verwenden (z.B. ATI) reicht die entsprechende Zitation mit einer kurzen Beschreibung. Bei lÃ¤ngeren FragebÃ¶gen oder InterviewleitfÃ¤den nennen Sie kurz die Abschnitte (z.B. soziodemographische Daten, Technikerfahrung, etc.) und verweisen Sie auf die vollstÃ¤ndigen FragebÃ¶gen im Anhang.

\textbf{ACHTUNG:} Zeitliche Reihenfolge ist hier egal. Hier geht es nach Gliederungspunkten wie Instrumente (FragebÃ¶gen, Interviews, etc.). Die zeitliche Reihenfolge wird in der Prozedur dargestellt.
\end{comment}

\paragraph{Setting}\mbox{} \\
Text \dots

\paragraph{Verwendete Anwendung}\mbox{} \\
Text \dots

\paragraph{FragebÃ¶gen}\mbox{} \\
Text \dots

\subsection{Procedure}
\begin{comment}
Beschreiben Sie chronologisch den Ablauf der Evaluation, von der BegrÃ¼ÃŸung bis zur Verabschiedung. Verweisen Sie dabei auf die anderen Abschnitte (v.a. Setting und Instrumente) und fÃ¼hren Sie nichts Neues mehr ein. Dieser Abschnitt ist der einzige Abschnitt, in dem Sie die zeitliche Reihenfolge klar einhalten mÃ¼ssen, alle anderen sind inhaltlich strukturiert.
\end{comment}

Text \dots

\begin{comment}
Nach dem Lesen der Methode muss deutlich geworden sein, wie Sie Ihre Evaluationsfragen messbar gemacht haben. Wie haben Sie z.B. Benutzerzufriedenheit oder Effizienz gemessen? Die Leser mÃ¼ssen sich aufgrund des Methodenteils in die Lage der Teilnehmer versetzen kÃ¶nnen und ein mentales Modell Ihrer Evaluation bilden kÃ¶nnen.
\end{comment}

\section{Results}
\begin{comment}
In den Ergebnissen zeigen Sie wie die Ergebnisse analysiert wurden um Ihre Evaluationsfragen zu beantworten. Nicht einfach die Daten auflisten, sondern stellen Sie die Ergebnisse strukturiert dar und betonen Sie die wichtigen Aspekte. Gliedern Sie die Ergebnisse nach Ihren Forschungsfragen / Fragestellungen (nicht zeitlich oder nach Erhebungsmethoden wie FragebÃ¶gen vs. Beobachtung). Zu Beginn (falls relevant) sollten Sie Ã¼berprÃ¼fen, ob die Anwendung auch wirklich so verwendet wurde, wie sie verwendet werden sollte (manipulation check). Haben die Personen also z. B. wirklich die Aufgaben mit der App gelÃ¶st oder haben sie die App schnell beiseite gelegt.

Stellen Sie die Befunde / Ergebnisse von Analysen / Evaluationen / etc. immer so neutral und so objektiv wie mÃ¶glich dar â€” ohne Ihre subjektive Interpretation oder Bewertung. Also keine Begriffe wie "lediglich", "nur", "kÃ¶nnte / wÃ¼rde / sollte / etc." oder Bewertungen wie "hat gut / nicht gut geklappt". Diese Bewertungen gehÃ¶rt in die Diskussion.

Ãœberlegen Sie sich mit welchen Tabellen und Abbildungen Sie die Ergebnisse gut darstellen kÃ¶nnen. Bei aggregierten Messwerten (auf mindestens Intervallskalenniveau) immer Mittelwerte (M), Standardabweichungen (SD) und die Anzahl der Messdaten (Personen, n) angeben. Bei ordinalskalierten Daten entsprechend Median, etc. (ja, Statistik war wichtig).

Statistische Tests korrekt angeben (siehe z.B. Pallant, 2010).

Pallant, J. (2007). SPSS Survival Manual (3rd ed.). Open University Press.

Abbildungen mÃ¼ssen in sich verstÃ¤ndlich sein (was abgebildet ist). Das heiÃŸt, die Achsen eindeutig beschriften, Skala (z.B. Likert-Skala von 1 starke Ablehnung bis 7 starke Zustimmung) in die Legende. 3D-Graphiken vermeiden â€” diese bieten oft keinen Mehrwert (vgl. Field, 2016).

Field, A. (2016). An Adventure in Statistics. Sage.

Am Ende des Ergebnis-Abschnitts muss deutlich geworden sein, wie Ihre Entwicklung von den Teilnehmenden eingesetzt wurde (hoffentlich wie geplant), was die Hauptergebnisse waren, sofern aufgrund der StichprobengrÃ¶ÃŸe mÃ¶glich welche Werte sich statistisch signifikant voneinander unterscheiden und was die statistischen Ergebnisse in den Variablen bedeuten (z. B. positive Korrelation zwischen A und B, dass Personen, die A besser bewertet haben auch B besser bewertet haben; aber keine Bewertung ob das gut oder schlecht ist). Falls Sie konkrete Ziele hatten (z. B. "SUS-Wert von x") dann sagen Sie, ob dieses Ziel erreicht wurde oder nicht (das ist keine Wertung, die in die Diskussion gehÃ¶ren wÃ¼rde, sondern ein grÃ¶ÃŸer, gleich oder kleiner was eindeutig ist).
\end{comment}

Text \dots

\section{Discussion}
\begin{comment}
In der Diskussion erklÃ¤ren und interpretieren Sie die Ergebnisse. Welche Schlussfolgerungen ziehen Sie daraus? Was sind die praktischen Konsequenzen fÃ¼r die (weitere) Entwicklung? Hier dÃ¼rfen Sie selbst die Ergebnisse bewerten â€” auf Basis von einer kritischen Reflektion.
In der Diskussion keine neuen Ergebnisse aus der Analyse / Evaluation / etc. einfÃ¼hren. Die Beweisaufnahme ist mit Ende des Ergebnisteils abgeschlossen. Es geht hier auch nicht um eine Mystery-Geschichte mit Spannungsbogen, sondern um klar nachvollziehbare Argumente. Neue Informationen aus der Literatur verwenden um die (v.a. Ã¼berraschende) Ergebnisse zu interpretieren ist dagegen mÃ¶glich.
\end{comment}

Text \dots

\section{Conclusion on the Evaluation}
\begin{comment}
Fassen Sie die Evaluation kurz zusammen â€” insbesondere was die zentralen Ergebnisse waren. Unterm Strich: Wie gut hat's geklappt?
\end{comment}

Text \dots

\newpage
\chapter{Summary and Outlook}
\begin{comment}
Kurze EinfÃ¼hrung, was in den folgenden Unterkapiteln behandelt wird.
\end{comment}

Text \dots

\section{Summary}
\begin{comment}
Fassen Sie die zentralen Schritte und Ergebnisse Ihrer Arbeit kurz zusammen. Personen mit wenig Zeit mÃ¼ssen aus dieser Darstellung die Kernpunkte Ihrer Arbeit mit-nehmen und Ihren Arbeitsaufwand und Erfolg bewerten kÃ¶nnen.

Ist Ã¤hnlich wie die Zusammenfassung zu Beginn der Arbeit, aber etwas lÃ¤nger (1 bis maximal 2 Seiten) mit Verweisen auf die entsprechenden Kapitel/Abschnitte und Sie kÃ¶nnen beim Leser etwas mehr voraussetzen (hat es gelesen oder kann wegen den Verweisen direkt dahin springen).
\end{comment}

Text \dots

\section{Outstanding Issues}
\begin{comment}
\textbf{Offene Punkte = Versprochene aber nicht umgesetzte Punkte:} Falls Schritte explizit geplant wurden (ExposÃ©! Pflichtenheft!), aber nicht realisiert werden konnten, dann diese hier klar darstellen und diskutieren.

MÃ¶gliche Features, die Sie nicht vor Beginn versprochen haben, gehÃ¶ren in den Ausblick.
\end{comment}

Text \dots

\section{Outlook}
\begin{comment}
Ideen, welche weiteren Entwicklungen oder Untersuchungen folgen sollten, oder was man noch umsetzen kÃ¶nnte, gehÃ¶ren in den Ausblick.

Versprochene aber nicht umgesetzte Elemente in die Offenen Punkte.

Bitte keine Allgemeinheiten ("kÃ¶nnen noch Features hinzugefÃ¼gt werden" oder "kÃ¶nnte besser evaluiert werden") sondern konkrete Beschreibungen und BegrÃ¼ndungen der Relevanz dieser Schritte.
\end{comment}

Text \dots

\section{Final Conclusion}
\begin{comment}
Die Arbeit, die Ergebnisse und weitere mÃ¶gliche Schritte kurz kritisch (ehrlich und konstruktiv, aber nicht selbstkreuzigend) reflektieren und positiv enden. Maximal eine halbe bis 3/4 Seite.
Ist kurz und hier kÃ¶nnen Sie von der Arbeit zurÃ¼cktreten und auch den Leser aus dem Text ziehen.

Das ist nur zum Test: \textcite{ford} \textcite{hadoop}
\end{comment}

Text \dots

\clearpage
\include{appendix}
\end{document}
