\chapter*{Acknowledgement}
Special thanks to Tim Schrills from the University of Lübeck and Luca Gilli from\\ Clearbox AI, who supported me in the creation of the thesis from start to finish.

\newpage
\chapter*{Abstract}
With the all pervasive use of artificial intelligence in current technological advances the medical domain is no exception. Performant but complex AI models, such as deep neural networks, can be used for decision support systems for medical professionals. However, such AI models are commonly referred to as a "black box" which humans struggle to understand. This lack of understanding leads to trust and compliance issues, especially in the medical context, where the consequences can be severe. Combining the HCI with the XAI domain allows for designing and developing a human-centric AI assessment system to facilitate the AI model's understandability and trustworthiness for the user. As part of this thesis a prototype was conceptualized based on user-centered research and XAI literature, implemented as a flexible browser-based application and evaluated with medical students. The results show connections between interactive explanations, understandability and trustworthiness of AI models. A summative evaluation of the prototype showed that the user's subjective understanding of the AI model increased through the interaction with the system. Furthermore the user's perceived trustworthiness of the AI model decreased. From this finding we can conclude that the presented interactive explanations are suitable for moderating the user's subjective understanding and perceived trustworthiness of the AI model. Additionally, guidance in HCI was observed to reduce the explanation satisfaction for the users surveyed, while having no significant effects on perceived understandability and trustworthiness of the AI model.

\section*{Keywords}
XAI, HCI, Human-Centered Design, Medical AI

\newpage
\chapter*{Kurzfassung}
Angesichts des allgegenwärtigen Einsatzes von künstlicher Intelligenz in den aktuellen technologischen Fortschritten bildet der medizinische Bereich keine Ausnahme. Leistungsstarke, aber komplexe KI-Modelle, wie z.B. tiefe neuronale Netze, können für Entscheidungshilfesysteme für Mediziner verwendet werden. Solche KI-Modelle werden jedoch gemeinhin als "Blackbox" bezeichnet, die für den Menschen schwer zu verstehen sind. Dieses mangelnde Verständnis führt zu Vertrauens- und Compliance-Problemen, insbesondere im medizinischen Kontext, wo die Folgen schwerwiegend sein können. Die Kombination der Bereiche HCI und XAI ermöglicht den Entwurf und die Entwicklung eines menschenzentrierten KI-Bewertungssystems, um die Verständlichkeit und Vertrauenswürdigkeit des KI-Modells für den Benutzer zu verbessern. Im Rahmen dieser Arbeit wurde ein Prototyp auf der Grundlage von nutzerzentrierter Forschung und XAI-Literatur konzipiert, als flexible browserbasierte Anwendung implementiert und mit Medizinstudenten evaluiert. Die Ergebnisse zeigen Zusammenhänge zwischen interaktiven Erklärungen, Verständlichkeit und Vertrauens\-würdigkeit von KI-Modellen. Eine summative Evaluation des Prototyps zeigte, dass das subjektive Verständnis des Nutzers für das KI-Modell durch die Interaktion mit dem System zunahm. Außerdem nahm die vom Benutzer wahrgenommene Vertrauenswürdigkeit des KI-Modells ab. Daraus lässt sich schließen, dass die vorgestellten interaktiven Erklärungen geeignet sind, das subjektive Verständnis und die wahrgenommene Vertrauenswürdigkeit des KI-Modells durch den Nutzer zu moderieren. Darüber hinaus wurde beobachtet, dass eine Anleitung des Nutzers in der Interaktion die Zufriedenheit der befragten Nutzer mit den Erklärungen verringert, während sie keine signifikanten Auswirkungen auf die wahrgenommene Verständlichkeit und Vertrauenswürdigkeit des KI-Modells hat.

\section*{Schlüsselwörter}
XAI, HCI, menschenzentriertens Design, medizinische KI

\newpage